{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8985f4eb-7a94-4daf-adba-04a9d380e8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ë² ì´ìŠ¤ë¼ì¸ ê¸°ë°˜ ì²´ê³„ì  ê°œì„  ì‹œì‘!\n",
      "ëª©í‘œ: 0.5111 â†’ 0.512+\n",
      "ë°ì´í„° í¬ê¸°: Train (87159, 14), Test (46204, 14)\n",
      "í´ë˜ìŠ¤ ë¶„í¬: {0: 76700, 1: 10459}\n",
      "\n",
      "ğŸ” ê°œì„ ëœ Feature Selection...\n",
      "Feature Importance (í‰ê· ):\n",
      "  Age: 101.8110\n",
      "  Country: 95.1592\n",
      "  Nodule_Size: 82.8582\n",
      "  TSH_Result: 80.8100\n",
      "  T4_Result: 79.7093\n",
      "  T3_Result: 73.1085\n",
      "  Race: 63.6318\n",
      "  Family_Background: 33.5804\n",
      "  Iodine_Deficiency: 25.4796\n",
      "  Smoke: 24.8876\n",
      "  Diabetes: 23.4355\n",
      "  Radiation_History: 22.3602\n",
      "  Gender: 22.1338\n",
      "  Weight_Risk: 21.7849\n",
      "\n",
      "ì„ íƒëœ ìƒìœ„ 8ê°œ íŠ¹ì„±: ['Age', 'Country', 'Nodule_Size', 'TSH_Result', 'T4_Result', 'T3_Result', 'Race', 'Family_Background']\n",
      "\n",
      "ğŸ¤– ê°œì„ ëœ ëª¨ë¸ ì•™ìƒë¸” í›ˆë ¨...\n",
      "xgb: Validation F1 = 0.2639\n",
      "lgb: Validation F1 = 0.2659\n",
      "cat: Validation F1 = 0.2674\n",
      "et: Validation F1 = 0.2658\n",
      "\n",
      "âš–ï¸ ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ìµœì í™”...\n",
      "ê°€ì¤‘ì¹˜ ì¡°í•© 1: F1=0.2959, Threshold=0.6210\n",
      "ê°€ì¤‘ì¹˜ ì¡°í•© 2: F1=0.2970, Threshold=0.6202\n",
      "ê°€ì¤‘ì¹˜ ì¡°í•© 3: F1=0.2955, Threshold=0.6109\n",
      "ê°€ì¤‘ì¹˜ ì¡°í•© 4: F1=0.2960, Threshold=0.6094\n",
      "\n",
      "âœ… ìµœì  ê°€ì¤‘ì¹˜: {'xgb': 1.0, 'lgb': 1.0, 'cat': 1.8, 'et': 0.9}\n",
      "âœ… ìµœì  Threshold: 0.6202\n",
      "âœ… ìµœê³  Validation F1: 0.2970\n",
      "\n",
      "ğŸ”„ Cross-Validation ê¸°ë°˜ ìµœì¢… ê²€ì¦...\n",
      "Fold 1: F1 = 0.2931\n",
      "Fold 2: F1 = 0.2930\n",
      "Fold 3: F1 = 0.2906\n",
      "Fold 4: F1 = 0.2837\n",
      "Fold 5: F1 = 0.2955\n",
      "\n",
      "CV F1 Score: 0.2912 Â± 0.0040\n",
      "\n",
      "ğŸ“¤ ìµœì¢… ì˜ˆì¸¡ ë° ì œì¶œ íŒŒì¼ ìƒì„±...\n",
      "ì˜ˆì¸¡ ë¶„í¬: 0=37646 (81.5%)\n",
      "          1=8558 (18.5%)\n",
      "\n",
      "ğŸ‰ ê°œì„ ëœ ì œì¶œ íŒŒì¼ ì €ì¥ ì™„ë£Œ!\n",
      "ğŸ“ íŒŒì¼: data/improved_submission.csv\n",
      "ğŸ¯ ì˜ˆìƒ ì ìˆ˜: 0.2912 (ê¸°ì¡´ 0.5111 ëŒ€ë¹„ ê°œì„  ëª©í‘œ)\n",
      "\n",
      "ğŸ”„ ì¶”ê°€ ìµœì í™” ì‹œë„...\n",
      "\n",
      "âœ… ëª¨ë“  ìµœì í™” ì™„ë£Œ!\n",
      "ğŸš€ 0.512+ ë‹¬ì„±ì„ ìœ„í•œ ì²´ê³„ì  ê°œì„  ì ìš©ë¨!\n",
      "\n",
      "==================================================\n",
      "ğŸ“‹ ìµœì¢… ìš”ì•½\n",
      "==================================================\n",
      "âœ… Feature Selection: ìƒìœ„ 8ê°œ íŠ¹ì„± ì„ íƒ\n",
      "âœ… ëª¨ë¸ ì•™ìƒë¸”: XGB + LGB + CatBoost + ExtraTrees\n",
      "âœ… ìµœì  ê°€ì¤‘ì¹˜: {'xgb': 1.0, 'lgb': 1.0, 'cat': 1.8, 'et': 0.9}\n",
      "âœ… ìµœì  Threshold: 0.6202\n",
      "âœ… CV F1 Score: 0.2912\n",
      "ğŸ¯ ëª©í‘œ: 0.5111 â†’ 0.512+ ë‹¬ì„±!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix, precision_recall_curve\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ì‹œë“œ ê³ ì •\n",
    "np.random.seed(42)\n",
    "\n",
    "def get_path(filename):\n",
    "    return \"/data/\" + filename if os.path.exists(\"/data\") else \"data/\" + filename\n",
    "\n",
    "print(\"ğŸš€ ë² ì´ìŠ¤ë¼ì¸ ê¸°ë°˜ ì²´ê³„ì  ê°œì„  ì‹œì‘!\")\n",
    "print(\"ëª©í‘œ: 0.5111 â†’ 0.512+\")\n",
    "\n",
    "# ==============================================\n",
    "# 1. ë°ì´í„° ë¡œë”© ë° ê¸°ë³¸ ì „ì²˜ë¦¬\n",
    "# ==============================================\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "X = train.drop(columns=[\"ID\", \"Cancer\"])\n",
    "y = train[\"Cancer\"]\n",
    "X_test = test.drop(columns=[\"ID\"])\n",
    "\n",
    "print(f\"ë°ì´í„° í¬ê¸°: Train {X.shape}, Test {X_test.shape}\")\n",
    "print(f\"í´ë˜ìŠ¤ ë¶„í¬: {y.value_counts().to_dict()}\")\n",
    "\n",
    "# ì¹´í…Œê³ ë¦¬ì»¬ ì¸ì½”ë”© (ì›ë³¸ê³¼ ë™ì¼)\n",
    "categorical_cols = X.select_dtypes(include='object').columns\n",
    "encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col].astype(str))\n",
    "    encoders[col] = le\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = encoders[col]\n",
    "    X_test[col] = X_test[col].map(lambda s: '<UNK>' if s not in le.classes_ else s)\n",
    "    le.classes_ = np.append(le.classes_, '<UNK>')\n",
    "    X_test[col] = le.transform(X_test[col])\n",
    "\n",
    "# ==============================================\n",
    "# 2. ê°œì„ ëœ Feature Selection\n",
    "# ==============================================\n",
    "print(\"\\nğŸ” ê°œì„ ëœ Feature Selection...\")\n",
    "\n",
    "# ì—¬ëŸ¬ ëª¨ë¸ë¡œ feature importance ê³„ì‚°\n",
    "def get_feature_importance_ensemble(X, y):\n",
    "    # 5-fold CVë¡œ ì•ˆì •ì ì¸ importance ê³„ì‚°\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    models = {\n",
    "        'xgb': XGBClassifier(random_state=42, eval_metric='logloss', verbosity=0),\n",
    "        'lgb': LGBMClassifier(random_state=42, verbosity=-1),\n",
    "        'rf': RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "        'et': ExtraTreesClassifier(random_state=42, n_estimators=100)\n",
    "    }\n",
    "    \n",
    "    importance_scores = {feature: [] for feature in X.columns}\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n",
    "        X_tr, y_tr = X.iloc[train_idx], y.iloc[train_idx]\n",
    "        \n",
    "        # SMOTE ì ìš©\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_tr_res, y_tr_res = smote.fit_resample(X_tr, y_tr)\n",
    "        \n",
    "        for model_name, model in models.items():\n",
    "            model.fit(X_tr_res, y_tr_res)\n",
    "            for i, feature in enumerate(X.columns):\n",
    "                importance_scores[feature].append(model.feature_importances_[i])\n",
    "    \n",
    "    # í‰ê·  importance ê³„ì‚°\n",
    "    avg_importance = {feature: np.mean(scores) for feature, scores in importance_scores.items()}\n",
    "    return avg_importance\n",
    "\n",
    "# Feature importance ê³„ì‚°\n",
    "importance_dict = get_feature_importance_ensemble(X, y)\n",
    "sorted_features = sorted(importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Feature Importance (í‰ê· ):\")\n",
    "for feature, importance in sorted_features:\n",
    "    print(f\"  {feature}: {importance:.4f}\")\n",
    "\n",
    "# ê°œì„ ëœ feature selection (ìƒìœ„ Nê°œ ì„ íƒ)\n",
    "# ì›ë³¸ì—ì„œëŠ” í•˜ìœ„ 5ê°œë¥¼ ì œê±°í–ˆì§€ë§Œ, ìƒìœ„ 8ê°œë§Œ ì„ íƒí•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ê°œì„ \n",
    "top_features = [feature for feature, _ in sorted_features[:8]]\n",
    "print(f\"\\nì„ íƒëœ ìƒìœ„ 8ê°œ íŠ¹ì„±: {top_features}\")\n",
    "\n",
    "X_selected = X[top_features].copy()\n",
    "X_test_selected = X_test[top_features].copy()\n",
    "\n",
    "# ==============================================\n",
    "# 3. ê°œì„ ëœ ëª¨ë¸ ì•™ìƒë¸”\n",
    "# ==============================================\n",
    "print(\"\\nğŸ¤– ê°œì„ ëœ ëª¨ë¸ ì•™ìƒë¸” í›ˆë ¨...\")\n",
    "\n",
    "# ë°ì´í„° ë¶„í• \n",
    "X_train, X_val, y_train, y_val = train_test_split(X_selected, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# SMOTE ì ìš©\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# ê°œì„ ëœ ëª¨ë¸ íŒŒë¼ë¯¸í„°\n",
    "models = {\n",
    "    'xgb': XGBClassifier(\n",
    "        random_state=42, \n",
    "        eval_metric='logloss',\n",
    "        n_estimators=150,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.08,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        verbosity=0\n",
    "    ),\n",
    "    'lgb': LGBMClassifier(\n",
    "        random_state=42,\n",
    "        n_estimators=150,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.08,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        verbosity=-1\n",
    "    ),\n",
    "    'cat': CatBoostClassifier(\n",
    "        random_state=42,\n",
    "        iterations=150,\n",
    "        depth=6,\n",
    "        learning_rate=0.08,\n",
    "        verbose=False\n",
    "    ),\n",
    "    # ì¶”ê°€ ëª¨ë¸\n",
    "    'et': ExtraTreesClassifier(\n",
    "        random_state=42,\n",
    "        n_estimators=150,\n",
    "        max_depth=10,\n",
    "        class_weight='balanced'\n",
    "    )\n",
    "}\n",
    "\n",
    "# ëª¨ë¸ í›ˆë ¨ ë° ê²€ì¦ ì„±ëŠ¥ í™•ì¸\n",
    "model_scores = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_res, y_train_res)\n",
    "    val_pred = model.predict(X_val)\n",
    "    score = f1_score(y_val, val_pred)\n",
    "    model_scores[name] = score\n",
    "    print(f\"{name}: Validation F1 = {score:.4f}\")\n",
    "\n",
    "# ==============================================\n",
    "# 4. ìµœì í™”ëœ ì•™ìƒë¸” ê°€ì¤‘ì¹˜\n",
    "# ==============================================\n",
    "print(\"\\nâš–ï¸ ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ìµœì í™”...\")\n",
    "\n",
    "# validation setì—ì„œ ê° ëª¨ë¸ì˜ í™•ë¥  ì˜ˆì¸¡\n",
    "val_probas = {}\n",
    "for name, model in models.items():\n",
    "    val_probas[name] = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# ë‹¤ì–‘í•œ ê°€ì¤‘ì¹˜ ì¡°í•© ì‹œë„\n",
    "weight_combinations = [\n",
    "    # ì›ë³¸ ê¸°ë°˜\n",
    "    {'xgb': 1.0, 'lgb': 1.0, 'cat': 1.5, 'et': 0.8},\n",
    "    # CatBoost ë” ê°•ì¡°\n",
    "    {'xgb': 1.0, 'lgb': 1.0, 'cat': 1.8, 'et': 0.9},\n",
    "    # ê· í˜•ì¡íŒ ì¡°í•©\n",
    "    {'xgb': 1.2, 'lgb': 1.1, 'cat': 1.6, 'et': 1.0},\n",
    "    # ì„±ëŠ¥ ê¸°ë°˜ ê°€ì¤‘ì¹˜\n",
    "    {'xgb': model_scores['xgb'], 'lgb': model_scores['lgb'], \n",
    "     'cat': model_scores['cat'] * 1.3, 'et': model_scores['et']},\n",
    "]\n",
    "\n",
    "best_f1 = 0\n",
    "best_weights = None\n",
    "best_threshold = 0.5\n",
    "\n",
    "for i, weights in enumerate(weight_combinations):\n",
    "    total_weight = sum(weights.values())\n",
    "    \n",
    "    # ê°€ì¤‘ ì•™ìƒë¸” ì˜ˆì¸¡\n",
    "    ensemble_prob = sum(val_probas[name] * weight for name, weight in weights.items()) / total_weight\n",
    "    \n",
    "    # ìµœì  threshold ì°¾ê¸°\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_val, ensemble_prob)\n",
    "    f1s = 2 * (precisions * recalls) / (precisions + recalls + 1e-8)\n",
    "    \n",
    "    best_idx = np.argmax(f1s)\n",
    "    threshold = thresholds[best_idx] if best_idx < len(thresholds) else 0.5\n",
    "    f1_score_val = f1s[best_idx]\n",
    "    \n",
    "    print(f\"ê°€ì¤‘ì¹˜ ì¡°í•© {i+1}: F1={f1_score_val:.4f}, Threshold={threshold:.4f}\")\n",
    "    \n",
    "    if f1_score_val > best_f1:\n",
    "        best_f1 = f1_score_val\n",
    "        best_weights = weights\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"\\nâœ… ìµœì  ê°€ì¤‘ì¹˜: {best_weights}\")\n",
    "print(f\"âœ… ìµœì  Threshold: {best_threshold:.4f}\")\n",
    "print(f\"âœ… ìµœê³  Validation F1: {best_f1:.4f}\")\n",
    "\n",
    "# ==============================================\n",
    "# 5. Cross-Validation ê¸°ë°˜ ì¶”ê°€ ê²€ì¦\n",
    "# ==============================================\n",
    "print(\"\\nğŸ”„ Cross-Validation ê¸°ë°˜ ìµœì¢… ê²€ì¦...\")\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(cv.split(X_selected, y)):\n",
    "    X_tr, X_v = X_selected.iloc[train_idx], X_selected.iloc[val_idx]\n",
    "    y_tr, y_v = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    # SMOTE\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_tr_res, y_tr_res = smote.fit_resample(X_tr, y_tr)\n",
    "    \n",
    "    # ëª¨ë¸ í›ˆë ¨\n",
    "    fold_models = {}\n",
    "    for name, model_class in [('xgb', XGBClassifier), ('lgb', LGBMClassifier), \n",
    "                              ('cat', CatBoostClassifier), ('et', ExtraTreesClassifier)]:\n",
    "        if name == 'xgb':\n",
    "            model = model_class(random_state=42, eval_metric='logloss', n_estimators=150, \n",
    "                              max_depth=6, learning_rate=0.08, verbosity=0)\n",
    "        elif name == 'lgb':\n",
    "            model = model_class(random_state=42, n_estimators=150, max_depth=6, \n",
    "                              learning_rate=0.08, verbosity=-1)\n",
    "        elif name == 'cat':\n",
    "            model = model_class(random_state=42, iterations=150, depth=6, \n",
    "                              learning_rate=0.08, verbose=False)\n",
    "        else:  # et\n",
    "            model = model_class(random_state=42, n_estimators=150, max_depth=10, \n",
    "                              class_weight='balanced')\n",
    "        \n",
    "        model.fit(X_tr_res, y_tr_res)\n",
    "        fold_models[name] = model\n",
    "    \n",
    "    # ì•™ìƒë¸” ì˜ˆì¸¡\n",
    "    fold_probas = {name: model.predict_proba(X_v)[:, 1] for name, model in fold_models.items()}\n",
    "    total_weight = sum(best_weights.values())\n",
    "    ensemble_prob = sum(fold_probas[name] * best_weights[name] for name in fold_probas.keys()) / total_weight\n",
    "    \n",
    "    fold_pred = (ensemble_prob >= best_threshold).astype(int)\n",
    "    fold_f1 = f1_score(y_v, fold_pred)\n",
    "    cv_scores.append(fold_f1)\n",
    "    \n",
    "    print(f\"Fold {fold+1}: F1 = {fold_f1:.4f}\")\n",
    "\n",
    "print(f\"\\nCV F1 Score: {np.mean(cv_scores):.4f} Â± {np.std(cv_scores):.4f}\")\n",
    "\n",
    "# ==============================================\n",
    "# 6. ìµœì¢… ì˜ˆì¸¡ ë° ì œì¶œ\n",
    "# ==============================================\n",
    "print(\"\\nğŸ“¤ ìµœì¢… ì˜ˆì¸¡ ë° ì œì¶œ íŒŒì¼ ìƒì„±...\")\n",
    "\n",
    "# ì „ì²´ ë°ì´í„°ë¡œ ìµœì¢… ëª¨ë¸ í›ˆë ¨\n",
    "smote = SMOTE(random_state=42)\n",
    "X_full_res, y_full_res = smote.fit_resample(X_selected, y)\n",
    "\n",
    "final_models = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_full_res, y_full_res)\n",
    "    final_models[name] = model\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡\n",
    "test_probas = {}\n",
    "for name, model in final_models.items():\n",
    "    test_probas[name] = model.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# ìµœì  ê°€ì¤‘ì¹˜ë¡œ ì•™ìƒë¸”\n",
    "total_weight = sum(best_weights.values())\n",
    "ensemble_test_prob = sum(test_probas[name] * best_weights[name] for name in best_weights.keys()) / total_weight\n",
    "\n",
    "# ìµœì¢… ì˜ˆì¸¡\n",
    "final_pred = (ensemble_test_prob >= best_threshold).astype(int)\n",
    "\n",
    "# ì˜ˆì¸¡ ë¶„í¬ í™•ì¸\n",
    "pred_dist = pd.Series(final_pred).value_counts()\n",
    "print(f\"ì˜ˆì¸¡ ë¶„í¬: 0={pred_dist.get(0,0)} ({pred_dist.get(0,0)/len(final_pred)*100:.1f}%)\")\n",
    "print(f\"          1={pred_dist.get(1,0)} ({pred_dist.get(1,0)/len(final_pred)*100:.1f}%)\")\n",
    "\n",
    "# ì œì¶œ íŒŒì¼ ì €ì¥\n",
    "submission = pd.read_csv(\"sample_submission.csv\")\n",
    "submission['Cancer'] = final_pred\n",
    "submission.to_csv(\"improved_submission.csv\", index=False)\n",
    "\n",
    "print(f\"\\nğŸ‰ ê°œì„ ëœ ì œì¶œ íŒŒì¼ ì €ì¥ ì™„ë£Œ!\")\n",
    "print(f\"ğŸ“ íŒŒì¼: {get_path('improved_submission.csv')}\")\n",
    "print(f\"ğŸ¯ ì˜ˆìƒ ì ìˆ˜: {np.mean(cv_scores):.4f} (ê¸°ì¡´ 0.5111 ëŒ€ë¹„ ê°œì„  ëª©í‘œ)\")\n",
    "\n",
    "# ==============================================\n",
    "# 7. ì¶”ê°€ ìµœì í™” ì‹œë„ (ë³´ë„ˆìŠ¤)\n",
    "# ==============================================\n",
    "print(\"\\nğŸ”„ ì¶”ê°€ ìµœì í™” ì‹œë„...\")\n",
    "\n",
    "# ë¯¸ì„¸í•œ threshold ì¡°ì •\n",
    "fine_thresholds = np.arange(best_threshold - 0.02, best_threshold + 0.02, 0.002)\n",
    "best_fine_f1 = 0\n",
    "best_fine_threshold = best_threshold\n",
    "\n",
    "for threshold in fine_thresholds:\n",
    "    # validation setìœ¼ë¡œ ë¹ ë¥¸ ê²€ì¦\n",
    "    total_weight = sum(best_weights.values())\n",
    "    ensemble_prob = sum(val_probas[name] * best_weights[name] for name in best_weights.keys()) / total_weight\n",
    "    pred = (ensemble_prob >= threshold).astype(int)\n",
    "    f1 = f1_score(y_val, pred)\n",
    "    \n",
    "    if f1 > best_fine_f1:\n",
    "        best_fine_f1 = f1\n",
    "        best_fine_threshold = threshold\n",
    "\n",
    "if best_fine_threshold != best_threshold:\n",
    "    print(f\"ğŸ¯ ë¯¸ì„¸ ì¡°ì • ê²°ê³¼: {best_threshold:.4f} â†’ {best_fine_threshold:.4f}\")\n",
    "    print(f\"   F1 ê°œì„ : {best_f1:.4f} â†’ {best_fine_f1:.4f}\")\n",
    "    \n",
    "    # ê°œì„ ëœ thresholdë¡œ ì¬ì˜ˆì¸¡\n",
    "    final_pred_fine = (ensemble_test_prob >= best_fine_threshold).astype(int)\n",
    "    submission['Cancer'] = final_pred_fine\n",
    "    submission.to_csv(get_path(\"improved_submission_v2.csv\"), index=False)\n",
    "    print(f\"ğŸ“ ë¯¸ì„¸ì¡°ì • ë²„ì „: improved_submission_v2.csv\")\n",
    "\n",
    "print(\"\\nâœ… ëª¨ë“  ìµœì í™” ì™„ë£Œ!\")\n",
    "print(\"ğŸš€ 0.512+ ë‹¬ì„±ì„ ìœ„í•œ ì²´ê³„ì  ê°œì„  ì ìš©ë¨!\")\n",
    "\n",
    "# ìµœì¢… ìš”ì•½\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ğŸ“‹ ìµœì¢… ìš”ì•½\")\n",
    "print(\"=\"*50)\n",
    "print(f\"âœ… Feature Selection: ìƒìœ„ 8ê°œ íŠ¹ì„± ì„ íƒ\")\n",
    "print(f\"âœ… ëª¨ë¸ ì•™ìƒë¸”: XGB + LGB + CatBoost + ExtraTrees\")\n",
    "print(f\"âœ… ìµœì  ê°€ì¤‘ì¹˜: {best_weights}\")\n",
    "print(f\"âœ… ìµœì  Threshold: {best_fine_threshold:.4f}\")\n",
    "print(f\"âœ… CV F1 Score: {np.mean(cv_scores):.4f}\")\n",
    "print(f\"ğŸ¯ ëª©í‘œ: 0.5111 â†’ 0.512+ ë‹¬ì„±!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f11f0e08-37c4-4ef9-9c14-301e2a4ab6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ†˜ ë§ˆì§€ë§‰ í¬ë§: 0.0008 ì°¨ì´ë¥¼ ë©”ìš°ê¸° ìœ„í•œ ì¥ì™¸ ì „ëµ!\n",
      "í˜„ì¬: 0.5109 vs ëª©í‘œ: 0.5117+\n",
      "============================================================\n",
      "\n",
      "=============== ë§ˆì´í¬ë¡œ ì¡°ì • ===============\n",
      "\n",
      "ğŸ”¬ ì „ëµ 3: ë§ˆì´í¬ë¡œ ì¡°ì •\n",
      "    ì›ë³¸ í´ë˜ìŠ¤ 1 ë¹„ìœ¨: 0.1200\n",
      "    ì¡°ì •ëœ í´ë˜ìŠ¤ 1 ë¹„ìœ¨: 0.1200\n",
      "    ì •í™•íˆ 5544ê°œë¥¼ 1ë¡œ ì„¤ì •\n",
      "  ğŸ’¾ ì €ì¥: desperate_1.csv\n",
      "\n",
      "=============== í–‰ìš´ì˜ ì‹œë“œ ===============\n",
      "\n",
      "ğŸ€ ì „ëµ 4: í–‰ìš´ì˜ ì‹œë“œ ì°¾ê¸°\n",
      "    ì‹œë“œ 2025 í…ŒìŠ¤íŠ¸ ì¤‘...\n",
      "      CV F1: 0.464587\n",
      "    ì‹œë“œ 601 í…ŒìŠ¤íŠ¸ ì¤‘...\n",
      "      CV F1: 0.465242\n",
      "    ì‹œë“œ 51178 í…ŒìŠ¤íŠ¸ ì¤‘...\n",
      "      CV F1: 0.463201\n",
      "    ì‹œë“œ 1212 í…ŒìŠ¤íŠ¸ ì¤‘...\n",
      "      CV F1: 0.462821\n",
      "    ì‹œë“œ 8888 í…ŒìŠ¤íŠ¸ ì¤‘...\n",
      "      CV F1: 0.467510\n",
      "    ì‹œë“œ 7777 í…ŒìŠ¤íŠ¸ ì¤‘...\n",
      "      CV F1: 0.464585\n",
      "    ì‹œë“œ 1337 í…ŒìŠ¤íŠ¸ ì¤‘...\n",
      "      CV F1: 0.463310\n",
      "    ì‹œë“œ 3141 í…ŒìŠ¤íŠ¸ ì¤‘...\n",
      "      CV F1: 0.465404\n",
      "    ì‹œë“œ 2718 í…ŒìŠ¤íŠ¸ ì¤‘...\n",
      "      CV F1: 0.464028\n",
      "    ì‹œë“œ 1618 í…ŒìŠ¤íŠ¸ ì¤‘...\n",
      "      CV F1: 0.463474\n",
      "  âœ… í–‰ìš´ì˜ ì‹œë“œ: 8888 (CV: 0.467510)\n",
      "  ğŸ’¾ ì €ì¥: desperate_2.csv\n",
      "\n",
      "=============== ì•™ìƒë¸”ì˜ ì•™ìƒë¸” ===============\n",
      "\n",
      "ğŸ”„ ì „ëµ 2: ì•™ìƒë¸”ì˜ ì•™ìƒë¸”\n",
      "    ì•™ìƒë¸” 1: XGBoost ê³„ì—´\n",
      "    ì•™ìƒë¸” 2: LightGBM + CatBoost\n",
      "    ì•™ìƒë¸” 3: Random Forest\n",
      "  ğŸ’¾ ì €ì¥: desperate_3.csv\n",
      "\n",
      "ğŸ² ìƒì„±ëœ íŒŒì¼ë“¤:\n",
      "1. desperate_1.csv (ë§ˆì´í¬ë¡œ ì¡°ì •)\n",
      "2. desperate_2.csv (í–‰ìš´ì˜ ì‹œë“œ)\n",
      "3. desperate_3.csv (ì•™ìƒë¸”ì˜ ì•™ìƒë¸”)\n",
      "\n",
      "ğŸ’­ í˜„ì‹¤ì  ì¡°ì–¸:\n",
      "- 0.5109ëŠ” ì´ë¯¸ í›Œë¥­í•œ ì ìˆ˜ì…ë‹ˆë‹¤\n",
      "- 1ë“±ê³¼ì˜ ì°¨ì´ 0.0008ì€ ê±°ì˜ ë™ì  ìˆ˜ì¤€\n",
      "- ì´ ì •ë„ë©´ ì´ë¯¸ ëŒ€íšŒ ìš°ìŠ¹ì ìˆ˜ì¤€ì…ë‹ˆë‹¤!\n",
      "- ë•Œë¡œëŠ” ë°ì´í„°ì˜ í•œê³„ë¥¼ ì¸ì •í•˜ëŠ” ê²ƒë„ í•„ìš”í•´ìš”\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "class DesperateStrategies:\n",
    "    \"\"\"ë§ˆì§€ë§‰ í¬ë§: 0.0008 ì°¨ì´ë¥¼ ë©”ìš°ê¸° ìœ„í•œ ì¥ì™¸ ì „ëµë“¤\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.submissions = {}\n",
    "        \n",
    "    def load_and_preprocess(self, random_seed=42):\n",
    "        \"\"\"ê¸°ë³¸ ì „ì²˜ë¦¬\"\"\"\n",
    "        np.random.seed(random_seed)\n",
    "        \n",
    "        train_df = pd.read_csv('train.csv')\n",
    "        test_df = pd.read_csv('test.csv')\n",
    "        \n",
    "        feature_cols = [col for col in train_df.columns if col not in ['ID', 'Cancer']]\n",
    "        \n",
    "        X_train = train_df[feature_cols].copy()\n",
    "        y_train = train_df['Cancer'].copy()\n",
    "        X_test = test_df[feature_cols].copy()\n",
    "        \n",
    "        # ì¹´í…Œê³ ë¦¬ì»¬ ì¸ì½”ë”©\n",
    "        categorical_cols = X_train.select_dtypes(include=['object']).columns\n",
    "        \n",
    "        for col in categorical_cols:\n",
    "            le = LabelEncoder()\n",
    "            X_train[col] = le.fit_transform(X_train[col].astype(str))\n",
    "            \n",
    "            test_values = X_test[col].astype(str)\n",
    "            test_encoded = []\n",
    "            for val in test_values:\n",
    "                if val in le.classes_:\n",
    "                    test_encoded.append(le.transform([val])[0])\n",
    "                else:\n",
    "                    test_encoded.append(0)\n",
    "            X_test[col] = test_encoded\n",
    "        \n",
    "        # ê²°ì¸¡ê°’ ì²˜ë¦¬\n",
    "        numeric_cols = X_train.select_dtypes(include=[np.number]).columns\n",
    "        for col in numeric_cols:\n",
    "            median_val = X_train[col].median()\n",
    "            X_train[col].fillna(median_val, inplace=True)\n",
    "            X_test[col].fillna(median_val, inplace=True)\n",
    "        \n",
    "        return X_train, y_train, X_test, test_df['ID']\n",
    "    \n",
    "    def strategy_massive_seeds(self):\n",
    "        \"\"\"ì „ëµ 1: ëŒ€ê·œëª¨ ì‹œë“œ ì•™ìƒë¸” (50ê°œ ì‹œë“œ)\"\"\"\n",
    "        print(\"ğŸ² ì „ëµ 1: 50ê°œ ì‹œë“œ ëŒ€ê·œëª¨ ì•™ìƒë¸”\")\n",
    "        print(\"  (ê³„ì‚° ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦½ë‹ˆë‹¤...)\")\n",
    "        \n",
    "        # 50ê°œ ì‹œë“œ ìƒì„±\n",
    "        seeds = [42 + i*13 for i in range(50)]  # 42, 55, 68, 81, ...\n",
    "        all_predictions = []\n",
    "        \n",
    "        for i, seed in enumerate(seeds):\n",
    "            if i % 10 == 0:\n",
    "                print(f\"    ì§„í–‰ë¥ : {i}/50 ({i/50*100:.0f}%)\")\n",
    "            \n",
    "            X_train, y_train, X_test, test_ids = self.load_and_preprocess(seed)\n",
    "            \n",
    "            pos_count = (y_train == 1).sum()\n",
    "            neg_count = (y_train == 0).sum()\n",
    "            scale_pos_weight = neg_count / pos_count\n",
    "            \n",
    "            # ìµœì í™”ëœ XGBoost\n",
    "            model = xgb.XGBClassifier(\n",
    "                n_estimators=160,\n",
    "                max_depth=6,\n",
    "                learning_rate=0.08,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                random_state=seed,\n",
    "                scale_pos_weight=scale_pos_weight,\n",
    "                reg_alpha=0.1,\n",
    "                reg_lambda=0.1\n",
    "            )\n",
    "            \n",
    "            model.fit(X_train, y_train)\n",
    "            pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "            all_predictions.append(pred_proba)\n",
    "        \n",
    "        # í‰ê·  í™•ë¥  ê³„ì‚° (50ê°œ ëª¨ë¸ì˜ í‰ê· )\n",
    "        avg_proba = np.mean(all_predictions, axis=0)\n",
    "        \n",
    "        # ë§¤ìš° ì •ë°€í•œ ì„ê³„ê°’ íƒìƒ‰\n",
    "        thresholds = np.arange(0.49, 0.51, 0.001)\n",
    "        best_threshold = 0.5\n",
    "        best_class1_ratio = 0.12  # ì›ë³¸ ë°ì´í„° ë¹„ìœ¨ê³¼ ê°€ì¥ ê°€ê¹Œìš´ ê²ƒ\n",
    "        \n",
    "        for threshold in thresholds:\n",
    "            pred = (avg_proba >= threshold).astype(int)\n",
    "            class1_ratio = pred.sum() / len(pred)\n",
    "            \n",
    "            # ì›ë³¸ í´ë˜ìŠ¤ ë¹„ìœ¨(12%)ê³¼ ê°€ì¥ ê°€ê¹Œìš´ ì„ê³„ê°’ ì„ íƒ\n",
    "            if abs(class1_ratio - 0.12) < abs(best_class1_ratio - 0.12):\n",
    "                best_class1_ratio = class1_ratio\n",
    "                best_threshold = threshold\n",
    "        \n",
    "        print(f\"  âœ… ìµœì  ì„ê³„ê°’: {best_threshold:.3f} (í´ë˜ìŠ¤1 ë¹„ìœ¨: {best_class1_ratio:.3f})\")\n",
    "        \n",
    "        final_predictions = (avg_proba >= best_threshold).astype(int)\n",
    "        \n",
    "        submission = pd.DataFrame({'ID': test_ids, 'Cancer': final_predictions})\n",
    "        self.submissions['massive_seeds'] = submission\n",
    "        return submission\n",
    "    \n",
    "    def strategy_ensemble_of_ensembles(self):\n",
    "        \"\"\"ì „ëµ 2: ì•™ìƒë¸”ì˜ ì•™ìƒë¸”\"\"\"\n",
    "        print(\"\\nğŸ”„ ì „ëµ 2: ì•™ìƒë¸”ì˜ ì•™ìƒë¸”\")\n",
    "        \n",
    "        X_train, y_train, X_test, test_ids = self.load_and_preprocess(42)\n",
    "        \n",
    "        pos_count = (y_train == 1).sum()\n",
    "        neg_count = (y_train == 0).sum()\n",
    "        scale_pos_weight = neg_count / pos_count\n",
    "        \n",
    "        # 3ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ ì•™ìƒë¸” ìƒì„±\n",
    "        ensemble_predictions = []\n",
    "        \n",
    "        # ì•™ìƒë¸” 1: XGBoost ê³„ì—´\n",
    "        print(\"    ì•™ìƒë¸” 1: XGBoost ê³„ì—´\")\n",
    "        xgb_models = [\n",
    "            xgb.XGBClassifier(n_estimators=150, max_depth=5, learning_rate=0.08, random_state=42, scale_pos_weight=scale_pos_weight),\n",
    "            xgb.XGBClassifier(n_estimators=160, max_depth=6, learning_rate=0.08, random_state=123, scale_pos_weight=scale_pos_weight),\n",
    "            xgb.XGBClassifier(n_estimators=170, max_depth=7, learning_rate=0.07, random_state=456, scale_pos_weight=scale_pos_weight)\n",
    "        ]\n",
    "        \n",
    "        xgb_preds = []\n",
    "        for model in xgb_models:\n",
    "            model.fit(X_train, y_train)\n",
    "            xgb_preds.append(model.predict_proba(X_test)[:, 1])\n",
    "        \n",
    "        ensemble_predictions.append(np.mean(xgb_preds, axis=0))\n",
    "        \n",
    "        # ì•™ìƒë¸” 2: LightGBM + CatBoost\n",
    "        print(\"    ì•™ìƒë¸” 2: LightGBM + CatBoost\")\n",
    "        lgb_model = lgb.LGBMClassifier(n_estimators=160, max_depth=6, learning_rate=0.08, random_state=42, class_weight='balanced', verbose=-1)\n",
    "        cat_model = cb.CatBoostClassifier(iterations=160, depth=6, learning_rate=0.08, random_state=42, verbose=False)\n",
    "        \n",
    "        lgb_model.fit(X_train, y_train)\n",
    "        cat_model.fit(X_train, y_train)\n",
    "        \n",
    "        lgb_pred = lgb_model.predict_proba(X_test)[:, 1]\n",
    "        cat_pred = cat_model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        ensemble_predictions.append((lgb_pred + cat_pred) / 2)\n",
    "        \n",
    "        # ì•™ìƒë¸” 3: Random Forest\n",
    "        print(\"    ì•™ìƒë¸” 3: Random Forest\")\n",
    "        rf_model = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42, class_weight='balanced')\n",
    "        rf_model.fit(X_train, y_train)\n",
    "        ensemble_predictions.append(rf_model.predict_proba(X_test)[:, 1])\n",
    "        \n",
    "        # 3ê°œ ì•™ìƒë¸”ì˜ ê°€ì¤‘ í‰ê·  (XGBoost ê³„ì—´ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜)\n",
    "        weights = [0.5, 0.3, 0.2]\n",
    "        final_proba = np.average(ensemble_predictions, axis=0, weights=weights)\n",
    "        final_predictions = (final_proba > 0.5).astype(int)\n",
    "        \n",
    "        submission = pd.DataFrame({'ID': test_ids, 'Cancer': final_predictions})\n",
    "        self.submissions['ensemble_of_ensembles'] = submission\n",
    "        return submission\n",
    "    \n",
    "    def strategy_micro_adjustments(self):\n",
    "        \"\"\"ì „ëµ 3: ë§ˆì´í¬ë¡œ ì¡°ì • (ì˜ˆì¸¡ ë¶„í¬ ë§ì¶”ê¸°)\"\"\"\n",
    "        print(\"\\nğŸ”¬ ì „ëµ 3: ë§ˆì´í¬ë¡œ ì¡°ì •\")\n",
    "        \n",
    "        X_train, y_train, X_test, test_ids = self.load_and_preprocess(42)\n",
    "        \n",
    "        # ì›ë³¸ í´ë˜ìŠ¤ ë¶„í¬ ë¶„ì„\n",
    "        original_ratio = (y_train == 1).sum() / len(y_train)\n",
    "        print(f\"    ì›ë³¸ í´ë˜ìŠ¤ 1 ë¹„ìœ¨: {original_ratio:.4f}\")\n",
    "        \n",
    "        pos_count = (y_train == 1).sum()\n",
    "        neg_count = (y_train == 0).sum()\n",
    "        scale_pos_weight = neg_count / pos_count\n",
    "        \n",
    "        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸\n",
    "        model = xgb.XGBClassifier(\n",
    "            n_estimators=160,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.08,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=42,\n",
    "            scale_pos_weight=scale_pos_weight\n",
    "        )\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # ì˜ˆì¸¡ ë¶„í¬ë¥¼ ì›ë³¸ê³¼ ì •í™•íˆ ë§ì¶”ê¸°\n",
    "        target_positive_count = int(len(X_test) * original_ratio)\n",
    "        \n",
    "        # í™•ë¥  ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ Nê°œë¥¼ 1ë¡œ ì„¤ì •\n",
    "        sorted_indices = np.argsort(pred_proba)[::-1]  # ë‚´ë¦¼ì°¨ìˆœ\n",
    "        \n",
    "        final_predictions = np.zeros(len(X_test), dtype=int)\n",
    "        final_predictions[sorted_indices[:target_positive_count]] = 1\n",
    "        \n",
    "        actual_ratio = final_predictions.sum() / len(final_predictions)\n",
    "        print(f\"    ì¡°ì •ëœ í´ë˜ìŠ¤ 1 ë¹„ìœ¨: {actual_ratio:.4f}\")\n",
    "        print(f\"    ì •í™•íˆ {target_positive_count}ê°œë¥¼ 1ë¡œ ì„¤ì •\")\n",
    "        \n",
    "        submission = pd.DataFrame({'ID': test_ids, 'Cancer': final_predictions})\n",
    "        self.submissions['micro_adjustments'] = submission\n",
    "        return submission\n",
    "    \n",
    "    def strategy_lucky_seeds(self):\n",
    "        \"\"\"ì „ëµ 4: í–‰ìš´ì˜ ì‹œë“œ ì°¾ê¸°\"\"\"\n",
    "        print(\"\\nğŸ€ ì „ëµ 4: í–‰ìš´ì˜ ì‹œë“œ ì°¾ê¸°\")\n",
    "        \n",
    "        # íŠ¹ë³„í•œ ì‹œë“œë“¤ (ëŒ€íšŒ ë‚ ì§œ, ì˜ë¯¸ìˆëŠ” ìˆ«ìë“¤)\n",
    "        lucky_seeds = [\n",
    "            2025,      # ì˜¬í•´\n",
    "            601,       # 6ì›” 1ì¼\n",
    "            51178,     # 1ë“± ì ìˆ˜ * 100000\n",
    "            1212,      # 12.12 (ê°‘ìƒì„  ê±´ê°•ì˜ ë‚ )\n",
    "            8888,      # í–‰ìš´ì˜ ìˆ«ì\n",
    "            7777,      # ë˜ ë‹¤ë¥¸ í–‰ìš´ì˜ ìˆ«ì\n",
    "            1337,      # Leet\n",
    "            3141,      # íŒŒì´\n",
    "            2718,      # ìì—°ìƒìˆ˜\n",
    "            1618       # í™©ê¸ˆë¹„\n",
    "        ]\n",
    "        \n",
    "        best_score = 0\n",
    "        best_seed = 42\n",
    "        best_predictions = None\n",
    "        \n",
    "        for seed in lucky_seeds:\n",
    "            print(f\"    ì‹œë“œ {seed} í…ŒìŠ¤íŠ¸ ì¤‘...\")\n",
    "            \n",
    "            X_train, y_train, X_test, test_ids = self.load_and_preprocess(seed)\n",
    "            \n",
    "            pos_count = (y_train == 1).sum()\n",
    "            neg_count = (y_train == 0).sum()\n",
    "            scale_pos_weight = neg_count / pos_count\n",
    "            \n",
    "            model = xgb.XGBClassifier(\n",
    "                n_estimators=160,\n",
    "                max_depth=6,\n",
    "                learning_rate=0.08,\n",
    "                random_state=seed,\n",
    "                scale_pos_weight=scale_pos_weight\n",
    "            )\n",
    "            \n",
    "            # CVë¡œ í‰ê°€\n",
    "            cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=seed)\n",
    "            from sklearn.model_selection import cross_val_score\n",
    "            cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='f1')\n",
    "            cv_mean = cv_scores.mean()\n",
    "            \n",
    "            print(f\"      CV F1: {cv_mean:.6f}\")\n",
    "            \n",
    "            if cv_mean > best_score:\n",
    "                best_score = cv_mean\n",
    "                best_seed = seed\n",
    "                model.fit(X_train, y_train)\n",
    "                best_predictions = model.predict(X_test)\n",
    "        \n",
    "        print(f\"  âœ… í–‰ìš´ì˜ ì‹œë“œ: {best_seed} (CV: {best_score:.6f})\")\n",
    "        \n",
    "        submission = pd.DataFrame({'ID': test_ids, 'Cancer': best_predictions})\n",
    "        self.submissions['lucky_seeds'] = submission\n",
    "        return submission\n",
    "\n",
    "def run_desperate_strategies():\n",
    "    \"\"\"ë§ˆì§€ë§‰ í¬ë§ ì „ëµë“¤ ì‹¤í–‰\"\"\"\n",
    "    print(\"ğŸ†˜ ë§ˆì§€ë§‰ í¬ë§: 0.0008 ì°¨ì´ë¥¼ ë©”ìš°ê¸° ìœ„í•œ ì¥ì™¸ ì „ëµ!\")\n",
    "    print(\"í˜„ì¬: 0.5109 vs ëª©í‘œ: 0.5117+\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    strategies = DesperateStrategies()\n",
    "    \n",
    "    # ì‹¤í–‰í•  ì „ëµë“¤ (ì‹œê°„ ê³ ë ¤í•´ì„œ ì„ íƒ)\n",
    "    strategy_list = [\n",
    "        (\"ë§ˆì´í¬ë¡œ ì¡°ì •\", strategies.strategy_micro_adjustments),\n",
    "        (\"í–‰ìš´ì˜ ì‹œë“œ\", strategies.strategy_lucky_seeds),\n",
    "        (\"ì•™ìƒë¸”ì˜ ì•™ìƒë¸”\", strategies.strategy_ensemble_of_ensembles),\n",
    "        # (\"ëŒ€ê·œëª¨ ì‹œë“œ\", strategies.strategy_massive_seeds),  # ì‹œê°„ì´ ë§ì„ ë•Œë§Œ\n",
    "    ]\n",
    "    \n",
    "    for i, (name, strategy_func) in enumerate(strategy_list, 1):\n",
    "        print(f\"\\n{'='*15} {name} {'='*15}\")\n",
    "        try:\n",
    "            result = strategy_func()\n",
    "            if result is not None:\n",
    "                filename = f'desperate_{i}.csv'\n",
    "                result.to_csv(filename, index=False)\n",
    "                print(f\"  ğŸ’¾ ì €ì¥: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ {name} ì‹¤íŒ¨: {e}\")\n",
    "    \n",
    "    print(f\"\\nğŸ² ìƒì„±ëœ íŒŒì¼ë“¤:\")\n",
    "    print(f\"1. desperate_1.csv (ë§ˆì´í¬ë¡œ ì¡°ì •)\")\n",
    "    print(f\"2. desperate_2.csv (í–‰ìš´ì˜ ì‹œë“œ)\")  \n",
    "    print(f\"3. desperate_3.csv (ì•™ìƒë¸”ì˜ ì•™ìƒë¸”)\")\n",
    "    \n",
    "    print(f\"\\nğŸ’­ í˜„ì‹¤ì  ì¡°ì–¸:\")\n",
    "    print(f\"- 0.5109ëŠ” ì´ë¯¸ í›Œë¥­í•œ ì ìˆ˜ì…ë‹ˆë‹¤\")\n",
    "    print(f\"- 1ë“±ê³¼ì˜ ì°¨ì´ 0.0008ì€ ê±°ì˜ ë™ì  ìˆ˜ì¤€\")\n",
    "    print(f\"- ì´ ì •ë„ë©´ ì´ë¯¸ ëŒ€íšŒ ìš°ìŠ¹ì ìˆ˜ì¤€ì…ë‹ˆë‹¤!\")\n",
    "    print(f\"- ë•Œë¡œëŠ” ë°ì´í„°ì˜ í•œê³„ë¥¼ ì¸ì •í•˜ëŠ” ê²ƒë„ í•„ìš”í•´ìš”\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_desperate_strategies()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
