{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1dc4468-8961-4ae4-a009-b36a0a7d89aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:256: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:257: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:256: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:257: SyntaxWarning: invalid escape sequence '\\d'\n",
      "/tmp/ipykernel_1165/3548927000.py:256: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  train_id_nums = train_ids.str.extract('(\\d+)').astype(float)\n",
      "/tmp/ipykernel_1165/3548927000.py:257: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  test_id_nums = test_ids.str.extract('(\\d+)').astype(float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 마지막 돌파 시도: 데이터 레벨 최적화!\n",
      "목표: 0.5109 → 0.512+ (1등 탈환!)\n",
      "============================================================\n",
      "\n",
      "🔍 데이터 누수 및 패턴 조사...\n",
      "  ID 패턴 분석...\n",
      "    Train ID 범위: 0 ~ 87158\n",
      "    Test ID 범위: 0 ~ 46203\n",
      "\n",
      "  Train vs Test 분포 비교...\n",
      "\n",
      "==================== power_transform ====================\n",
      "🔧 고급 전처리 전략: power_transform\n",
      "  Power Transform 적용...\n",
      "\n",
      "⚙️ 초정밀 하이퍼파라미터 튜닝...\n",
      "  그리드 서치 실행 중... (시간이 걸립니다)\n",
      "Fitting 3 folds for each of 900 candidates, totalling 2700 fits\n",
      "  ✅ 최적 파라미터: {'colsample_bytree': 0.8, 'learning_rate': 0.07, 'max_depth': 5, 'n_estimators': 140, 'subsample': 0.85}\n",
      "  ✅ 최적 CV F1: 0.474495\n",
      "\n",
      "🏗️ 최적 파라미터 기반 고급 스태킹...\n",
      "  Level 1: 5개 최적화된 모델\n",
      "    xgb_optimal 처리 중...\n",
      "    xgb_variant1 처리 중...\n",
      "    xgb_variant2 처리 중...\n",
      "    lgb_optimal 처리 중...\n",
      "    cat_optimal 처리 중...\n",
      "  ✅ 최고 메타 모델 CV F1: 0.486957\n",
      "\n",
      "==================== quantile_transform ====================\n",
      "🔧 고급 전처리 전략: quantile_transform\n",
      "  Quantile Transform 적용...\n",
      "\n",
      "⚙️ 초정밀 하이퍼파라미터 튜닝...\n",
      "  그리드 서치 실행 중... (시간이 걸립니다)\n",
      "Fitting 3 folds for each of 900 candidates, totalling 2700 fits\n",
      "  ✅ 최적 파라미터: {'colsample_bytree': 0.8, 'learning_rate': 0.07, 'max_depth': 5, 'n_estimators': 140, 'subsample': 0.85}\n",
      "  ✅ 최적 CV F1: 0.474495\n",
      "\n",
      "🏗️ 최적 파라미터 기반 고급 스태킹...\n",
      "  Level 1: 5개 최적화된 모델\n",
      "    xgb_optimal 처리 중...\n",
      "    xgb_variant1 처리 중...\n",
      "    xgb_variant2 처리 중...\n",
      "    lgb_optimal 처리 중...\n",
      "    cat_optimal 처리 중...\n",
      "  ✅ 최고 메타 모델 CV F1: 0.486957\n",
      "\n",
      "==================== robust_scaling ====================\n",
      "🔧 고급 전처리 전략: robust_scaling\n",
      "  Robust Scaling 적용...\n",
      "\n",
      "⚙️ 초정밀 하이퍼파라미터 튜닝...\n",
      "  그리드 서치 실행 중... (시간이 걸립니다)\n",
      "Fitting 3 folds for each of 900 candidates, totalling 2700 fits\n",
      "  ✅ 최적 파라미터: {'colsample_bytree': 0.8, 'learning_rate': 0.07, 'max_depth': 5, 'n_estimators': 140, 'subsample': 0.85}\n",
      "  ✅ 최적 CV F1: 0.474495\n",
      "\n",
      "🏗️ 최적 파라미터 기반 고급 스태킹...\n",
      "  Level 1: 5개 최적화된 모델\n",
      "    xgb_optimal 처리 중...\n",
      "    xgb_variant1 처리 중...\n",
      "    xgb_variant2 처리 중...\n",
      "    lgb_optimal 처리 중...\n",
      "    cat_optimal 처리 중...\n",
      "  ✅ 최고 메타 모델 CV F1: 0.486957\n",
      "\n",
      "🎯 모든 전략 최종 앙상블...\n",
      "  전략: precision_tuned\n",
      "  전략: advanced_stacking\n",
      "\n",
      "🎯 생성된 파일들:\n",
      "1. FINAL_BREAKTHROUGH.csv ⭐ (최종 돌파 시도)\n",
      "2. breakthrough_stacking_*.csv (각 전처리별 스태킹)\n",
      "3. breakthrough_tuned_*.csv (각 전처리별 튜닝)\n",
      "\n",
      "💡 이번 시도의 핵심:\n",
      "- 고급 전처리로 데이터 품질 향상\n",
      "- 초정밀 하이퍼파라미터 튜닝\n",
      "- 데이터 누수 가능성 조사\n",
      "- 최적화된 스태킹 앙상별\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, PowerTransformer, QuantileTransformer\n",
    "from sklearn.metrics import f1_score\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class FinalBreakthroughOptimizer:\n",
    "    \"\"\"마지막 돌파를 위한 데이터 레벨 최적화\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.submissions = {}\n",
    "        self.transformers = {}\n",
    "        \n",
    "    def load_and_advanced_preprocess(self, strategy='standard'):\n",
    "        \"\"\"고급 전처리 전략들\"\"\"\n",
    "        print(f\"🔧 고급 전처리 전략: {strategy}\")\n",
    "        \n",
    "        train_df = pd.read_csv('train.csv')\n",
    "        test_df = pd.read_csv('test.csv')\n",
    "        \n",
    "        feature_cols = [col for col in train_df.columns if col not in ['ID', 'Cancer']]\n",
    "        \n",
    "        X_train = train_df[feature_cols].copy()\n",
    "        y_train = train_df['Cancer'].copy()\n",
    "        X_test = test_df[feature_cols].copy()\n",
    "        \n",
    "        # 카테고리컬 인코딩\n",
    "        categorical_cols = X_train.select_dtypes(include=['object']).columns\n",
    "        \n",
    "        for col in categorical_cols:\n",
    "            le = LabelEncoder()\n",
    "            X_train[col] = le.fit_transform(X_train[col].astype(str))\n",
    "            \n",
    "            test_values = X_test[col].astype(str)\n",
    "            test_encoded = []\n",
    "            for val in test_values:\n",
    "                if val in le.classes_:\n",
    "                    test_encoded.append(le.transform([val])[0])\n",
    "                else:\n",
    "                    test_encoded.append(0)\n",
    "            X_test[col] = test_encoded\n",
    "        \n",
    "        # 결측값 처리\n",
    "        numeric_cols = X_train.select_dtypes(include=[np.number]).columns\n",
    "        for col in numeric_cols:\n",
    "            median_val = X_train[col].median()\n",
    "            X_train[col].fillna(median_val, inplace=True)\n",
    "            X_test[col].fillna(median_val, inplace=True)\n",
    "        \n",
    "        # 고급 전처리 전략 적용\n",
    "        if strategy == 'power_transform':\n",
    "            # Power Transform (Yeo-Johnson)\n",
    "            print(\"  Power Transform 적용...\")\n",
    "            pt = PowerTransformer(method='yeo-johnson', standardize=True)\n",
    "            X_train[numeric_cols] = pt.fit_transform(X_train[numeric_cols])\n",
    "            X_test[numeric_cols] = pt.transform(X_test[numeric_cols])\n",
    "            \n",
    "        elif strategy == 'quantile_transform':\n",
    "            # Quantile Transform (Uniform distribution)\n",
    "            print(\"  Quantile Transform 적용...\")\n",
    "            qt = QuantileTransformer(n_quantiles=1000, output_distribution='uniform', random_state=42)\n",
    "            X_train[numeric_cols] = qt.fit_transform(X_train[numeric_cols])\n",
    "            X_test[numeric_cols] = qt.transform(X_test[numeric_cols])\n",
    "            \n",
    "        elif strategy == 'robust_scaling':\n",
    "            # Robust Scaling (이상치에 강함)\n",
    "            print(\"  Robust Scaling 적용...\")\n",
    "            from sklearn.preprocessing import RobustScaler\n",
    "            rs = RobustScaler()\n",
    "            X_train[numeric_cols] = rs.fit_transform(X_train[numeric_cols])\n",
    "            X_test[numeric_cols] = rs.transform(X_test[numeric_cols])\n",
    "            \n",
    "        elif strategy == 'log_transform':\n",
    "            # Log Transform\n",
    "            print(\"  Log Transform 적용...\")\n",
    "            for col in numeric_cols:\n",
    "                if X_train[col].min() > 0:  # 양수만 가능\n",
    "                    X_train[col] = np.log1p(X_train[col])\n",
    "                    X_test[col] = np.log1p(X_test[col])\n",
    "            \n",
    "            # 그 후 StandardScaling\n",
    "            scaler = StandardScaler()\n",
    "            X_train[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])\n",
    "            X_test[numeric_cols] = scaler.transform(X_test[numeric_cols])\n",
    "            \n",
    "        else:  # standard\n",
    "            # 기본 StandardScaling\n",
    "            scaler = StandardScaler()\n",
    "            X_train[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])\n",
    "            X_test[numeric_cols] = scaler.transform(X_test[numeric_cols])\n",
    "        \n",
    "        return X_train, y_train, X_test, test_df['ID']\n",
    "    \n",
    "    def hyperparameter_precision_tuning(self, X_train, y_train, X_test, test_ids):\n",
    "        \"\"\"초정밀 하이퍼파라미터 튜닝\"\"\"\n",
    "        print(\"\\n⚙️ 초정밀 하이퍼파라미터 튜닝...\")\n",
    "        \n",
    "        pos_count = (y_train == 1).sum()\n",
    "        neg_count = (y_train == 0).sum()\n",
    "        scale_pos_weight = neg_count / pos_count\n",
    "        \n",
    "        # 매우 세밀한 파라미터 그리드\n",
    "        param_grid = {\n",
    "            'n_estimators': [140, 150, 160, 170, 180],\n",
    "            'max_depth': [5, 6, 7],\n",
    "            'learning_rate': [0.07, 0.075, 0.08, 0.085, 0.09],\n",
    "            'subsample': [0.75, 0.8, 0.85],\n",
    "            'colsample_bytree': [0.75, 0.8, 0.85, 0.9]\n",
    "        }\n",
    "        \n",
    "        base_model = xgb.XGBClassifier(\n",
    "            random_state=42,\n",
    "            scale_pos_weight=scale_pos_weight,\n",
    "            reg_alpha=0.1,\n",
    "            reg_lambda=0.1,\n",
    "            eval_metric='logloss'\n",
    "        )\n",
    "        \n",
    "        # 3-fold로 빠른 탐색\n",
    "        cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        \n",
    "        grid_search = GridSearchCV(\n",
    "            base_model, \n",
    "            param_grid,\n",
    "            cv=cv,\n",
    "            scoring='f1',\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        print(\"  그리드 서치 실행 중... (시간이 걸립니다)\")\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        print(f\"  ✅ 최적 파라미터: {grid_search.best_params_}\")\n",
    "        print(f\"  ✅ 최적 CV F1: {grid_search.best_score_:.6f}\")\n",
    "        \n",
    "        # 최적 모델로 예측\n",
    "        best_model = grid_search.best_estimator_\n",
    "        predictions = best_model.predict(X_test)\n",
    "        \n",
    "        submission = pd.DataFrame({'ID': test_ids, 'Cancer': predictions})\n",
    "        self.submissions['precision_tuned'] = submission\n",
    "        \n",
    "        return submission, grid_search.best_params_\n",
    "    \n",
    "    def advanced_stacking_with_best_params(self, X_train, y_train, X_test, test_ids, best_params):\n",
    "        \"\"\"최적 파라미터로 고급 스태킹\"\"\"\n",
    "        print(\"\\n🏗️ 최적 파라미터 기반 고급 스태킹...\")\n",
    "        \n",
    "        pos_count = (y_train == 1).sum()\n",
    "        neg_count = (y_train == 0).sum()\n",
    "        scale_pos_weight = neg_count / pos_count\n",
    "        \n",
    "        # Level 1: 최적화된 다양한 모델들\n",
    "        models = {\n",
    "            'xgb_optimal': xgb.XGBClassifier(**best_params, random_state=42, scale_pos_weight=scale_pos_weight, reg_alpha=0.1, reg_lambda=0.1),\n",
    "            'xgb_variant1': xgb.XGBClassifier(**best_params, random_state=123, scale_pos_weight=scale_pos_weight, reg_alpha=0.05, reg_lambda=0.15),\n",
    "            'xgb_variant2': xgb.XGBClassifier(**best_params, random_state=456, scale_pos_weight=scale_pos_weight, reg_alpha=0.15, reg_lambda=0.05),\n",
    "            \n",
    "            'lgb_optimal': lgb.LGBMClassifier(\n",
    "                n_estimators=best_params['n_estimators'],\n",
    "                max_depth=best_params['max_depth'], \n",
    "                learning_rate=best_params['learning_rate'],\n",
    "                subsample=best_params['subsample'],\n",
    "                colsample_bytree=best_params['colsample_bytree'],\n",
    "                random_state=42, class_weight='balanced', verbose=-1\n",
    "            ),\n",
    "            \n",
    "            'cat_optimal': cb.CatBoostClassifier(\n",
    "                iterations=best_params['n_estimators'],\n",
    "                depth=best_params['max_depth'],\n",
    "                learning_rate=best_params['learning_rate'],\n",
    "                subsample=best_params['subsample'],\n",
    "                random_state=42, verbose=False\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        print(f\"  Level 1: {len(models)}개 최적화된 모델\")\n",
    "        \n",
    "        # 7-fold Cross-validation\n",
    "        cv = StratifiedKFold(n_splits=7, shuffle=True, random_state=42)\n",
    "        oof_predictions = np.zeros((len(X_train), len(models)))\n",
    "        test_predictions = np.zeros((len(X_test), len(models)))\n",
    "        \n",
    "        for i, (name, model) in enumerate(models.items()):\n",
    "            print(f\"    {name} 처리 중...\")\n",
    "            \n",
    "            oof_pred = np.zeros(len(X_train))\n",
    "            \n",
    "            for train_idx, val_idx in cv.split(X_train, y_train):\n",
    "                X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "                y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "                \n",
    "                model.fit(X_tr, y_tr)\n",
    "                oof_pred[val_idx] = model.predict_proba(X_val)[:, 1]\n",
    "            \n",
    "            oof_predictions[:, i] = oof_pred\n",
    "            \n",
    "            # 전체 데이터로 재학습\n",
    "            model.fit(X_train, y_train)\n",
    "            test_predictions[:, i] = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # Level 2: 정밀 조정된 메타 모델\n",
    "        meta_models = [\n",
    "            LogisticRegression(random_state=42, class_weight='balanced', C=0.1),\n",
    "            LogisticRegression(random_state=42, class_weight='balanced', C=1.0),\n",
    "            LogisticRegression(random_state=42, class_weight='balanced', C=10.0)\n",
    "        ]\n",
    "        \n",
    "        best_meta_score = 0\n",
    "        best_meta_model = None\n",
    "        \n",
    "        cv_meta = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        \n",
    "        for meta_model in meta_models:\n",
    "            scores = cross_val_score(meta_model, oof_predictions, y_train, cv=cv_meta, scoring='f1')\n",
    "            score = scores.mean()\n",
    "            \n",
    "            if score > best_meta_score:\n",
    "                best_meta_score = score\n",
    "                best_meta_model = meta_model\n",
    "        \n",
    "        print(f\"  ✅ 최고 메타 모델 CV F1: {best_meta_score:.6f}\")\n",
    "        \n",
    "        # 최종 예측\n",
    "        best_meta_model.fit(oof_predictions, y_train)\n",
    "        final_proba = best_meta_model.predict_proba(test_predictions)[:, 1]\n",
    "        final_predictions = (final_proba > 0.5).astype(int)\n",
    "        \n",
    "        submission = pd.DataFrame({'ID': test_ids, 'Cancer': final_predictions})\n",
    "        self.submissions['advanced_stacking'] = submission\n",
    "        \n",
    "        return submission\n",
    "    \n",
    "    def data_leakage_investigation(self):\n",
    "        \"\"\"데이터 누수 조사\"\"\"\n",
    "        print(\"\\n🔍 데이터 누수 및 패턴 조사...\")\n",
    "        \n",
    "        train_df = pd.read_csv('train.csv')\n",
    "        test_df = pd.read_csv('test.csv')\n",
    "        \n",
    "        # ID 패턴 분석\n",
    "        print(\"  ID 패턴 분석...\")\n",
    "        train_ids = train_df['ID'].astype(str)\n",
    "        test_ids = test_df['ID'].astype(str)\n",
    "        \n",
    "        # ID에서 숫자 추출하여 패턴 확인\n",
    "        train_id_nums = train_ids.str.extract('(\\d+)').astype(float)\n",
    "        test_id_nums = test_ids.str.extract('(\\d+)').astype(float)\n",
    "        \n",
    "        print(f\"    Train ID 범위: {train_id_nums.min().values[0]:.0f} ~ {train_id_nums.max().values[0]:.0f}\")\n",
    "        print(f\"    Test ID 범위: {test_id_nums.min().values[0]:.0f} ~ {test_id_nums.max().values[0]:.0f}\")\n",
    "        \n",
    "        # 특성 분포 비교\n",
    "        print(\"\\n  Train vs Test 분포 비교...\")\n",
    "        feature_cols = [col for col in train_df.columns if col not in ['ID', 'Cancer']]\n",
    "        \n",
    "        for col in feature_cols:\n",
    "            if train_df[col].dtype in ['int64', 'float64']:\n",
    "                # 수치형 변수\n",
    "                train_mean = train_df[col].mean()\n",
    "                test_mean = test_df[col].mean()\n",
    "                diff_pct = abs(train_mean - test_mean) / train_mean * 100\n",
    "                \n",
    "                if diff_pct > 5:  # 5% 이상 차이\n",
    "                    print(f\"    ⚠️  {col}: Train {train_mean:.3f} vs Test {test_mean:.3f} ({diff_pct:.1f}% 차이)\")\n",
    "            else:\n",
    "                # 카테고리컬 변수\n",
    "                train_unique = set(train_df[col].unique())\n",
    "                test_unique = set(test_df[col].unique())\n",
    "                \n",
    "                only_in_test = test_unique - train_unique\n",
    "                if only_in_test:\n",
    "                    print(f\"    ⚠️  {col}: 테스트에만 있는 값 {len(only_in_test)}개\")\n",
    "    \n",
    "    def ensemble_all_strategies(self):\n",
    "        \"\"\"모든 전략의 최종 앙상별\"\"\"\n",
    "        print(\"\\n🎯 모든 전략 최종 앙상블...\")\n",
    "        \n",
    "        if len(self.submissions) < 2:\n",
    "            print(\"  ❌ 충분한 전략이 실행되지 않음\")\n",
    "            return None\n",
    "        \n",
    "        # 모든 예측 수집\n",
    "        all_predictions = []\n",
    "        strategy_names = []\n",
    "        \n",
    "        for name, submission in self.submissions.items():\n",
    "            all_predictions.append(submission['Cancer'].values)\n",
    "            strategy_names.append(name)\n",
    "            print(f\"  전략: {name}\")\n",
    "        \n",
    "        # 단순 다수결\n",
    "        all_predictions = np.array(all_predictions)\n",
    "        majority_vote = np.round(np.mean(all_predictions, axis=0)).astype(int)\n",
    "        \n",
    "        # 가중 평균 (최신 전략에 높은 가중치)\n",
    "        weights = np.linspace(0.8, 1.2, len(strategy_names))  # 최신일수록 높은 가중치\n",
    "        weighted_avg = np.average(all_predictions, axis=0, weights=weights)\n",
    "        weighted_predictions = np.round(weighted_avg).astype(int)\n",
    "        \n",
    "        # 두 가지 앙상블 결과\n",
    "        test_ids = list(self.submissions.values())[0]['ID']\n",
    "        \n",
    "        majority_submission = pd.DataFrame({'ID': test_ids, 'Cancer': majority_vote})\n",
    "        weighted_submission = pd.DataFrame({'ID': test_ids, 'Cancer': weighted_predictions})\n",
    "        \n",
    "        self.submissions['final_majority'] = majority_submission\n",
    "        self.submissions['final_weighted'] = weighted_submission\n",
    "        \n",
    "        return weighted_submission  # 가중 평균을 기본으로 반환\n",
    "\n",
    "def run_final_breakthrough():\n",
    "    \"\"\"마지막 돌파 시도 실행\"\"\"\n",
    "    print(\"🚀 마지막 돌파 시도: 데이터 레벨 최적화!\")\n",
    "    print(\"목표: 0.5109 → 0.512+ (1등 탈환!)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    optimizer = FinalBreakthroughOptimizer()\n",
    "    \n",
    "    # 1. 데이터 누수 조사\n",
    "    optimizer.data_leakage_investigation()\n",
    "    \n",
    "    # 2. 다양한 전처리 전략 시도\n",
    "    strategies = ['power_transform', 'quantile_transform', 'robust_scaling']\n",
    "    \n",
    "    for strategy in strategies:\n",
    "        print(f\"\\n{'='*20} {strategy} {'='*20}\")\n",
    "        try:\n",
    "            X_train, y_train, X_test, test_ids = optimizer.load_and_advanced_preprocess(strategy)\n",
    "            \n",
    "            # 초정밀 하이퍼파라미터 튜닝\n",
    "            tuned_submission, best_params = optimizer.hyperparameter_precision_tuning(\n",
    "                X_train, y_train, X_test, test_ids\n",
    "            )\n",
    "            \n",
    "            # 최적 파라미터로 고급 스태킹\n",
    "            stacking_submission = optimizer.advanced_stacking_with_best_params(\n",
    "                X_train, y_train, X_test, test_ids, best_params\n",
    "            )\n",
    "            \n",
    "            # 파일 저장\n",
    "            tuned_submission.to_csv(f'breakthrough_tuned_{strategy}.csv', index=False)\n",
    "            stacking_submission.to_csv(f'breakthrough_stacking_{strategy}.csv', index=False)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ {strategy} 실패: {e}\")\n",
    "    \n",
    "    # 3. 최종 앙상별\n",
    "    final_submission = optimizer.ensemble_all_strategies()\n",
    "    if final_submission is not None:\n",
    "        final_submission.to_csv('FINAL_BREAKTHROUGH.csv', index=False)\n",
    "    \n",
    "    print(f\"\\n🎯 생성된 파일들:\")\n",
    "    print(f\"1. FINAL_BREAKTHROUGH.csv ⭐ (최종 돌파 시도)\")\n",
    "    print(f\"2. breakthrough_stacking_*.csv (각 전처리별 스태킹)\")\n",
    "    print(f\"3. breakthrough_tuned_*.csv (각 전처리별 튜닝)\")\n",
    "    \n",
    "    print(f\"\\n💡 이번 시도의 핵심:\")\n",
    "    print(f\"- 고급 전처리로 데이터 품질 향상\")\n",
    "    print(f\"- 초정밀 하이퍼파라미터 튜닝\")\n",
    "    print(f\"- 데이터 누수 가능성 조사\")\n",
    "    print(f\"- 최적화된 스태킹 앙상별\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_final_breakthrough()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
