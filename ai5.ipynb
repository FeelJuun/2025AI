{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1dc4468-8961-4ae4-a009-b36a0a7d89aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:256: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:257: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:256: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:257: SyntaxWarning: invalid escape sequence '\\d'\n",
      "/tmp/ipykernel_1165/3548927000.py:256: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  train_id_nums = train_ids.str.extract('(\\d+)').astype(float)\n",
      "/tmp/ipykernel_1165/3548927000.py:257: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  test_id_nums = test_ids.str.extract('(\\d+)').astype(float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ë§ˆì§€ë§‰ ëŒíŒŒ ì‹œë„: ë°ì´í„° ë ˆë²¨ ìµœì í™”!\n",
      "ëª©í‘œ: 0.5109 â†’ 0.512+ (1ë“± íƒˆí™˜!)\n",
      "============================================================\n",
      "\n",
      "ğŸ” ë°ì´í„° ëˆ„ìˆ˜ ë° íŒ¨í„´ ì¡°ì‚¬...\n",
      "  ID íŒ¨í„´ ë¶„ì„...\n",
      "    Train ID ë²”ìœ„: 0 ~ 87158\n",
      "    Test ID ë²”ìœ„: 0 ~ 46203\n",
      "\n",
      "  Train vs Test ë¶„í¬ ë¹„êµ...\n",
      "\n",
      "==================== power_transform ====================\n",
      "ğŸ”§ ê³ ê¸‰ ì „ì²˜ë¦¬ ì „ëµ: power_transform\n",
      "  Power Transform ì ìš©...\n",
      "\n",
      "âš™ï¸ ì´ˆì •ë°€ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹...\n",
      "  ê·¸ë¦¬ë“œ ì„œì¹˜ ì‹¤í–‰ ì¤‘... (ì‹œê°„ì´ ê±¸ë¦½ë‹ˆë‹¤)\n",
      "Fitting 3 folds for each of 900 candidates, totalling 2700 fits\n",
      "  âœ… ìµœì  íŒŒë¼ë¯¸í„°: {'colsample_bytree': 0.8, 'learning_rate': 0.07, 'max_depth': 5, 'n_estimators': 140, 'subsample': 0.85}\n",
      "  âœ… ìµœì  CV F1: 0.474495\n",
      "\n",
      "ğŸ—ï¸ ìµœì  íŒŒë¼ë¯¸í„° ê¸°ë°˜ ê³ ê¸‰ ìŠ¤íƒœí‚¹...\n",
      "  Level 1: 5ê°œ ìµœì í™”ëœ ëª¨ë¸\n",
      "    xgb_optimal ì²˜ë¦¬ ì¤‘...\n",
      "    xgb_variant1 ì²˜ë¦¬ ì¤‘...\n",
      "    xgb_variant2 ì²˜ë¦¬ ì¤‘...\n",
      "    lgb_optimal ì²˜ë¦¬ ì¤‘...\n",
      "    cat_optimal ì²˜ë¦¬ ì¤‘...\n",
      "  âœ… ìµœê³  ë©”íƒ€ ëª¨ë¸ CV F1: 0.486957\n",
      "\n",
      "==================== quantile_transform ====================\n",
      "ğŸ”§ ê³ ê¸‰ ì „ì²˜ë¦¬ ì „ëµ: quantile_transform\n",
      "  Quantile Transform ì ìš©...\n",
      "\n",
      "âš™ï¸ ì´ˆì •ë°€ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹...\n",
      "  ê·¸ë¦¬ë“œ ì„œì¹˜ ì‹¤í–‰ ì¤‘... (ì‹œê°„ì´ ê±¸ë¦½ë‹ˆë‹¤)\n",
      "Fitting 3 folds for each of 900 candidates, totalling 2700 fits\n",
      "  âœ… ìµœì  íŒŒë¼ë¯¸í„°: {'colsample_bytree': 0.8, 'learning_rate': 0.07, 'max_depth': 5, 'n_estimators': 140, 'subsample': 0.85}\n",
      "  âœ… ìµœì  CV F1: 0.474495\n",
      "\n",
      "ğŸ—ï¸ ìµœì  íŒŒë¼ë¯¸í„° ê¸°ë°˜ ê³ ê¸‰ ìŠ¤íƒœí‚¹...\n",
      "  Level 1: 5ê°œ ìµœì í™”ëœ ëª¨ë¸\n",
      "    xgb_optimal ì²˜ë¦¬ ì¤‘...\n",
      "    xgb_variant1 ì²˜ë¦¬ ì¤‘...\n",
      "    xgb_variant2 ì²˜ë¦¬ ì¤‘...\n",
      "    lgb_optimal ì²˜ë¦¬ ì¤‘...\n",
      "    cat_optimal ì²˜ë¦¬ ì¤‘...\n",
      "  âœ… ìµœê³  ë©”íƒ€ ëª¨ë¸ CV F1: 0.486957\n",
      "\n",
      "==================== robust_scaling ====================\n",
      "ğŸ”§ ê³ ê¸‰ ì „ì²˜ë¦¬ ì „ëµ: robust_scaling\n",
      "  Robust Scaling ì ìš©...\n",
      "\n",
      "âš™ï¸ ì´ˆì •ë°€ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹...\n",
      "  ê·¸ë¦¬ë“œ ì„œì¹˜ ì‹¤í–‰ ì¤‘... (ì‹œê°„ì´ ê±¸ë¦½ë‹ˆë‹¤)\n",
      "Fitting 3 folds for each of 900 candidates, totalling 2700 fits\n",
      "  âœ… ìµœì  íŒŒë¼ë¯¸í„°: {'colsample_bytree': 0.8, 'learning_rate': 0.07, 'max_depth': 5, 'n_estimators': 140, 'subsample': 0.85}\n",
      "  âœ… ìµœì  CV F1: 0.474495\n",
      "\n",
      "ğŸ—ï¸ ìµœì  íŒŒë¼ë¯¸í„° ê¸°ë°˜ ê³ ê¸‰ ìŠ¤íƒœí‚¹...\n",
      "  Level 1: 5ê°œ ìµœì í™”ëœ ëª¨ë¸\n",
      "    xgb_optimal ì²˜ë¦¬ ì¤‘...\n",
      "    xgb_variant1 ì²˜ë¦¬ ì¤‘...\n",
      "    xgb_variant2 ì²˜ë¦¬ ì¤‘...\n",
      "    lgb_optimal ì²˜ë¦¬ ì¤‘...\n",
      "    cat_optimal ì²˜ë¦¬ ì¤‘...\n",
      "  âœ… ìµœê³  ë©”íƒ€ ëª¨ë¸ CV F1: 0.486957\n",
      "\n",
      "ğŸ¯ ëª¨ë“  ì „ëµ ìµœì¢… ì•™ìƒë¸”...\n",
      "  ì „ëµ: precision_tuned\n",
      "  ì „ëµ: advanced_stacking\n",
      "\n",
      "ğŸ¯ ìƒì„±ëœ íŒŒì¼ë“¤:\n",
      "1. FINAL_BREAKTHROUGH.csv â­ (ìµœì¢… ëŒíŒŒ ì‹œë„)\n",
      "2. breakthrough_stacking_*.csv (ê° ì „ì²˜ë¦¬ë³„ ìŠ¤íƒœí‚¹)\n",
      "3. breakthrough_tuned_*.csv (ê° ì „ì²˜ë¦¬ë³„ íŠœë‹)\n",
      "\n",
      "ğŸ’¡ ì´ë²ˆ ì‹œë„ì˜ í•µì‹¬:\n",
      "- ê³ ê¸‰ ì „ì²˜ë¦¬ë¡œ ë°ì´í„° í’ˆì§ˆ í–¥ìƒ\n",
      "- ì´ˆì •ë°€ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\n",
      "- ë°ì´í„° ëˆ„ìˆ˜ ê°€ëŠ¥ì„± ì¡°ì‚¬\n",
      "- ìµœì í™”ëœ ìŠ¤íƒœí‚¹ ì•™ìƒë³„\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, PowerTransformer, QuantileTransformer\n",
    "from sklearn.metrics import f1_score\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class FinalBreakthroughOptimizer:\n",
    "    \"\"\"ë§ˆì§€ë§‰ ëŒíŒŒë¥¼ ìœ„í•œ ë°ì´í„° ë ˆë²¨ ìµœì í™”\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.submissions = {}\n",
    "        self.transformers = {}\n",
    "        \n",
    "    def load_and_advanced_preprocess(self, strategy='standard'):\n",
    "        \"\"\"ê³ ê¸‰ ì „ì²˜ë¦¬ ì „ëµë“¤\"\"\"\n",
    "        print(f\"ğŸ”§ ê³ ê¸‰ ì „ì²˜ë¦¬ ì „ëµ: {strategy}\")\n",
    "        \n",
    "        train_df = pd.read_csv('train.csv')\n",
    "        test_df = pd.read_csv('test.csv')\n",
    "        \n",
    "        feature_cols = [col for col in train_df.columns if col not in ['ID', 'Cancer']]\n",
    "        \n",
    "        X_train = train_df[feature_cols].copy()\n",
    "        y_train = train_df['Cancer'].copy()\n",
    "        X_test = test_df[feature_cols].copy()\n",
    "        \n",
    "        # ì¹´í…Œê³ ë¦¬ì»¬ ì¸ì½”ë”©\n",
    "        categorical_cols = X_train.select_dtypes(include=['object']).columns\n",
    "        \n",
    "        for col in categorical_cols:\n",
    "            le = LabelEncoder()\n",
    "            X_train[col] = le.fit_transform(X_train[col].astype(str))\n",
    "            \n",
    "            test_values = X_test[col].astype(str)\n",
    "            test_encoded = []\n",
    "            for val in test_values:\n",
    "                if val in le.classes_:\n",
    "                    test_encoded.append(le.transform([val])[0])\n",
    "                else:\n",
    "                    test_encoded.append(0)\n",
    "            X_test[col] = test_encoded\n",
    "        \n",
    "        # ê²°ì¸¡ê°’ ì²˜ë¦¬\n",
    "        numeric_cols = X_train.select_dtypes(include=[np.number]).columns\n",
    "        for col in numeric_cols:\n",
    "            median_val = X_train[col].median()\n",
    "            X_train[col].fillna(median_val, inplace=True)\n",
    "            X_test[col].fillna(median_val, inplace=True)\n",
    "        \n",
    "        # ê³ ê¸‰ ì „ì²˜ë¦¬ ì „ëµ ì ìš©\n",
    "        if strategy == 'power_transform':\n",
    "            # Power Transform (Yeo-Johnson)\n",
    "            print(\"  Power Transform ì ìš©...\")\n",
    "            pt = PowerTransformer(method='yeo-johnson', standardize=True)\n",
    "            X_train[numeric_cols] = pt.fit_transform(X_train[numeric_cols])\n",
    "            X_test[numeric_cols] = pt.transform(X_test[numeric_cols])\n",
    "            \n",
    "        elif strategy == 'quantile_transform':\n",
    "            # Quantile Transform (Uniform distribution)\n",
    "            print(\"  Quantile Transform ì ìš©...\")\n",
    "            qt = QuantileTransformer(n_quantiles=1000, output_distribution='uniform', random_state=42)\n",
    "            X_train[numeric_cols] = qt.fit_transform(X_train[numeric_cols])\n",
    "            X_test[numeric_cols] = qt.transform(X_test[numeric_cols])\n",
    "            \n",
    "        elif strategy == 'robust_scaling':\n",
    "            # Robust Scaling (ì´ìƒì¹˜ì— ê°•í•¨)\n",
    "            print(\"  Robust Scaling ì ìš©...\")\n",
    "            from sklearn.preprocessing import RobustScaler\n",
    "            rs = RobustScaler()\n",
    "            X_train[numeric_cols] = rs.fit_transform(X_train[numeric_cols])\n",
    "            X_test[numeric_cols] = rs.transform(X_test[numeric_cols])\n",
    "            \n",
    "        elif strategy == 'log_transform':\n",
    "            # Log Transform\n",
    "            print(\"  Log Transform ì ìš©...\")\n",
    "            for col in numeric_cols:\n",
    "                if X_train[col].min() > 0:  # ì–‘ìˆ˜ë§Œ ê°€ëŠ¥\n",
    "                    X_train[col] = np.log1p(X_train[col])\n",
    "                    X_test[col] = np.log1p(X_test[col])\n",
    "            \n",
    "            # ê·¸ í›„ StandardScaling\n",
    "            scaler = StandardScaler()\n",
    "            X_train[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])\n",
    "            X_test[numeric_cols] = scaler.transform(X_test[numeric_cols])\n",
    "            \n",
    "        else:  # standard\n",
    "            # ê¸°ë³¸ StandardScaling\n",
    "            scaler = StandardScaler()\n",
    "            X_train[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])\n",
    "            X_test[numeric_cols] = scaler.transform(X_test[numeric_cols])\n",
    "        \n",
    "        return X_train, y_train, X_test, test_df['ID']\n",
    "    \n",
    "    def hyperparameter_precision_tuning(self, X_train, y_train, X_test, test_ids):\n",
    "        \"\"\"ì´ˆì •ë°€ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\"\"\"\n",
    "        print(\"\\nâš™ï¸ ì´ˆì •ë°€ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹...\")\n",
    "        \n",
    "        pos_count = (y_train == 1).sum()\n",
    "        neg_count = (y_train == 0).sum()\n",
    "        scale_pos_weight = neg_count / pos_count\n",
    "        \n",
    "        # ë§¤ìš° ì„¸ë°€í•œ íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ\n",
    "        param_grid = {\n",
    "            'n_estimators': [140, 150, 160, 170, 180],\n",
    "            'max_depth': [5, 6, 7],\n",
    "            'learning_rate': [0.07, 0.075, 0.08, 0.085, 0.09],\n",
    "            'subsample': [0.75, 0.8, 0.85],\n",
    "            'colsample_bytree': [0.75, 0.8, 0.85, 0.9]\n",
    "        }\n",
    "        \n",
    "        base_model = xgb.XGBClassifier(\n",
    "            random_state=42,\n",
    "            scale_pos_weight=scale_pos_weight,\n",
    "            reg_alpha=0.1,\n",
    "            reg_lambda=0.1,\n",
    "            eval_metric='logloss'\n",
    "        )\n",
    "        \n",
    "        # 3-foldë¡œ ë¹ ë¥¸ íƒìƒ‰\n",
    "        cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        \n",
    "        grid_search = GridSearchCV(\n",
    "            base_model, \n",
    "            param_grid,\n",
    "            cv=cv,\n",
    "            scoring='f1',\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        print(\"  ê·¸ë¦¬ë“œ ì„œì¹˜ ì‹¤í–‰ ì¤‘... (ì‹œê°„ì´ ê±¸ë¦½ë‹ˆë‹¤)\")\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        print(f\"  âœ… ìµœì  íŒŒë¼ë¯¸í„°: {grid_search.best_params_}\")\n",
    "        print(f\"  âœ… ìµœì  CV F1: {grid_search.best_score_:.6f}\")\n",
    "        \n",
    "        # ìµœì  ëª¨ë¸ë¡œ ì˜ˆì¸¡\n",
    "        best_model = grid_search.best_estimator_\n",
    "        predictions = best_model.predict(X_test)\n",
    "        \n",
    "        submission = pd.DataFrame({'ID': test_ids, 'Cancer': predictions})\n",
    "        self.submissions['precision_tuned'] = submission\n",
    "        \n",
    "        return submission, grid_search.best_params_\n",
    "    \n",
    "    def advanced_stacking_with_best_params(self, X_train, y_train, X_test, test_ids, best_params):\n",
    "        \"\"\"ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ê³ ê¸‰ ìŠ¤íƒœí‚¹\"\"\"\n",
    "        print(\"\\nğŸ—ï¸ ìµœì  íŒŒë¼ë¯¸í„° ê¸°ë°˜ ê³ ê¸‰ ìŠ¤íƒœí‚¹...\")\n",
    "        \n",
    "        pos_count = (y_train == 1).sum()\n",
    "        neg_count = (y_train == 0).sum()\n",
    "        scale_pos_weight = neg_count / pos_count\n",
    "        \n",
    "        # Level 1: ìµœì í™”ëœ ë‹¤ì–‘í•œ ëª¨ë¸ë“¤\n",
    "        models = {\n",
    "            'xgb_optimal': xgb.XGBClassifier(**best_params, random_state=42, scale_pos_weight=scale_pos_weight, reg_alpha=0.1, reg_lambda=0.1),\n",
    "            'xgb_variant1': xgb.XGBClassifier(**best_params, random_state=123, scale_pos_weight=scale_pos_weight, reg_alpha=0.05, reg_lambda=0.15),\n",
    "            'xgb_variant2': xgb.XGBClassifier(**best_params, random_state=456, scale_pos_weight=scale_pos_weight, reg_alpha=0.15, reg_lambda=0.05),\n",
    "            \n",
    "            'lgb_optimal': lgb.LGBMClassifier(\n",
    "                n_estimators=best_params['n_estimators'],\n",
    "                max_depth=best_params['max_depth'], \n",
    "                learning_rate=best_params['learning_rate'],\n",
    "                subsample=best_params['subsample'],\n",
    "                colsample_bytree=best_params['colsample_bytree'],\n",
    "                random_state=42, class_weight='balanced', verbose=-1\n",
    "            ),\n",
    "            \n",
    "            'cat_optimal': cb.CatBoostClassifier(\n",
    "                iterations=best_params['n_estimators'],\n",
    "                depth=best_params['max_depth'],\n",
    "                learning_rate=best_params['learning_rate'],\n",
    "                subsample=best_params['subsample'],\n",
    "                random_state=42, verbose=False\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        print(f\"  Level 1: {len(models)}ê°œ ìµœì í™”ëœ ëª¨ë¸\")\n",
    "        \n",
    "        # 7-fold Cross-validation\n",
    "        cv = StratifiedKFold(n_splits=7, shuffle=True, random_state=42)\n",
    "        oof_predictions = np.zeros((len(X_train), len(models)))\n",
    "        test_predictions = np.zeros((len(X_test), len(models)))\n",
    "        \n",
    "        for i, (name, model) in enumerate(models.items()):\n",
    "            print(f\"    {name} ì²˜ë¦¬ ì¤‘...\")\n",
    "            \n",
    "            oof_pred = np.zeros(len(X_train))\n",
    "            \n",
    "            for train_idx, val_idx in cv.split(X_train, y_train):\n",
    "                X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "                y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "                \n",
    "                model.fit(X_tr, y_tr)\n",
    "                oof_pred[val_idx] = model.predict_proba(X_val)[:, 1]\n",
    "            \n",
    "            oof_predictions[:, i] = oof_pred\n",
    "            \n",
    "            # ì „ì²´ ë°ì´í„°ë¡œ ì¬í•™ìŠµ\n",
    "            model.fit(X_train, y_train)\n",
    "            test_predictions[:, i] = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # Level 2: ì •ë°€ ì¡°ì •ëœ ë©”íƒ€ ëª¨ë¸\n",
    "        meta_models = [\n",
    "            LogisticRegression(random_state=42, class_weight='balanced', C=0.1),\n",
    "            LogisticRegression(random_state=42, class_weight='balanced', C=1.0),\n",
    "            LogisticRegression(random_state=42, class_weight='balanced', C=10.0)\n",
    "        ]\n",
    "        \n",
    "        best_meta_score = 0\n",
    "        best_meta_model = None\n",
    "        \n",
    "        cv_meta = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        \n",
    "        for meta_model in meta_models:\n",
    "            scores = cross_val_score(meta_model, oof_predictions, y_train, cv=cv_meta, scoring='f1')\n",
    "            score = scores.mean()\n",
    "            \n",
    "            if score > best_meta_score:\n",
    "                best_meta_score = score\n",
    "                best_meta_model = meta_model\n",
    "        \n",
    "        print(f\"  âœ… ìµœê³  ë©”íƒ€ ëª¨ë¸ CV F1: {best_meta_score:.6f}\")\n",
    "        \n",
    "        # ìµœì¢… ì˜ˆì¸¡\n",
    "        best_meta_model.fit(oof_predictions, y_train)\n",
    "        final_proba = best_meta_model.predict_proba(test_predictions)[:, 1]\n",
    "        final_predictions = (final_proba > 0.5).astype(int)\n",
    "        \n",
    "        submission = pd.DataFrame({'ID': test_ids, 'Cancer': final_predictions})\n",
    "        self.submissions['advanced_stacking'] = submission\n",
    "        \n",
    "        return submission\n",
    "    \n",
    "    def data_leakage_investigation(self):\n",
    "        \"\"\"ë°ì´í„° ëˆ„ìˆ˜ ì¡°ì‚¬\"\"\"\n",
    "        print(\"\\nğŸ” ë°ì´í„° ëˆ„ìˆ˜ ë° íŒ¨í„´ ì¡°ì‚¬...\")\n",
    "        \n",
    "        train_df = pd.read_csv('train.csv')\n",
    "        test_df = pd.read_csv('test.csv')\n",
    "        \n",
    "        # ID íŒ¨í„´ ë¶„ì„\n",
    "        print(\"  ID íŒ¨í„´ ë¶„ì„...\")\n",
    "        train_ids = train_df['ID'].astype(str)\n",
    "        test_ids = test_df['ID'].astype(str)\n",
    "        \n",
    "        # IDì—ì„œ ìˆ«ì ì¶”ì¶œí•˜ì—¬ íŒ¨í„´ í™•ì¸\n",
    "        train_id_nums = train_ids.str.extract('(\\d+)').astype(float)\n",
    "        test_id_nums = test_ids.str.extract('(\\d+)').astype(float)\n",
    "        \n",
    "        print(f\"    Train ID ë²”ìœ„: {train_id_nums.min().values[0]:.0f} ~ {train_id_nums.max().values[0]:.0f}\")\n",
    "        print(f\"    Test ID ë²”ìœ„: {test_id_nums.min().values[0]:.0f} ~ {test_id_nums.max().values[0]:.0f}\")\n",
    "        \n",
    "        # íŠ¹ì„± ë¶„í¬ ë¹„êµ\n",
    "        print(\"\\n  Train vs Test ë¶„í¬ ë¹„êµ...\")\n",
    "        feature_cols = [col for col in train_df.columns if col not in ['ID', 'Cancer']]\n",
    "        \n",
    "        for col in feature_cols:\n",
    "            if train_df[col].dtype in ['int64', 'float64']:\n",
    "                # ìˆ˜ì¹˜í˜• ë³€ìˆ˜\n",
    "                train_mean = train_df[col].mean()\n",
    "                test_mean = test_df[col].mean()\n",
    "                diff_pct = abs(train_mean - test_mean) / train_mean * 100\n",
    "                \n",
    "                if diff_pct > 5:  # 5% ì´ìƒ ì°¨ì´\n",
    "                    print(f\"    âš ï¸  {col}: Train {train_mean:.3f} vs Test {test_mean:.3f} ({diff_pct:.1f}% ì°¨ì´)\")\n",
    "            else:\n",
    "                # ì¹´í…Œê³ ë¦¬ì»¬ ë³€ìˆ˜\n",
    "                train_unique = set(train_df[col].unique())\n",
    "                test_unique = set(test_df[col].unique())\n",
    "                \n",
    "                only_in_test = test_unique - train_unique\n",
    "                if only_in_test:\n",
    "                    print(f\"    âš ï¸  {col}: í…ŒìŠ¤íŠ¸ì—ë§Œ ìˆëŠ” ê°’ {len(only_in_test)}ê°œ\")\n",
    "    \n",
    "    def ensemble_all_strategies(self):\n",
    "        \"\"\"ëª¨ë“  ì „ëµì˜ ìµœì¢… ì•™ìƒë³„\"\"\"\n",
    "        print(\"\\nğŸ¯ ëª¨ë“  ì „ëµ ìµœì¢… ì•™ìƒë¸”...\")\n",
    "        \n",
    "        if len(self.submissions) < 2:\n",
    "            print(\"  âŒ ì¶©ë¶„í•œ ì „ëµì´ ì‹¤í–‰ë˜ì§€ ì•ŠìŒ\")\n",
    "            return None\n",
    "        \n",
    "        # ëª¨ë“  ì˜ˆì¸¡ ìˆ˜ì§‘\n",
    "        all_predictions = []\n",
    "        strategy_names = []\n",
    "        \n",
    "        for name, submission in self.submissions.items():\n",
    "            all_predictions.append(submission['Cancer'].values)\n",
    "            strategy_names.append(name)\n",
    "            print(f\"  ì „ëµ: {name}\")\n",
    "        \n",
    "        # ë‹¨ìˆœ ë‹¤ìˆ˜ê²°\n",
    "        all_predictions = np.array(all_predictions)\n",
    "        majority_vote = np.round(np.mean(all_predictions, axis=0)).astype(int)\n",
    "        \n",
    "        # ê°€ì¤‘ í‰ê·  (ìµœì‹  ì „ëµì— ë†’ì€ ê°€ì¤‘ì¹˜)\n",
    "        weights = np.linspace(0.8, 1.2, len(strategy_names))  # ìµœì‹ ì¼ìˆ˜ë¡ ë†’ì€ ê°€ì¤‘ì¹˜\n",
    "        weighted_avg = np.average(all_predictions, axis=0, weights=weights)\n",
    "        weighted_predictions = np.round(weighted_avg).astype(int)\n",
    "        \n",
    "        # ë‘ ê°€ì§€ ì•™ìƒë¸” ê²°ê³¼\n",
    "        test_ids = list(self.submissions.values())[0]['ID']\n",
    "        \n",
    "        majority_submission = pd.DataFrame({'ID': test_ids, 'Cancer': majority_vote})\n",
    "        weighted_submission = pd.DataFrame({'ID': test_ids, 'Cancer': weighted_predictions})\n",
    "        \n",
    "        self.submissions['final_majority'] = majority_submission\n",
    "        self.submissions['final_weighted'] = weighted_submission\n",
    "        \n",
    "        return weighted_submission  # ê°€ì¤‘ í‰ê· ì„ ê¸°ë³¸ìœ¼ë¡œ ë°˜í™˜\n",
    "\n",
    "def run_final_breakthrough():\n",
    "    \"\"\"ë§ˆì§€ë§‰ ëŒíŒŒ ì‹œë„ ì‹¤í–‰\"\"\"\n",
    "    print(\"ğŸš€ ë§ˆì§€ë§‰ ëŒíŒŒ ì‹œë„: ë°ì´í„° ë ˆë²¨ ìµœì í™”!\")\n",
    "    print(\"ëª©í‘œ: 0.5109 â†’ 0.512+ (1ë“± íƒˆí™˜!)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    optimizer = FinalBreakthroughOptimizer()\n",
    "    \n",
    "    # 1. ë°ì´í„° ëˆ„ìˆ˜ ì¡°ì‚¬\n",
    "    optimizer.data_leakage_investigation()\n",
    "    \n",
    "    # 2. ë‹¤ì–‘í•œ ì „ì²˜ë¦¬ ì „ëµ ì‹œë„\n",
    "    strategies = ['power_transform', 'quantile_transform', 'robust_scaling']\n",
    "    \n",
    "    for strategy in strategies:\n",
    "        print(f\"\\n{'='*20} {strategy} {'='*20}\")\n",
    "        try:\n",
    "            X_train, y_train, X_test, test_ids = optimizer.load_and_advanced_preprocess(strategy)\n",
    "            \n",
    "            # ì´ˆì •ë°€ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\n",
    "            tuned_submission, best_params = optimizer.hyperparameter_precision_tuning(\n",
    "                X_train, y_train, X_test, test_ids\n",
    "            )\n",
    "            \n",
    "            # ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ê³ ê¸‰ ìŠ¤íƒœí‚¹\n",
    "            stacking_submission = optimizer.advanced_stacking_with_best_params(\n",
    "                X_train, y_train, X_test, test_ids, best_params\n",
    "            )\n",
    "            \n",
    "            # íŒŒì¼ ì €ì¥\n",
    "            tuned_submission.to_csv(f'breakthrough_tuned_{strategy}.csv', index=False)\n",
    "            stacking_submission.to_csv(f'breakthrough_stacking_{strategy}.csv', index=False)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ {strategy} ì‹¤íŒ¨: {e}\")\n",
    "    \n",
    "    # 3. ìµœì¢… ì•™ìƒë³„\n",
    "    final_submission = optimizer.ensemble_all_strategies()\n",
    "    if final_submission is not None:\n",
    "        final_submission.to_csv('FINAL_BREAKTHROUGH.csv', index=False)\n",
    "    \n",
    "    print(f\"\\nğŸ¯ ìƒì„±ëœ íŒŒì¼ë“¤:\")\n",
    "    print(f\"1. FINAL_BREAKTHROUGH.csv â­ (ìµœì¢… ëŒíŒŒ ì‹œë„)\")\n",
    "    print(f\"2. breakthrough_stacking_*.csv (ê° ì „ì²˜ë¦¬ë³„ ìŠ¤íƒœí‚¹)\")\n",
    "    print(f\"3. breakthrough_tuned_*.csv (ê° ì „ì²˜ë¦¬ë³„ íŠœë‹)\")\n",
    "    \n",
    "    print(f\"\\nğŸ’¡ ì´ë²ˆ ì‹œë„ì˜ í•µì‹¬:\")\n",
    "    print(f\"- ê³ ê¸‰ ì „ì²˜ë¦¬ë¡œ ë°ì´í„° í’ˆì§ˆ í–¥ìƒ\")\n",
    "    print(f\"- ì´ˆì •ë°€ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\")\n",
    "    print(f\"- ë°ì´í„° ëˆ„ìˆ˜ ê°€ëŠ¥ì„± ì¡°ì‚¬\")\n",
    "    print(f\"- ìµœì í™”ëœ ìŠ¤íƒœí‚¹ ì•™ìƒë³„\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_final_breakthrough()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
