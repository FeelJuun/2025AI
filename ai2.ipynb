{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9517d262-56ce-442a-b2ba-acd979e47977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ CuPy ì—†ìŒ - CPUë§Œ ì‚¬ìš©\n",
      "âš ï¸ RAPIDS ì—†ìŒ\n",
      "ğŸ§  PyTorch ì‚¬ìš© ê°€ëŠ¥ - ë”¥ëŸ¬ë‹ í™œì„±í™”!\n",
      "ğŸ‘¹ H200 GPU í’€íŒŒì›Œ ê´´ë¬¼ ì‚¬ëƒ¥!\n",
      "============================================================\n",
      "ğŸ”¥ NVIDIA H200 + 140GB ë©”ëª¨ë¦¬ í’€ê°€ë™!\n",
      "ğŸ¯ ëª©í‘œ: 0.5120+ (ê´´ë¬¼ ì™„ì „ ì œì••!)\n",
      "âš¡ GPU ê°€ì† ë”¥ëŸ¬ë‹ + ë©”ê°€ ì•™ìƒë¸” + ê·¹í•œ ìµœì í™”\n",
      "============================================================\n",
      "ğŸ’ª H200 í’€íŒŒì›Œ ëª¨ë“œ í™œì„±í™”!\n",
      "ğŸ¯ ëª©í‘œ: 0.5120+ (ê´´ë¬¼ ì™„ì „ ì œì••!)\n",
      "\n",
      "==================== PyTorch ë”¥ëŸ¬ë‹ ====================\n",
      "\n",
      "ğŸ§  ì „ëµ 1: PyTorch ë”¥ëŸ¬ë‹ (H200 GPU ê°€ì†)!\n",
      "  ğŸ”¬ H200 ê·¹í•œ ì „ì²˜ë¦¬ ì‹œì‘... (ì‹œë“œ: 42)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# GPU ê°€ì† ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤\n",
    "try:\n",
    "    import cupy as cp\n",
    "    GPU_AVAILABLE = True\n",
    "    print(\"ğŸ”¥ CuPy ì‚¬ìš© ê°€ëŠ¥ - GPU ê°€ì† í™œì„±í™”!\")\n",
    "except:\n",
    "    import numpy as cp\n",
    "    GPU_AVAILABLE = False\n",
    "    print(\"âš ï¸ CuPy ì—†ìŒ - CPUë§Œ ì‚¬ìš©\")\n",
    "\n",
    "try:\n",
    "    import cudf\n",
    "    import cuml\n",
    "    from cuml.ensemble import RandomForestClassifier as cuRF\n",
    "    from cuml.linear_model import LogisticRegression as cuLR\n",
    "    RAPIDS_AVAILABLE = True\n",
    "    print(\"ğŸš€ RAPIDS ì‚¬ìš© ê°€ëŠ¥ - GPU ML ê°€ì†!\")\n",
    "except:\n",
    "    RAPIDS_AVAILABLE = False\n",
    "    print(\"âš ï¸ RAPIDS ì—†ìŒ\")\n",
    "\n",
    "# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, PolynomialFeatures\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFECV\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, VotingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "\n",
    "# ê³ ê¸‰ ìµœì í™”\n",
    "try:\n",
    "    import optuna\n",
    "    from optuna.integration import XGBoostPruningCallback\n",
    "    OPTUNA_AVAILABLE = True\n",
    "    print(\"ğŸ¯ Optuna ì‚¬ìš© ê°€ëŠ¥ - ë² ì´ì§€ì•ˆ ìµœì í™”!\")\n",
    "except:\n",
    "    OPTUNA_AVAILABLE = False\n",
    "\n",
    "# ë”¥ëŸ¬ë‹\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "    PYTORCH_AVAILABLE = True\n",
    "    print(\"ğŸ§  PyTorch ì‚¬ìš© ê°€ëŠ¥ - ë”¥ëŸ¬ë‹ í™œì„±í™”!\")\n",
    "except:\n",
    "    PYTORCH_AVAILABLE = False\n",
    "\n",
    "import time\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "class H200MonsterKiller:\n",
    "    \"\"\"H200 GPU í’€íŒŒì›Œ ê´´ë¬¼ ì œì••ê¸° - 0.5120+ ëª©í‘œ\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.submissions = {}\n",
    "        self.best_score = 0.5109\n",
    "        \n",
    "        # GPU ë©”ëª¨ë¦¬ ì •ë³´\n",
    "        if GPU_AVAILABLE:\n",
    "            print(f\"ğŸ”¥ GPU ë©”ëª¨ë¦¬: {cp.cuda.Device().mem_info[1] / 1024**3:.1f} GB\")\n",
    "            \n",
    "        print(\"ğŸ’ª H200 í’€íŒŒì›Œ ëª¨ë“œ í™œì„±í™”!\")\n",
    "        print(\"ğŸ¯ ëª©í‘œ: 0.5120+ (ê´´ë¬¼ ì™„ì „ ì œì••!)\")\n",
    "        \n",
    "    def load_and_ultimate_preprocess(self, random_seed=42):\n",
    "        \"\"\"H200 íŒŒì›Œë¡œ ê·¹í•œ ì „ì²˜ë¦¬\"\"\"\n",
    "        np.random.seed(random_seed)\n",
    "        \n",
    "        train_df = pd.read_csv('train.csv')\n",
    "        test_df = pd.read_csv('test.csv')\n",
    "        \n",
    "        print(f\"  ğŸ”¬ H200 ê·¹í•œ ì „ì²˜ë¦¬ ì‹œì‘... (ì‹œë“œ: {random_seed})\")\n",
    "        \n",
    "        feature_cols = [col for col in train_df.columns if col not in ['ID', 'Cancer']]\n",
    "        \n",
    "        X_train = train_df[feature_cols].copy()\n",
    "        y_train = train_df['Cancer'].copy()\n",
    "        X_test = test_df[feature_cols].copy()\n",
    "        \n",
    "        # 1. ê¸°ë³¸ ì „ì²˜ë¦¬\n",
    "        categorical_cols = X_train.select_dtypes(include=['object']).columns\n",
    "        \n",
    "        for col in categorical_cols:\n",
    "            le = LabelEncoder()\n",
    "            X_train[col] = le.fit_transform(X_train[col].astype(str))\n",
    "            \n",
    "            test_values = X_test[col].astype(str)\n",
    "            test_encoded = []\n",
    "            for val in test_values:\n",
    "                if val in le.classes_:\n",
    "                    test_encoded.append(le.transform([val])[0])\n",
    "                else:\n",
    "                    test_encoded.append(0)\n",
    "            X_test[col] = test_encoded\n",
    "        \n",
    "        # ê²°ì¸¡ê°’ ì²˜ë¦¬\n",
    "        numeric_cols = X_train.select_dtypes(include=[np.number]).columns\n",
    "        for col in numeric_cols:\n",
    "            median_val = X_train[col].median()\n",
    "            X_train[col].fillna(median_val, inplace=True)\n",
    "            X_test[col].fillna(median_val, inplace=True)\n",
    "        \n",
    "        # 2. ì˜ë£Œ ë„ë©”ì¸ ì§€ì‹ í”¼ì²˜ (ê³ ê¸‰ ë²„ì „)\n",
    "        print(\"  ğŸ¥ ê³ ê¸‰ ì˜ë£Œ ë„ë©”ì¸ í”¼ì²˜ ìƒì„±...\")\n",
    "        \n",
    "        # ê°‘ìƒì„ ì•” ìœ„í—˜ ì ìˆ˜ (ì˜ë£Œ ë…¼ë¬¸ ê¸°ë°˜ ê°€ì¤‘ì¹˜)\n",
    "        X_train['Thyroid_Risk_Score'] = (\n",
    "            X_train['Family_Background'] * 0.35 +\n",
    "            X_train['Radiation_History'] * 0.30 +\n",
    "            X_train['Iodine_Deficiency'] * 0.20 +\n",
    "            (X_train['Gender'] == 1) * 0.15\n",
    "        )\n",
    "        X_test['Thyroid_Risk_Score'] = (\n",
    "            X_test['Family_Background'] * 0.35 +\n",
    "            X_test['Radiation_History'] * 0.30 +\n",
    "            X_test['Iodine_Deficiency'] * 0.20 +\n",
    "            (X_test['Gender'] == 1) * 0.15\n",
    "        )\n",
    "        \n",
    "        # í˜¸ë¥´ëª¬ í”„ë¡œíŒŒì¼ ë¶„ì„\n",
    "        # TSH ì •ìƒ ë²”ìœ„: 0.4-4.0 mIU/L\n",
    "        X_train['TSH_Category'] = 0  # ì •ìƒ\n",
    "        X_train.loc[X_train['TSH_Result'] < 0.4, 'TSH_Category'] = -1  # ë‚®ìŒ (ê°‘ìƒì„ ê¸°ëŠ¥í•­ì§„)\n",
    "        X_train.loc[X_train['TSH_Result'] > 4.0, 'TSH_Category'] = 1   # ë†’ìŒ (ê°‘ìƒì„ ê¸°ëŠ¥ì €í•˜)\n",
    "        \n",
    "        X_test['TSH_Category'] = 0\n",
    "        X_test.loc[X_test['TSH_Result'] < 0.4, 'TSH_Category'] = -1\n",
    "        X_test.loc[X_test['TSH_Result'] > 4.0, 'TSH_Category'] = 1\n",
    "        \n",
    "        # T4 ì •ìƒ ë²”ìœ„: 5.0-12.0 Î¼g/dL\n",
    "        X_train['T4_Category'] = 0\n",
    "        X_train.loc[X_train['T4_Result'] < 5.0, 'T4_Category'] = -1\n",
    "        X_train.loc[X_train['T4_Result'] > 12.0, 'T4_Category'] = 1\n",
    "        \n",
    "        X_test['T4_Category'] = 0\n",
    "        X_test.loc[X_test['T4_Result'] < 5.0, 'T4_Category'] = -1\n",
    "        X_test.loc[X_test['T4_Result'] > 12.0, 'T4_Category'] = 1\n",
    "        \n",
    "        # T3 ì •ìƒ ë²”ìœ„: 0.8-2.0 ng/dL\n",
    "        X_train['T3_Category'] = 0\n",
    "        X_train.loc[X_train['T3_Result'] < 0.8, 'T3_Category'] = -1\n",
    "        X_train.loc[X_train['T3_Result'] > 2.0, 'T3_Category'] = 1\n",
    "        \n",
    "        X_test['T3_Category'] = 0\n",
    "        X_test.loc[X_test['T3_Result'] < 0.8, 'T3_Category'] = -1\n",
    "        X_test.loc[X_test['T3_Result'] > 2.0, 'T3_Category'] = 1\n",
    "        \n",
    "        # í˜¸ë¥´ëª¬ ë¶ˆê· í˜• ì ìˆ˜\n",
    "        X_train['Hormone_Imbalance'] = (\n",
    "            abs(X_train['TSH_Category']) * 0.4 +\n",
    "            abs(X_train['T4_Category']) * 0.3 +\n",
    "            abs(X_train['T3_Category']) * 0.3\n",
    "        )\n",
    "        X_test['Hormone_Imbalance'] = (\n",
    "            abs(X_test['TSH_Category']) * 0.4 +\n",
    "            abs(X_test['T4_Category']) * 0.3 +\n",
    "            abs(X_test['T3_Category']) * 0.3\n",
    "        )\n",
    "        \n",
    "        # 3. ê³ ê¸‰ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ (H200 íŒŒì›Œ í™œìš©)\n",
    "        print(\"  âš¡ H200 íŒŒì›Œ ê³ ê¸‰ í”¼ì²˜ ìƒì„±...\")\n",
    "        \n",
    "        # ë‹¤í•­ì‹ í”¼ì²˜ (2ì°¨)\n",
    "        numeric_features = ['Age', 'Nodule_Size', 'TSH_Result', 'T4_Result', 'T3_Result']\n",
    "        poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)\n",
    "        \n",
    "        poly_train = poly.fit_transform(X_train[numeric_features])\n",
    "        poly_test = poly.transform(X_test[numeric_features])\n",
    "        \n",
    "        poly_feature_names = [f'poly_{i}' for i in range(poly_train.shape[1])]\n",
    "        \n",
    "        for i, name in enumerate(poly_feature_names):\n",
    "            X_train[name] = poly_train[:, i]\n",
    "            X_test[name] = poly_test[:, i]\n",
    "        \n",
    "        # í†µê³„ì  í”¼ì²˜ë“¤\n",
    "        X_train['Hormone_Mean'] = (X_train['TSH_Result'] + X_train['T4_Result'] + X_train['T3_Result']) / 3\n",
    "        X_train['Hormone_Std'] = ((X_train['TSH_Result'] - X_train['Hormone_Mean'])**2 + \n",
    "                                  (X_train['T4_Result'] - X_train['Hormone_Mean'])**2 + \n",
    "                                  (X_train['T3_Result'] - X_train['Hormone_Mean'])**2)**0.5\n",
    "        \n",
    "        X_test['Hormone_Mean'] = (X_test['TSH_Result'] + X_test['T4_Result'] + X_test['T3_Result']) / 3\n",
    "        X_test['Hormone_Std'] = ((X_test['TSH_Result'] - X_test['Hormone_Mean'])**2 + \n",
    "                                 (X_test['T4_Result'] - X_test['Hormone_Mean'])**2 + \n",
    "                                 (X_test['T3_Result'] - X_test['Hormone_Mean'])**2)**0.5\n",
    "        \n",
    "        # 4. GPU ê°€ì† íŠ¹ì„± ì„ íƒ (H200 í™œìš©)\n",
    "        if RAPIDS_AVAILABLE:\n",
    "            print(\"  ğŸš€ RAPIDS GPU íŠ¹ì„± ì„ íƒ...\")\n",
    "            try:\n",
    "                # GPU ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜\n",
    "                X_train_gpu = cudf.from_pandas(X_train)\n",
    "                y_train_gpu = cudf.from_pandas(y_train)\n",
    "                \n",
    "                # GPU ëœë¤ í¬ë ˆìŠ¤íŠ¸ë¡œ íŠ¹ì„± ì¤‘ìš”ë„ ê³„ì‚°\n",
    "                gpu_rf = cuRF(n_estimators=100, random_state=random_seed)\n",
    "                gpu_rf.fit(X_train_gpu, y_train_gpu)\n",
    "                \n",
    "                # ì¤‘ìš”ë„ ìƒìœ„ íŠ¹ì„± ì„ íƒ\n",
    "                importances = gpu_rf.feature_importances_\n",
    "                top_features_idx = np.argsort(importances)[-30:]  # ìƒìœ„ 30ê°œ\n",
    "                \n",
    "                feature_names = X_train.columns\n",
    "                selected_features = feature_names[top_features_idx]\n",
    "                \n",
    "                X_train = X_train[selected_features]\n",
    "                X_test = X_test[selected_features]\n",
    "                \n",
    "                print(f\"    GPU íŠ¹ì„± ì„ íƒ ì™„ë£Œ: {len(selected_features)}ê°œ íŠ¹ì„±\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    RAPIDS íŠ¹ì„± ì„ íƒ ì‹¤íŒ¨: {e}\")\n",
    "        \n",
    "        return X_train, y_train, X_test, test_df['ID']\n",
    "    \n",
    "    def strategy_1_pytorch_deep_learning(self):\n",
    "        \"\"\"ì „ëµ 1: PyTorch ë”¥ëŸ¬ë‹ (GPU ê°€ì†)\"\"\"\n",
    "        if not PYTORCH_AVAILABLE:\n",
    "            print(\"\\nâŒ PyTorch ì—†ìŒ - ì „ëµ 1 ìŠ¤í‚µ\")\n",
    "            return None\n",
    "            \n",
    "        print(\"\\nğŸ§  ì „ëµ 1: PyTorch ë”¥ëŸ¬ë‹ (H200 GPU ê°€ì†)!\")\n",
    "        \n",
    "        X_train, y_train, X_test, test_ids = self.load_and_ultimate_preprocess(42)\n",
    "        \n",
    "        # GPU ì„¤ì •\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"  ë””ë°”ì´ìŠ¤: {device}\")\n",
    "        \n",
    "        # ë°ì´í„° ì •ê·œí™”\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # PyTorch ë°ì´í„°ì…‹ í´ë˜ìŠ¤\n",
    "        class ThyroidDataset(Dataset):\n",
    "            def __init__(self, X, y=None):\n",
    "                self.X = torch.FloatTensor(X)\n",
    "                self.y = torch.LongTensor(y) if y is not None else None\n",
    "                \n",
    "            def __len__(self):\n",
    "                return len(self.X)\n",
    "                \n",
    "            def __getitem__(self, idx):\n",
    "                if self.y is not None:\n",
    "                    return self.X[idx], self.y[idx]\n",
    "                return self.X[idx]\n",
    "        \n",
    "        # ë”¥ëŸ¬ë‹ ëª¨ë¸ ì •ì˜\n",
    "        class ThyroidNet(nn.Module):\n",
    "            def __init__(self, input_dim, hidden_dims=[512, 256, 128, 64]):\n",
    "                super(ThyroidNet, self).__init__()\n",
    "                \n",
    "                layers = []\n",
    "                prev_dim = input_dim\n",
    "                \n",
    "                for hidden_dim in hidden_dims:\n",
    "                    layers.extend([\n",
    "                        nn.Linear(prev_dim, hidden_dim),\n",
    "                        nn.BatchNorm1d(hidden_dim),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(0.3)\n",
    "                    ])\n",
    "                    prev_dim = hidden_dim\n",
    "                \n",
    "                layers.append(nn.Linear(prev_dim, 2))\n",
    "                self.network = nn.Sequential(*layers)\n",
    "                \n",
    "            def forward(self, x):\n",
    "                return self.network(x)\n",
    "        \n",
    "        # 5-fold CV\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        oof_predictions = np.zeros(len(X_train))\n",
    "        test_predictions = np.zeros(len(X_test))\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(cv.split(X_train_scaled, y_train)):\n",
    "            print(f\"  Fold {fold+1}/5 í›ˆë ¨ ì¤‘...\")\n",
    "            \n",
    "            X_tr, X_val = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
    "            y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "            \n",
    "            # ë°ì´í„°ì…‹ ë° ë°ì´í„°ë¡œë”\n",
    "            train_dataset = ThyroidDataset(X_tr, y_tr.values)\n",
    "            val_dataset = ThyroidDataset(X_val, y_val.values)\n",
    "            \n",
    "            train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=1024, shuffle=False)\n",
    "            \n",
    "            # ëª¨ë¸ ì´ˆê¸°í™”\n",
    "            model = ThyroidNet(X_train_scaled.shape[1]).to(device)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10)\n",
    "            \n",
    "            # í›ˆë ¨\n",
    "            best_val_loss = float('inf')\n",
    "            patience_counter = 0\n",
    "            \n",
    "            for epoch in range(200):\n",
    "                # í›ˆë ¨\n",
    "                model.train()\n",
    "                train_loss = 0\n",
    "                for batch_X, batch_y in train_loader:\n",
    "                    batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(batch_X)\n",
    "                    loss = criterion(outputs, batch_y)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    train_loss += loss.item()\n",
    "                \n",
    "                # ê²€ì¦\n",
    "                model.eval()\n",
    "                val_loss = 0\n",
    "                with torch.no_grad():\n",
    "                    for batch_X, batch_y in val_loader:\n",
    "                        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                        outputs = model(batch_X)\n",
    "                        loss = criterion(outputs, batch_y)\n",
    "                        val_loss += loss.item()\n",
    "                \n",
    "                avg_val_loss = val_loss / len(val_loader)\n",
    "                scheduler.step(avg_val_loss)\n",
    "                \n",
    "                # Early stopping\n",
    "                if avg_val_loss < best_val_loss:\n",
    "                    best_val_loss = avg_val_loss\n",
    "                    patience_counter = 0\n",
    "                    # ìµœê³  ëª¨ë¸ ì €ì¥\n",
    "                    torch.save(model.state_dict(), f'best_model_fold_{fold}.pth')\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= 20:\n",
    "                        break\n",
    "            \n",
    "            # ìµœê³  ëª¨ë¸ ë¡œë“œ ë° ì˜ˆì¸¡\n",
    "            model.load_state_dict(torch.load(f'best_model_fold_{fold}.pth'))\n",
    "            model.eval()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # OOF ì˜ˆì¸¡\n",
    "                val_tensor = torch.FloatTensor(X_val).to(device)\n",
    "                val_outputs = model(val_tensor)\n",
    "                val_probs = torch.softmax(val_outputs, dim=1)[:, 1].cpu().numpy()\n",
    "                oof_predictions[val_idx] = val_probs\n",
    "                \n",
    "                # í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡\n",
    "                test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "                test_outputs = model(test_tensor)\n",
    "                test_probs = torch.softmax(test_outputs, dim=1)[:, 1].cpu().numpy()\n",
    "                test_predictions += test_probs / 5\n",
    "        \n",
    "        # CV ì„±ëŠ¥ í‰ê°€\n",
    "        oof_pred_binary = (oof_predictions > 0.5).astype(int)\n",
    "        cv_f1 = f1_score(y_train, oof_pred_binary)\n",
    "        print(f\"  âœ… PyTorch CV F1: {cv_f1:.6f}\")\n",
    "        \n",
    "        # ìµœì¢… ì˜ˆì¸¡\n",
    "        final_predictions = (test_predictions > 0.5).astype(int)\n",
    "        \n",
    "        submission = pd.DataFrame({'ID': test_ids, 'Cancer': final_predictions})\n",
    "        self.submissions['pytorch_deep'] = submission\n",
    "        return submission\n",
    "    \n",
    "    def strategy_2_gpu_mega_ensemble(self):\n",
    "        \"\"\"ì „ëµ 2: GPU ë©”ê°€ ì•™ìƒë¸” (100ê°œ ëª¨ë¸)\"\"\"\n",
    "        print(\"\\nğŸ”„ ì „ëµ 2: GPU ë©”ê°€ ì•™ìƒë¸” (100ê°œ ëª¨ë¸)!\")\n",
    "        print(\"  H200 íŒŒì›Œë¡œ 100ê°œ ëª¨ë¸ ë™ì‹œ í›ˆë ¨...\")\n",
    "        \n",
    "        X_train, y_train, X_test, test_ids = self.load_and_ultimate_preprocess(42)\n",
    "        \n",
    "        pos_count = (y_train == 1).sum()\n",
    "        neg_count = (y_train == 0).sum()\n",
    "        scale_pos_weight = neg_count / pos_count\n",
    "        \n",
    "        # 100ê°œ ë‹¤ì–‘í•œ ëª¨ë¸ ìƒì„± (GPU ë©”ëª¨ë¦¬ í™œìš©)\n",
    "        models = []\n",
    "        \n",
    "        print(\"  100ê°œ ëª¨ë¸ ìƒì„± ì¤‘...\")\n",
    "        \n",
    "        # XGBoost ë³€í˜•ë“¤ (40ê°œ)\n",
    "        for i in range(40):\n",
    "            seed = 42 + i * 137\n",
    "            params = {\n",
    "                'n_estimators': np.random.randint(150, 300),\n",
    "                'max_depth': np.random.randint(4, 10),\n",
    "                'learning_rate': np.random.uniform(0.03, 0.15),\n",
    "                'subsample': np.random.uniform(0.7, 1.0),\n",
    "                'colsample_bytree': np.random.uniform(0.7, 1.0),\n",
    "                'reg_alpha': np.random.uniform(0, 1.0),\n",
    "                'reg_lambda': np.random.uniform(0, 1.0),\n",
    "                'random_state': seed,\n",
    "                'scale_pos_weight': scale_pos_weight,\n",
    "                'tree_method': 'gpu_hist',  # GPU ê°€ì†\n",
    "                'gpu_id': 0,\n",
    "                'verbosity': 0\n",
    "            }\n",
    "            models.append(('xgb_' + str(i), xgb.XGBClassifier(**params)))\n",
    "        \n",
    "        # LightGBM ë³€í˜•ë“¤ (30ê°œ)\n",
    "        for i in range(30):\n",
    "            seed = 42 + i * 239\n",
    "            params = {\n",
    "                'n_estimators': np.random.randint(150, 300),\n",
    "                'max_depth': np.random.randint(4, 10),\n",
    "                'learning_rate': np.random.uniform(0.03, 0.15),\n",
    "                'subsample': np.random.uniform(0.7, 1.0),\n",
    "                'colsample_bytree': np.random.uniform(0.7, 1.0),\n",
    "                'reg_alpha': np.random.uniform(0, 1.0),\n",
    "                'reg_lambda': np.random.uniform(0, 1.0),\n",
    "                'random_state': seed,\n",
    "                'class_weight': 'balanced',\n",
    "                'device': 'gpu',  # GPU ê°€ì†\n",
    "                'verbosity': -1\n",
    "            }\n",
    "            models.append(('lgb_' + str(i), lgb.LGBMClassifier(**params)))\n",
    "        \n",
    "        # CatBoost ë³€í˜•ë“¤ (20ê°œ)\n",
    "        for i in range(20):\n",
    "            seed = 42 + i * 317\n",
    "            params = {\n",
    "                'iterations': np.random.randint(150, 300),\n",
    "                'depth': np.random.randint(4, 10),\n",
    "                'learning_rate': np.random.uniform(0.03, 0.15),\n",
    "                'random_state': seed,\n",
    "                'task_type': 'GPU',  # GPU ê°€ì†\n",
    "                'verbose': False\n",
    "            }\n",
    "            models.append(('cat_' + str(i), cb.CatBoostClassifier(**params)))\n",
    "        \n",
    "        # ExtraTrees ë³€í˜•ë“¤ (10ê°œ)\n",
    "        for i in range(10):\n",
    "            seed = 42 + i * 419\n",
    "            params = {\n",
    "                'n_estimators': np.random.randint(200, 400),\n",
    "                'max_depth': np.random.randint(10, 20),\n",
    "                'random_state': seed,\n",
    "                'class_weight': 'balanced',\n",
    "                'n_jobs': -1\n",
    "            }\n",
    "            models.append(('et_' + str(i), ExtraTreesClassifier(**params)))\n",
    "        \n",
    "        print(f\"  ì´ {len(models)}ê°œ ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ\")\n",
    "        \n",
    "        # ìŠ¤íƒœí‚¹ ì•™ìƒë¸”\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        oof_predictions = np.zeros((len(X_train), len(models)))\n",
    "        test_predictions = np.zeros((len(X_test), len(models)))\n",
    "        \n",
    "        # ë³‘ë ¬ ì²˜ë¦¬ë¡œ ëª¨ë¸ í›ˆë ¨\n",
    "        for i, (name, model) in enumerate(models):\n",
    "            if i % 20 == 0:\n",
    "                print(f\"  ëª¨ë¸ {i+1}/{len(models)} í›ˆë ¨ ì¤‘...\")\n",
    "            \n",
    "            try:\n",
    "                oof_pred = np.zeros(len(X_train))\n",
    "                \n",
    "                for train_idx, val_idx in cv.split(X_train, y_train):\n",
    "                    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "                    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "                    \n",
    "                    model.fit(X_tr, y_tr)\n",
    "                    oof_pred[val_idx] = model.predict_proba(X_val)[:, 1]\n",
    "                \n",
    "                oof_predictions[:, i] = oof_pred\n",
    "                model.fit(X_train, y_train)\n",
    "                test_predictions[:, i] = model.predict_proba(X_test)[:, 1]\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    âŒ {name} ì‹¤íŒ¨: {e}\")\n",
    "                oof_predictions[:, i] = 0.12\n",
    "                test_predictions[:, i] = 0.12\n",
    "        \n",
    "        # GPU ë©”íƒ€ ëª¨ë¸ (RAPIDS)\n",
    "        if RAPIDS_AVAILABLE:\n",
    "            try:\n",
    "                print(\"  ğŸš€ RAPIDS GPU ë©”íƒ€ëª¨ë¸ í›ˆë ¨...\")\n",
    "                oof_gpu = cudf.from_pandas(pd.DataFrame(oof_predictions))\n",
    "                y_gpu = cudf.from_pandas(y_train)\n",
    "                \n",
    "                meta_model = cuLR(class_weight='balanced')\n",
    "                meta_model.fit(oof_gpu, y_gpu)\n",
    "                \n",
    "                test_gpu = cudf.from_pandas(pd.DataFrame(test_predictions))\n",
    "                final_proba = meta_model.predict_proba(test_gpu)[:, 1].to_array()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    RAPIDS ë©”íƒ€ëª¨ë¸ ì‹¤íŒ¨: {e}, sklearn ì‚¬ìš©\")\n",
    "                meta_model = LogisticRegression(random_state=42, class_weight='balanced', C=0.1)\n",
    "                meta_model.fit(oof_predictions, y_train)\n",
    "                final_proba = meta_model.predict_proba(test_predictions)[:, 1]\n",
    "        else:\n",
    "            meta_model = LogisticRegression(random_state=42, class_weight='balanced', C=0.1)\n",
    "            meta_model.fit(oof_predictions, y_train)\n",
    "            final_proba = meta_model.predict_proba(test_predictions)[:, 1]\n",
    "        \n",
    "        final_predictions = (final_proba > 0.5).astype(int)\n",
    "        \n",
    "        submission = pd.DataFrame({'ID': test_ids, 'Cancer': final_predictions})\n",
    "        self.submissions['gpu_mega_ensemble'] = submission\n",
    "        \n",
    "        print(\"  âœ… 100ê°œ ëª¨ë¸ GPU ì•™ìƒë¸” ì™„ë£Œ!\")\n",
    "        return submission\n",
    "    \n",
    "    def strategy_3_optuna_extreme_optimization(self):\n",
    "        \"\"\"ì „ëµ 3: Optuna ê·¹í•œ ìµœì í™” (1000íšŒ ì‹œë„)\"\"\"\n",
    "        if not OPTUNA_AVAILABLE:\n",
    "            print(\"\\nâŒ Optuna ì—†ìŒ - ì „ëµ 3 ìŠ¤í‚µ\")\n",
    "            return None\n",
    "            \n",
    "        print(\"\\nğŸ¯ ì „ëµ 3: Optuna ê·¹í•œ ìµœì í™” (1000íšŒ ì‹œë„)!\")\n",
    "        print(\"  H200 íŒŒì›Œë¡œ 1000íšŒ ë² ì´ì§€ì•ˆ ìµœì í™”...\")\n",
    "        \n",
    "        X_train, y_train, X_test, test_ids = self.load_and_ultimate_preprocess(42)\n",
    "        \n",
    "        pos_count = (y_train == 1).sum()\n",
    "        neg_count = (y_train == 0).sum()\n",
    "        scale_pos_weight = neg_count / pos_count\n",
    "        \n",
    "        def objective(trial):\n",
    "            # ë§¤ìš° ì„¸ë°€í•œ íŒŒë¼ë¯¸í„° íƒìƒ‰\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.3, log=True),\n",
    "                'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 0, 2.0),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 0, 2.0),\n",
    "                'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "                'gamma': trial.suggest_float('gamma', 0, 2.0),\n",
    "                'random_state': 42,\n",
    "                'scale_pos_weight': scale_pos_weight,\n",
    "                'tree_method': 'gpu_hist',\n",
    "                'gpu_id': 0,\n",
    "                'verbosity': 0\n",
    "            }\n",
    "            \n",
    "            model = xgb.XGBClassifier(**params)\n",
    "            \n",
    "            # ë¹ ë¥¸ CV (3-fold)\n",
    "            cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "            cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='f1', n_jobs=1)\n",
    "            \n",
    "            return cv_scores.mean()\n",
    "        \n",
    "        try:\n",
    "            # Optuna ì—°êµ¬ ìƒì„±\n",
    "            study = optuna.create_study(\n",
    "                direction='maximize',\n",
    "                pruner=optuna.pruners.MedianPruner(n_startup_trials=50, n_warmup_steps=10)\n",
    "            )\n",
    "            \n",
    "            # 1000íšŒ ìµœì í™” (H200 íŒŒì›Œë¡œ!)\n",
    "            study.optimize(objective, n_trials=1000, timeout=3600)  # 1ì‹œê°„ í•œë„\n",
    "            \n",
    "            best_params = study.best_params\n",
    "            best_score = study.best_value\n",
    "            \n",
    "            print(f\"  âœ… Optuna ìµœê³  ì ìˆ˜: {best_score:.6f}\")\n",
    "            print(f\"  ìµœì  íŒŒë¼ë¯¸í„°: {best_params}\")\n",
    "            \n",
    "            # ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ìŠ¤íƒœí‚¹ ì•™ìƒë¸”\n",
    "            models = {\n",
    "                'xgb_optimized': xgb.XGBClassifier(**best_params),\n",
    "                'lgb_baseline': lgb.LGBMClassifier(n_estimators=200, max_depth=6, learning_rate=0.08, random_state=42, class_weight='balanced', device='gpu', verbosity=-1),\n",
    "                'cat_baseline': cb.CatBoostClassifier(iterations=200, depth=6, learning_rate=0.08, random_state=42, task_type='GPU', verbose=False)\n",
    "            }\n",
    "            \n",
    "            cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "            oof_predictions = np.zeros((len(X_train), len(models)))\n",
    "            test_predictions = np.zeros((len(X_test), len(models)))\n",
    "            \n",
    "            for i, (name, model) in enumerate(models.items()):\n",
    "                print(f\"  {name} í›ˆë ¨ ì¤‘...\")\n",
    "                oof_pred = np.zeros(len(X_train))\n",
    "                \n",
    "                for train_idx, val_idx in cv.split(X_train, y_train):\n",
    "                    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "                    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "                    \n",
    "                    model.fit(X_tr, y_tr)\n",
    "                    oof_pred[val_idx] = model.predict_proba(X_val)[:, 1]\n",
    "                \n",
    "                oof_predictions[:, i] = oof_pred\n",
    "                model.fit(X_train, y_train)\n",
    "                test_predictions[:, i] = model.predict_proba(X_test)[:, 1]\n",
    "            \n",
    "            # ë©”íƒ€ ëª¨ë¸\n",
    "            meta_model = LogisticRegression(random_state=42, class_weight='balanced', C=0.1)\n",
    "            meta_model.fit(oof_predictions, y_train)\n",
    "            \n",
    "            final_proba = meta_model.predict_proba(test_predictions)[:, 1]\n",
    "            final_predictions = (final_proba > 0.5).astype(int)\n",
    "            \n",
    "            submission = pd.DataFrame({'ID': test_ids, 'Cancer': final_predictions})\n",
    "            self.submissions['optuna_extreme'] = submission\n",
    "            return submission\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ Optuna ê·¹í•œ ìµœì í™” ì‹¤íŒ¨: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def strategy_4_h200_ultimate_fusion(self):\n",
    "        \"\"\"ì „ëµ 4: H200 ê¶ê·¹ í“¨ì „\"\"\"\n",
    "        print(\"\\nğŸ‘¹ ì „ëµ 4: H200 ê¶ê·¹ í“¨ì „!\")\n",
    "        \n",
    "        if len(self.submissions) < 2:\n",
    "            print(\"  ì¶©ë¶„í•œ ì „ëµì´ ì‹¤í–‰ë˜ì§€ ì•ŠìŒ\")\n",
    "            return None\n",
    "        \n",
    "        # ê°€ì¤‘ì¹˜ (GPU ê¸°ë°˜ ì „ëµë“¤ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜)\n",
    "        strategy_weights = {\n",
    "            'pytorch_deep': 1.5,        # ë”¥ëŸ¬ë‹\n",
    "            'gpu_mega_ensemble': 1.6,   # ê°€ì¥ ë†’ì€ ê°€ì¤‘ì¹˜\n",
    "            'optuna_extreme': 1.4,      # ê·¹í•œ ìµœì í™”\n",
    "        }\n",
    "        \n",
    "        weights = []\n",
    "        predictions = []\n",
    "        \n",
    "        for name, submission in self.submissions.items():\n",
    "            if name in strategy_weights:\n",
    "                predictions.append(submission['Cancer'].values)\n",
    "                weights.append(strategy_weights[name])\n",
    "                print(f\"  {name}: ê°€ì¤‘ì¹˜ {strategy_weights[name]}\")\n",
    "        \n",
    "        if len(predictions) == 0:\n",
    "            print(\"  ìœ íš¨í•œ ì „ëµì´ ì—†ìŒ\")\n",
    "            return None\n",
    "        \n",
    "        # ê°€ì¤‘ í‰ê· \n",
    "        weighted_avg = np.average(predictions, axis=0, weights=weights)\n",
    "        \n",
    "        # ì„ê³„ê°’ ë¯¸ì„¸ ì¡°ì •\n",
    "        thresholds = np.arange(0.45, 0.55, 0.001)\n",
    "        best_threshold = 0.5\n",
    "        \n",
    "        for threshold in thresholds:\n",
    "            final_predictions = (weighted_avg >= threshold).astype(int)\n",
    "            pred_ratio = final_predictions.mean()\n",
    "            \n",
    "            # 12% Â± 0.3% ë²”ìœ„ ë‚´ì—ì„œ ì„ íƒ\n",
    "            if abs(pred_ratio - 0.12) < 0.003:\n",
    "                best_threshold = threshold\n",
    "                print(f\"  âœ… ì„ê³„ê°’ {threshold:.3f} ì„ íƒ (ë¹„ìœ¨: {pred_ratio:.4f})\")\n",
    "                break\n",
    "        \n",
    "        final_predictions = (weighted_avg >= best_threshold).astype(int)\n",
    "        \n",
    "        test_ids = list(self.submissions.values())[0]['ID']\n",
    "        submission = pd.DataFrame({'ID': test_ids, 'Cancer': final_predictions})\n",
    "        \n",
    "        self.submissions['h200_ultimate'] = submission\n",
    "        return submission\n",
    "\n",
    "def run_h200_monster_hunt():\n",
    "    \"\"\"H200 ê´´ë¬¼ ì‚¬ëƒ¥ ì‹¤í–‰\"\"\"\n",
    "    print(\"ğŸ‘¹ H200 GPU í’€íŒŒì›Œ ê´´ë¬¼ ì‚¬ëƒ¥!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ğŸ”¥ NVIDIA H200 + 140GB ë©”ëª¨ë¦¬ í’€ê°€ë™!\")\n",
    "    print(\"ğŸ¯ ëª©í‘œ: 0.5120+ (ê´´ë¬¼ ì™„ì „ ì œì••!)\")\n",
    "    print(\"âš¡ GPU ê°€ì† ë”¥ëŸ¬ë‹ + ë©”ê°€ ì•™ìƒë¸” + ê·¹í•œ ìµœì í™”\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    killer = H200MonsterKiller()\n",
    "    \n",
    "    strategies = [\n",
    "        (\"PyTorch ë”¥ëŸ¬ë‹\", killer.strategy_1_pytorch_deep_learning),\n",
    "        (\"GPU ë©”ê°€ ì•™ìƒë¸”\", killer.strategy_2_gpu_mega_ensemble),\n",
    "        (\"Optuna ê·¹í•œ ìµœì í™”\", killer.strategy_3_optuna_extreme_optimization),\n",
    "        (\"H200 ê¶ê·¹ í“¨ì „\", killer.strategy_4_h200_ultimate_fusion)\n",
    "    ]\n",
    "    \n",
    "    for i, (name, strategy_func) in enumerate(strategies, 1):\n",
    "        print(f\"\\n{'='*20} {name} {'='*20}\")\n",
    "        strategy_start = time.time()\n",
    "        \n",
    "        try:\n",
    "            result = strategy_func()\n",
    "            if result is not None:\n",
    "                filename = f'h200_kill_{i}.csv'\n",
    "                result.to_csv(filename, index=False)\n",
    "                print(f\"  ğŸ’¾ ì €ì¥: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ {name} ì‹¤íŒ¨: {e}\")\n",
    "        \n",
    "        strategy_time = time.time() - strategy_start\n",
    "        print(f\"  â±ï¸ ì†Œìš” ì‹œê°„: {strategy_time:.1f}ì´ˆ\")\n",
    "    \n",
    "    # ìµœì¢… H200 í‚¬ëŸ¬\n",
    "    if 'h200_ultimate' in killer.submissions:\n",
    "        best_submission = killer.submissions['h200_ultimate']\n",
    "        best_submission.to_csv('H200_MONSTER_DESTROYER.csv', index=False)\n",
    "        print(f\"\\nğŸ‘¹ H200 ê´´ë¬¼ íŒŒê´´ì: H200_MONSTER_DESTROYER.csv\")\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\nğŸ† H200 ì‘ì „ ì™„ë£Œ!\")\n",
    "    print(f\"â±ï¸ ì´ ì†Œìš” ì‹œê°„: {total_time:.1f}ì´ˆ\")\n",
    "    print(f\"ğŸ”¥ GPU íŒŒì›Œ: 100% í™œìš©\")\n",
    "    print(f\"ğŸ“ ìƒì„±ëœ íŒŒì¼:\")\n",
    "    print(f\"  1. H200_MONSTER_DESTROYER.csv â­ (ìµœì¢… ë³´ìŠ¤)\")\n",
    "    print(f\"  2. h200_kill_2.csv (100ê°œ ëª¨ë¸ ë©”ê°€ ì•™ìƒë¸”)\")\n",
    "    print(f\"  3. h200_kill_1.csv (PyTorch ë”¥ëŸ¬ë‹)\")\n",
    "    print(f\"  4. h200_kill_3.csv (1000íšŒ Optuna)\")\n",
    "    \n",
    "    print(f\"\\nğŸ¯ ê¸°ëŒ€ ê²°ê³¼:\")\n",
    "    print(f\"  - 0.5112+: GPU ë”¥ëŸ¬ë‹ íš¨ê³¼!\")\n",
    "    print(f\"  - 0.5116+: ë©”ê°€ ì•™ìƒë¸” íŒŒì›Œ!\")\n",
    "    print(f\"  - 0.5120+: ì§„ì§œ ê´´ë¬¼ ì™„ì „ ì œì••! ğŸ‘¹ğŸ’€\")\n",
    "    \n",
    "    print(f\"\\nğŸ”¥ H200ì˜ ì§„ì§œ íŒŒì›Œë¥¼ ë³´ì—¬ì¤¬ìŠµë‹ˆë‹¤!\")\n",
    "    print(f\"ğŸ’ª 140GB ë©”ëª¨ë¦¬ + GPU ê°€ì†ìœ¼ë¡œ ëª¨ë“  ê±¸ ì‹œë„í–ˆìŠµë‹ˆë‹¤!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_h200_monster_hunt()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
