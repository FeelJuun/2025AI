{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f467480-ba59-444c-ac30-de4eaf66c9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏥 갑상선암 진단 AI 모델 개발\n",
      "==================================================\n",
      "🚀 갑상선암 예측 파이프라인 시작!\n",
      "============================================================\n",
      "📊 데이터 로딩 중...\n",
      "✅ 훈련 데이터: (87159, 16)\n",
      "✅ 테스트 데이터: (46204, 15)\n",
      "✅ Feature 개수: 14\n",
      "\n",
      "📈 기본 EDA 수행 중...\n",
      "\n",
      "클래스 분포:\n",
      "  클래스 0: 76,700개 (88.0%)\n",
      "  클래스 1: 10,459개 (12.0%)\n",
      "\n",
      "결측값 확인:\n",
      "\n",
      "수치형 변수: 5개\n",
      "카테고리컬 변수: 9개\n",
      "\n",
      "🔧 특성 전처리 중...\n",
      "📋 결측값 처리 중...\n",
      "🏷️  카테고리컬 변수 인코딩 중...\n",
      "  Gender: 2개 클래스 인코딩 완료\n",
      "  Country: 10개 클래스 인코딩 완료\n",
      "  Race: 5개 클래스 인코딩 완료\n",
      "  Family_Background: 2개 클래스 인코딩 완료\n",
      "  Radiation_History: 2개 클래스 인코딩 완료\n",
      "  Iodine_Deficiency: 2개 클래스 인코딩 완료\n",
      "  Smoke: 2개 클래스 인코딩 완료\n",
      "  Weight_Risk: 2개 클래스 인코딩 완료\n",
      "  Diabetes: 2개 클래스 인코딩 완료\n",
      "⚙️  Feature Engineering 수행 중...\n",
      "📏 수치형 변수 스케일링 중...\n",
      "✅ 전처리 완료: (87159, 20)\n",
      "✅ 최종 feature 개수: 20\n",
      "🔄 테스트 데이터 전처리 중...\n",
      "✅ 테스트 데이터 전처리 완료: (46204, 20)\n",
      "\n",
      "🎯 전체 모델 훈련 시작...\n",
      "\n",
      "🤖 XGB 모델 훈련 중...\n",
      "CV F1 Score: 0.00946 (+/- 0.00783)\n",
      "\n",
      "🤖 XGB 모델 훈련 중...\n",
      "SMOTE 적용: (87159, 20) → (153400, 20)\n",
      "CV F1 Score: 0.92438 (+/- 0.00165)\n",
      "\n",
      "🤖 LGB 모델 훈련 중...\n",
      "CV F1 Score: 0.41890 (+/- 0.01299)\n",
      "\n",
      "🤖 LGB 모델 훈련 중...\n",
      "SMOTE 적용: (87159, 20) → (153400, 20)\n",
      "CV F1 Score: 0.92657 (+/- 0.00105)\n",
      "\n",
      "🤖 CATBOOST 모델 훈련 중...\n",
      "CV F1 Score: 0.46614 (+/- 0.00410)\n",
      "\n",
      "🤖 RF 모델 훈련 중...\n",
      "CV F1 Score: 0.46327 (+/- 0.01164)\n",
      "\n",
      "📊 모델별 CV 성능:\n",
      "  lgb: 0.92657\n",
      "  lgb_smote: 0.92657\n",
      "  xgb: 0.92438\n",
      "  xgb_smote: 0.92438\n",
      "  catboost: 0.46614\n",
      "  rf: 0.46327\n",
      "\n",
      "🔄 VOTING 앙상블 생성 중...\n",
      "앙상블 CV F1 Score: 0.28125 (+/- 0.00961)\n",
      "\n",
      "📤 예측 및 제출 파일 생성 중...\n",
      "✅ 선택된 모델: lgb (CV F1: 0.92657)\n",
      "✅ 제출 파일 저장: submission.csv\n",
      "\n",
      "예측 분포:\n",
      "  클래스 0: 42,604개 (92.2%)\n",
      "  클래스 1: 3,600개 (7.8%)\n",
      "✅ 모델들이 models에 저장되었습니다.\n",
      "\n",
      "🎉 파이프라인 완료!\n",
      "==================================================\n",
      "\n",
      "📊 최종 결과 요약:\n",
      "  lgb: CV F1 = 0.92657\n",
      "  lgb_smote: CV F1 = 0.92657\n",
      "  xgb: CV F1 = 0.92438\n",
      "  xgb_smote: CV F1 = 0.92438\n",
      "  catboost: CV F1 = 0.46614\n",
      "  rf: CV F1 = 0.46327\n",
      "  ensemble: CV F1 = 0.28125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n# 특성 중요도 분석\\nif 'xgb' in predictor.trainer.models:\\n    analyze_feature_importance(\\n        predictor.trainer.models['xgb'], \\n        predictor.X_train.columns\\n    )\\n\\n# 임계값 최적화 (검증 데이터가 있는 경우)\\n# best_threshold, best_f1 = cross_validate_threshold(\\n#     predictor.trainer.models['xgb'], \\n#     X_val, y_val\\n# )\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ================================================================================================\n",
    "# 갑상선암 진단 대회 - 완전한 솔루션\n",
    "# 전략: 안정적이고 재현 가능한 앙상블 기반 접근법\n",
    "# ================================================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 머신러닝 라이브러리\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# 그래디언트 부스팅\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "\n",
    "# 불균형 데이터 처리\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "# 하이퍼파라미터 튜닝\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ================================================================================================\n",
    "# 1. 설정 및 유틸리티 함수\n",
    "# ================================================================================================\n",
    "\n",
    "# 재현성을 위한 시드 고정\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "def set_seeds(seed=42):\n",
    "    \"\"\"모든 라이브러리의 시드를 고정하는 함수\"\"\"\n",
    "    import random\n",
    "    import os\n",
    "    \n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "class Config:\n",
    "    \"\"\"설정 클래스\"\"\"\n",
    "    RANDOM_STATE = 42\n",
    "    N_SPLITS = 5\n",
    "    EARLY_STOPPING_ROUNDS = 100\n",
    "    VERBOSE = False\n",
    "    \n",
    "    # 모델별 기본 파라미터\n",
    "    XGB_PARAMS = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'logloss',\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'n_estimators': 1000,\n",
    "        'learning_rate': 0.05,\n",
    "        'max_depth': 6,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'reg_alpha': 0.1,\n",
    "        'reg_lambda': 0.1\n",
    "    }\n",
    "    \n",
    "    LGB_PARAMS = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'n_estimators': 1000,\n",
    "        'learning_rate': 0.05,\n",
    "        'max_depth': 6,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'reg_alpha': 0.1,\n",
    "        'reg_lambda': 0.1,\n",
    "        'verbose': -1\n",
    "    }\n",
    "    \n",
    "    CB_PARAMS = {\n",
    "        'objective': 'Logloss',\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'iterations': 1000,\n",
    "        'learning_rate': 0.05,\n",
    "        'depth': 6,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bylevel': 0.8,\n",
    "        'reg_lambda': 0.1,\n",
    "        'verbose': False\n",
    "    }\n",
    "\n",
    "set_seeds(Config.RANDOM_STATE)\n",
    "\n",
    "# ================================================================================================\n",
    "# 2. 데이터 로딩 및 기본 분석\n",
    "# ================================================================================================\n",
    "\n",
    "class DataLoader:\n",
    "    \"\"\"데이터 로딩 및 기본 전처리 클래스\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.train_data = None\n",
    "        self.test_data = None\n",
    "        self.feature_columns = None\n",
    "        self.target_column = 'Cancer'\n",
    "        \n",
    "    def load_data(self, train_path='train.csv', test_path='test.csv'):\n",
    "        \"\"\"데이터 로딩\"\"\"\n",
    "        print(\"📊 데이터 로딩 중...\")\n",
    "        \n",
    "        self.train_data = pd.read_csv(train_path)\n",
    "        self.test_data = pd.read_csv(test_path)\n",
    "        \n",
    "        # Feature columns 정의 (ID와 target 제외)\n",
    "        self.feature_columns = [col for col in self.train_data.columns \n",
    "                               if col not in ['ID', self.target_column]]\n",
    "        \n",
    "        print(f\"✅ 훈련 데이터: {self.train_data.shape}\")\n",
    "        print(f\"✅ 테스트 데이터: {self.test_data.shape}\")\n",
    "        print(f\"✅ Feature 개수: {len(self.feature_columns)}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def basic_eda(self):\n",
    "        \"\"\"기본 EDA 수행\"\"\"\n",
    "        print(\"\\n📈 기본 EDA 수행 중...\")\n",
    "        \n",
    "        # 클래스 분포\n",
    "        class_dist = self.train_data[self.target_column].value_counts()\n",
    "        print(f\"\\n클래스 분포:\")\n",
    "        for cls, count in class_dist.items():\n",
    "            pct = count / len(self.train_data) * 100\n",
    "            print(f\"  클래스 {cls}: {count:,}개 ({pct:.1f}%)\")\n",
    "        \n",
    "        # 결측값 확인\n",
    "        print(f\"\\n결측값 확인:\")\n",
    "        missing_train = self.train_data.isnull().sum()\n",
    "        missing_test = self.test_data.isnull().sum()\n",
    "        \n",
    "        for col in self.feature_columns:\n",
    "            train_missing = missing_train[col] if col in missing_train.index else 0\n",
    "            test_missing = missing_test[col] if col in missing_test.index else 0\n",
    "            if train_missing > 0 or test_missing > 0:\n",
    "                print(f\"  {col}: 훈련({train_missing}) 테스트({test_missing})\")\n",
    "        \n",
    "        # 수치형/카테고리컬 변수 분리\n",
    "        numeric_cols = self.train_data[self.feature_columns].select_dtypes(\n",
    "            include=[np.number]).columns.tolist()\n",
    "        categorical_cols = [col for col in self.feature_columns if col not in numeric_cols]\n",
    "        \n",
    "        print(f\"\\n수치형 변수: {len(numeric_cols)}개\")\n",
    "        print(f\"카테고리컬 변수: {len(categorical_cols)}개\")\n",
    "        \n",
    "        return numeric_cols, categorical_cols\n",
    "\n",
    "# ================================================================================================\n",
    "# 3. 전처리 파이프라인\n",
    "# ================================================================================================\n",
    "\n",
    "class FeatureProcessor:\n",
    "    \"\"\"특성 전처리 및 엔지니어링 클래스\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.label_encoders = {}\n",
    "        self.scaler = StandardScaler()\n",
    "        self.numeric_cols = []\n",
    "        self.categorical_cols = []\n",
    "        \n",
    "    def fit_transform(self, train_data, feature_columns, target_column='Cancer'):\n",
    "        \"\"\"훈련 데이터에 fit하고 transform\"\"\"\n",
    "        print(\"\\n🔧 특성 전처리 중...\")\n",
    "        \n",
    "        X_train = train_data[feature_columns].copy()\n",
    "        y_train = train_data[target_column].copy()\n",
    "        \n",
    "        # 결측값 사전 처리\n",
    "        print(\"📋 결측값 처리 중...\")\n",
    "        \n",
    "        # 수치형/카테고리컬 변수 분리\n",
    "        self.numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        self.categorical_cols = [col for col in feature_columns if col not in self.numeric_cols]\n",
    "        \n",
    "        # 수치형 변수 결측값 처리 (중앙값으로 대체)\n",
    "        for col in self.numeric_cols:\n",
    "            if X_train[col].isnull().any():\n",
    "                median_val = X_train[col].median()\n",
    "                X_train[col].fillna(median_val, inplace=True)\n",
    "                print(f\"  {col}: 결측값을 {median_val:.3f}로 대체\")\n",
    "        \n",
    "        # 카테고리컬 변수 결측값 처리 (최빈값으로 대체)\n",
    "        for col in self.categorical_cols:\n",
    "            if X_train[col].isnull().any() or X_train[col].isna().any():\n",
    "                mode_val = X_train[col].mode().iloc[0] if len(X_train[col].mode()) > 0 else 'Unknown'\n",
    "                X_train[col].fillna(mode_val, inplace=True)\n",
    "                print(f\"  {col}: 결측값을 '{mode_val}'로 대체\")\n",
    "        \n",
    "        # 1. 카테고리컬 변수 인코딩\n",
    "        print(\"🏷️  카테고리컬 변수 인코딩 중...\")\n",
    "        for col in self.categorical_cols:\n",
    "            le = LabelEncoder()\n",
    "            # 문자열로 변환하여 안전하게 처리\n",
    "            X_train[col] = X_train[col].astype(str)\n",
    "            X_train[col] = le.fit_transform(X_train[col])\n",
    "            self.label_encoders[col] = le\n",
    "            print(f\"  {col}: {len(le.classes_)}개 클래스 인코딩 완료\")\n",
    "        \n",
    "        # 2. Feature Engineering\n",
    "        print(\"⚙️  Feature Engineering 수행 중...\")\n",
    "        X_train = self._feature_engineering(X_train)\n",
    "        \n",
    "        # 3. 최종 수치형 변수들 확인 및 스케일링\n",
    "        numeric_cols_final = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        \n",
    "        # 무한대나 NaN 값 최종 체크\n",
    "        for col in numeric_cols_final:\n",
    "            # 무한대 값 처리\n",
    "            inf_mask = np.isinf(X_train[col])\n",
    "            if inf_mask.any():\n",
    "                print(f\"  {col}: {inf_mask.sum()}개 무한대 값 발견, 중앙값으로 대체\")\n",
    "                X_train.loc[inf_mask, col] = X_train[col][~inf_mask].median()\n",
    "            \n",
    "            # NaN 값 최종 처리\n",
    "            nan_mask = X_train[col].isnull()\n",
    "            if nan_mask.any():\n",
    "                print(f\"  {col}: {nan_mask.sum()}개 NaN 값 발견, 중앙값으로 대체\")\n",
    "                X_train.loc[nan_mask, col] = X_train[col][~nan_mask].median()\n",
    "        \n",
    "        # 스케일링\n",
    "        print(\"📏 수치형 변수 스케일링 중...\")\n",
    "        X_train[numeric_cols_final] = self.scaler.fit_transform(X_train[numeric_cols_final])\n",
    "        \n",
    "        print(f\"✅ 전처리 완료: {X_train.shape}\")\n",
    "        print(f\"✅ 최종 feature 개수: {len(X_train.columns)}\")\n",
    "        \n",
    "        return X_train, y_train\n",
    "    \n",
    "    def transform(self, data, feature_columns):\n",
    "        \"\"\"테스트 데이터 transform\"\"\"\n",
    "        print(\"🔄 테스트 데이터 전처리 중...\")\n",
    "        X_test = data[feature_columns].copy()\n",
    "        \n",
    "        # 결측값 사전 처리 (훈련 데이터와 동일한 방식)\n",
    "        # 수치형 변수 결측값 처리\n",
    "        for col in self.numeric_cols:\n",
    "            if X_test[col].isnull().any():\n",
    "                # 훈련 데이터의 중앙값 사용 (scaler에서 추출)\n",
    "                if hasattr(self.scaler, 'scale_') and col in X_test.columns:\n",
    "                    # 스케일러가 fit되어 있다면 mean 값 사용\n",
    "                    fill_value = 0  # 스케일링 후 평균값\n",
    "                else:\n",
    "                    fill_value = X_test[col].median()\n",
    "                X_test[col].fillna(fill_value, inplace=True)\n",
    "        \n",
    "        # 카테고리컬 변수 결측값 처리\n",
    "        for col in self.categorical_cols:\n",
    "            if X_test[col].isnull().any() or X_test[col].isna().any():\n",
    "                # 훈련 데이터에서 가장 많았던 값으로 대체 (첫 번째 클래스)\n",
    "                if col in self.label_encoders:\n",
    "                    most_common = self.label_encoders[col].classes_[0]\n",
    "                    X_test[col].fillna(most_common, inplace=True)\n",
    "                else:\n",
    "                    X_test[col].fillna('Unknown', inplace=True)\n",
    "        \n",
    "        # 1. 카테고리컬 변수 인코딩\n",
    "        for col in self.categorical_cols:\n",
    "            if col in self.label_encoders:\n",
    "                # 문자열로 변환\n",
    "                X_test[col] = X_test[col].astype(str)\n",
    "                \n",
    "                # 훈련 데이터에 없던 값은 가장 빈도가 높은 값으로 처리\n",
    "                unknown_mask = ~X_test[col].isin(self.label_encoders[col].classes_)\n",
    "                if unknown_mask.any():\n",
    "                    most_common = self.label_encoders[col].classes_[0]\n",
    "                    X_test.loc[unknown_mask, col] = most_common\n",
    "                    print(f\"  {col}: {unknown_mask.sum()}개 미지의 값을 '{most_common}'로 대체\")\n",
    "                \n",
    "                X_test[col] = self.label_encoders[col].transform(X_test[col])\n",
    "        \n",
    "        # 2. Feature Engineering\n",
    "        X_test = self._feature_engineering(X_test)\n",
    "        \n",
    "        # 3. 최종 안전성 체크\n",
    "        numeric_cols_final = X_test.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        \n",
    "        # 무한대나 NaN 값 최종 체크\n",
    "        for col in numeric_cols_final:\n",
    "            # 무한대 값 처리\n",
    "            inf_mask = np.isinf(X_test[col])\n",
    "            if inf_mask.any():\n",
    "                X_test.loc[inf_mask, col] = 0  # 스케일링된 평균값\n",
    "            \n",
    "            # NaN 값 최종 처리\n",
    "            nan_mask = X_test[col].isnull()\n",
    "            if nan_mask.any():\n",
    "                X_test.loc[nan_mask, col] = 0  # 스케일링된 평균값\n",
    "        \n",
    "        # 4. 수치형 변수 스케일링\n",
    "        X_test[numeric_cols_final] = self.scaler.transform(X_test[numeric_cols_final])\n",
    "        \n",
    "        print(f\"✅ 테스트 데이터 전처리 완료: {X_test.shape}\")\n",
    "        return X_test\n",
    "    \n",
    "    def _feature_engineering(self, data):\n",
    "        \"\"\"피처 엔지니어링\"\"\"\n",
    "        # 기본적인 피처 엔지니어링\n",
    "        data = data.copy()\n",
    "        \n",
    "        # Age 그룹화 (NaN 안전 처리)\n",
    "        if 'Age' in data.columns:\n",
    "            # NaN이 아닌 값들만 처리\n",
    "            age_mask = data['Age'].notna()\n",
    "            data['Age_Group'] = 0  # 기본값\n",
    "            \n",
    "            if age_mask.any():\n",
    "                age_groups = pd.cut(data.loc[age_mask, 'Age'], \n",
    "                                  bins=[0, 30, 50, 70, 100], \n",
    "                                  labels=[0, 1, 2, 3],\n",
    "                                  include_lowest=True)\n",
    "                data.loc[age_mask, 'Age_Group'] = age_groups.astype(int)\n",
    "        \n",
    "        # 호르몬 수치 비율 (안전한 나눗셈)\n",
    "        if all(col in data.columns for col in ['TSH_Result', 'T4_Result', 'T3_Result']):\n",
    "            # NaN 값 처리\n",
    "            tsh_safe = data['TSH_Result'].fillna(data['TSH_Result'].median())\n",
    "            t4_safe = data['T4_Result'].fillna(data['T4_Result'].median())\n",
    "            t3_safe = data['T3_Result'].fillna(data['T3_Result'].median())\n",
    "            \n",
    "            data['TSH_T4_ratio'] = tsh_safe / (t4_safe + 1e-8)\n",
    "            data['T3_T4_ratio'] = t3_safe / (t4_safe + 1e-8)\n",
    "            \n",
    "            # 극값 처리 (이상치 제거)\n",
    "            data['TSH_T4_ratio'] = np.clip(data['TSH_T4_ratio'], 0, 10)\n",
    "            data['T3_T4_ratio'] = np.clip(data['T3_T4_ratio'], 0, 5)\n",
    "        \n",
    "        # 결절 크기 그룹화 (NaN 안전 처리)\n",
    "        if 'Nodule_Size' in data.columns:\n",
    "            # NaN이 아닌 값들만 처리\n",
    "            nodule_mask = data['Nodule_Size'].notna()\n",
    "            data['Nodule_Size_Group'] = 0  # 기본값 (가장 작은 그룹)\n",
    "            \n",
    "            if nodule_mask.any():\n",
    "                # 실제 데이터 범위에 맞게 bins 조정\n",
    "                min_size = data['Nodule_Size'].min()\n",
    "                max_size = data['Nodule_Size'].max()\n",
    "                \n",
    "                # 더 안전한 bins 설정\n",
    "                bins = [min_size - 0.1, 1, 2, 3, max_size + 0.1]\n",
    "                nodule_groups = pd.cut(data.loc[nodule_mask, 'Nodule_Size'], \n",
    "                                     bins=bins, \n",
    "                                     labels=[0, 1, 2, 3],\n",
    "                                     include_lowest=True)\n",
    "                data.loc[nodule_mask, 'Nodule_Size_Group'] = nodule_groups.astype(int)\n",
    "        \n",
    "        # 추가 안전 피처들\n",
    "        if 'Age' in data.columns:\n",
    "            data['Age_squared'] = data['Age'] ** 2\n",
    "            data['Age_log'] = np.log1p(data['Age'])  # log(1+x) - 0 방지\n",
    "        \n",
    "        return data\n",
    "\n",
    "# ================================================================================================\n",
    "# 4. 모델 클래스들\n",
    "# ================================================================================================\n",
    "\n",
    "class ModelTrainer:\n",
    "    \"\"\"모델 훈련 및 예측 클래스\"\"\"\n",
    "    \n",
    "    def __init__(self, config=Config()):\n",
    "        self.config = config\n",
    "        self.models = {}\n",
    "        self.cv_scores = {}\n",
    "        \n",
    "    def train_single_model(self, X_train, y_train, model_type='xgb', \n",
    "                          class_weight=None, use_smote=False):\n",
    "        \"\"\"단일 모델 훈련\"\"\"\n",
    "        print(f\"\\n🤖 {model_type.upper()} 모델 훈련 중...\")\n",
    "        \n",
    "        # 클래스 불균형 처리\n",
    "        if use_smote:\n",
    "            smote = SMOTE(random_state=self.config.RANDOM_STATE)\n",
    "            X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "            print(f\"SMOTE 적용: {X_train.shape} → {X_train_balanced.shape}\")\n",
    "        else:\n",
    "            X_train_balanced, y_train_balanced = X_train, y_train\n",
    "        \n",
    "        # 모델 생성\n",
    "        if model_type == 'xgb':\n",
    "            params = self.config.XGB_PARAMS.copy()\n",
    "            if class_weight:\n",
    "                scale_pos_weight = class_weight[0] / class_weight[1]\n",
    "                params['scale_pos_weight'] = scale_pos_weight\n",
    "            model = xgb.XGBClassifier(**params)\n",
    "            \n",
    "        elif model_type == 'lgb':\n",
    "            params = self.config.LGB_PARAMS.copy()\n",
    "            if class_weight:\n",
    "                params['class_weight'] = 'balanced'\n",
    "            model = lgb.LGBMClassifier(**params)\n",
    "            \n",
    "        elif model_type == 'catboost':\n",
    "            params = self.config.CB_PARAMS.copy()\n",
    "            if class_weight:\n",
    "                params['class_weights'] = class_weight\n",
    "            model = cb.CatBoostClassifier(**params)\n",
    "            \n",
    "        elif model_type == 'rf':\n",
    "            model = RandomForestClassifier(\n",
    "                n_estimators=500,\n",
    "                max_depth=10,\n",
    "                random_state=self.config.RANDOM_STATE,\n",
    "                class_weight='balanced' if class_weight else None,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "        \n",
    "        # Cross Validation\n",
    "        cv = StratifiedKFold(n_splits=self.config.N_SPLITS, \n",
    "                           shuffle=True, \n",
    "                           random_state=self.config.RANDOM_STATE)\n",
    "        \n",
    "        cv_scores = cross_val_score(model, X_train_balanced, y_train_balanced, \n",
    "                                   cv=cv, scoring='f1', n_jobs=-1)\n",
    "        \n",
    "        print(f\"CV F1 Score: {cv_scores.mean():.5f} (+/- {cv_scores.std() * 2:.5f})\")\n",
    "        \n",
    "        # 전체 데이터로 훈련\n",
    "        model.fit(X_train_balanced, y_train_balanced)\n",
    "        \n",
    "        # 저장\n",
    "        self.models[model_type] = model\n",
    "        self.cv_scores[model_type] = cv_scores.mean()\n",
    "        \n",
    "        return model, cv_scores.mean()\n",
    "    \n",
    "    def train_all_models(self, X_train, y_train):\n",
    "        \"\"\"모든 모델 훈련\"\"\"\n",
    "        print(\"\\n🎯 전체 모델 훈련 시작...\")\n",
    "        \n",
    "        # 클래스 가중치 계산\n",
    "        class_weights = compute_class_weight('balanced', \n",
    "                                           classes=np.unique(y_train), \n",
    "                                           y=y_train)\n",
    "        class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
    "        \n",
    "        # 각 모델 훈련\n",
    "        model_configs = [\n",
    "            ('xgb', False, class_weight_dict),\n",
    "            ('xgb_smote', True, None),\n",
    "            ('lgb', False, class_weight_dict),\n",
    "            ('lgb_smote', True, None),\n",
    "            ('catboost', False, class_weight_dict),\n",
    "            ('rf', False, class_weight_dict)\n",
    "        ]\n",
    "        \n",
    "        for model_name, use_smote, class_weight in model_configs:\n",
    "            try:\n",
    "                base_name = model_name.replace('_smote', '')\n",
    "                self.train_single_model(X_train, y_train, \n",
    "                                      model_type=base_name,\n",
    "                                      class_weight=class_weight,\n",
    "                                      use_smote=use_smote)\n",
    "                if use_smote:\n",
    "                    # SMOTE 버전을 별도로 저장\n",
    "                    self.models[model_name] = self.models[base_name]\n",
    "                    self.cv_scores[model_name] = self.cv_scores[base_name]\n",
    "            except Exception as e:\n",
    "                print(f\"❌ {model_name} 훈련 실패: {e}\")\n",
    "        \n",
    "        # 결과 요약\n",
    "        print(f\"\\n📊 모델별 CV 성능:\")\n",
    "        for model_name, score in sorted(self.cv_scores.items(), \n",
    "                                       key=lambda x: x[1], reverse=True):\n",
    "            print(f\"  {model_name}: {score:.5f}\")\n",
    "    \n",
    "    def create_ensemble(self, X_train, y_train, ensemble_type='voting'):\n",
    "        \"\"\"앙상블 모델 생성\"\"\"\n",
    "        print(f\"\\n🔄 {ensemble_type.upper()} 앙상블 생성 중...\")\n",
    "        \n",
    "        if len(self.models) < 2:\n",
    "            print(\"❌ 앙상블을 위한 충분한 모델이 없습니다.\")\n",
    "            return None\n",
    "        \n",
    "        # 상위 성능 모델들 선택\n",
    "        top_models = sorted(self.cv_scores.items(), \n",
    "                           key=lambda x: x[1], reverse=True)[:4]\n",
    "        \n",
    "        estimators = [(name, self.models[name]) for name, _ in top_models]\n",
    "        \n",
    "        if ensemble_type == 'voting':\n",
    "            # 가중 투표 (CV 성능 기반)\n",
    "            weights = [score for _, score in top_models]\n",
    "            ensemble = VotingClassifier(estimators=estimators, \n",
    "                                      voting='soft', \n",
    "                                      weights=weights)\n",
    "        \n",
    "        # 앙상블 모델 훈련\n",
    "        ensemble.fit(X_train, y_train)\n",
    "        \n",
    "        # CV 평가\n",
    "        cv = StratifiedKFold(n_splits=self.config.N_SPLITS, \n",
    "                           shuffle=True, \n",
    "                           random_state=self.config.RANDOM_STATE)\n",
    "        ensemble_scores = cross_val_score(ensemble, X_train, y_train, \n",
    "                                        cv=cv, scoring='f1', n_jobs=-1)\n",
    "        \n",
    "        print(f\"앙상블 CV F1 Score: {ensemble_scores.mean():.5f} (+/- {ensemble_scores.std() * 2:.5f})\")\n",
    "        \n",
    "        self.models['ensemble'] = ensemble\n",
    "        self.cv_scores['ensemble'] = ensemble_scores.mean()\n",
    "        \n",
    "        return ensemble\n",
    "\n",
    "# ================================================================================================\n",
    "# 5. 하이퍼파라미터 튜닝\n",
    "# ================================================================================================\n",
    "\n",
    "class HyperparameterTuner:\n",
    "    \"\"\"하이퍼파라미터 튜닝 클래스\"\"\"\n",
    "    \n",
    "    def __init__(self, random_state=42):\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def tune_xgboost(self, X_train, y_train, n_iter=50):\n",
    "        \"\"\"XGBoost 하이퍼파라미터 튜닝\"\"\"\n",
    "        print(\"\\n🎯 XGBoost 하이퍼파라미터 튜닝 중...\")\n",
    "        \n",
    "        param_dist = {\n",
    "            'n_estimators': [500, 1000, 1500],\n",
    "            'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "            'max_depth': [3, 4, 5, 6, 7],\n",
    "            'subsample': [0.7, 0.8, 0.9],\n",
    "            'colsample_bytree': [0.7, 0.8, 0.9],\n",
    "            'reg_alpha': [0, 0.1, 0.5],\n",
    "            'reg_lambda': [0.1, 0.5, 1.0]\n",
    "        }\n",
    "        \n",
    "        xgb_model = xgb.XGBClassifier(\n",
    "            objective='binary:logistic',\n",
    "            eval_metric='logloss',\n",
    "            random_state=self.random_state,\n",
    "            scale_pos_weight=7.3  # 클래스 불균형 비율\n",
    "        )\n",
    "        \n",
    "        cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=self.random_state)\n",
    "        \n",
    "        random_search = RandomizedSearchCV(\n",
    "            xgb_model, param_dist,\n",
    "            n_iter=n_iter,\n",
    "            scoring='f1',\n",
    "            cv=cv,\n",
    "            random_state=self.random_state,\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        random_search.fit(X_train, y_train)\n",
    "        \n",
    "        print(f\"✅ 최적 XGB 파라미터: {random_search.best_params_}\")\n",
    "        print(f\"✅ 최적 CV F1 Score: {random_search.best_score_:.5f}\")\n",
    "        \n",
    "        return random_search.best_estimator_, random_search.best_params_\n",
    "\n",
    "# ================================================================================================\n",
    "# 6. 메인 실행 클래스\n",
    "# ================================================================================================\n",
    "\n",
    "class ThyroidCancerPredictor:\n",
    "    \"\"\"갑상선암 예측 메인 클래스\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.data_loader = DataLoader()\n",
    "        self.processor = FeatureProcessor()\n",
    "        self.trainer = ModelTrainer()\n",
    "        self.tuner = HyperparameterTuner()\n",
    "        \n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        self.X_test = None\n",
    "        \n",
    "    def run_pipeline(self, train_path='train.csv', test_path='test.csv', \n",
    "                    tune_hyperparams=False):\n",
    "        \"\"\"전체 파이프라인 실행\"\"\"\n",
    "        print(\"🚀 갑상선암 예측 파이프라인 시작!\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # 1. 데이터 로딩\n",
    "        self.data_loader.load_data(train_path, test_path)\n",
    "        numeric_cols, categorical_cols = self.data_loader.basic_eda()\n",
    "        \n",
    "        # 2. 전처리\n",
    "        self.X_train, self.y_train = self.processor.fit_transform(\n",
    "            self.data_loader.train_data, \n",
    "            self.data_loader.feature_columns\n",
    "        )\n",
    "        \n",
    "        self.X_test = self.processor.transform(\n",
    "            self.data_loader.test_data,\n",
    "            self.data_loader.feature_columns\n",
    "        )\n",
    "        \n",
    "        # 3. 모델 훈련\n",
    "        self.trainer.train_all_models(self.X_train, self.y_train)\n",
    "        \n",
    "        # 4. 하이퍼파라미터 튜닝 (선택사항)\n",
    "        if tune_hyperparams:\n",
    "            best_xgb, best_params = self.tuner.tune_xgboost(self.X_train, self.y_train)\n",
    "            self.trainer.models['xgb_tuned'] = best_xgb\n",
    "            \n",
    "            # 튜닝된 모델 CV 평가\n",
    "            cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "            tuned_scores = cross_val_score(best_xgb, self.X_train, self.y_train,\n",
    "                                         cv=cv, scoring='f1', n_jobs=-1)\n",
    "            self.trainer.cv_scores['xgb_tuned'] = tuned_scores.mean()\n",
    "            print(f\"튜닝된 XGB CV F1: {tuned_scores.mean():.5f}\")\n",
    "        \n",
    "        # 5. 앙상블 생성\n",
    "        ensemble_model = self.trainer.create_ensemble(self.X_train, self.y_train)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict_and_submit(self, submission_path='submission.csv'):\n",
    "        \"\"\"예측 및 제출 파일 생성\"\"\"\n",
    "        print(\"\\n📤 예측 및 제출 파일 생성 중...\")\n",
    "        \n",
    "        # 최고 성능 모델 선택\n",
    "        best_model_name = max(self.trainer.cv_scores.items(), \n",
    "                             key=lambda x: x[1])[0]\n",
    "        best_model = self.trainer.models[best_model_name]\n",
    "        \n",
    "        print(f\"✅ 선택된 모델: {best_model_name} (CV F1: {self.trainer.cv_scores[best_model_name]:.5f})\")\n",
    "        \n",
    "        # 예측\n",
    "        predictions = best_model.predict(self.X_test)\n",
    "        \n",
    "        # 제출 파일 생성\n",
    "        submission = pd.DataFrame({\n",
    "            'ID': self.data_loader.test_data['ID'],\n",
    "            'Cancer': predictions\n",
    "        })\n",
    "        \n",
    "        submission.to_csv(submission_path, index=False)\n",
    "        print(f\"✅ 제출 파일 저장: {submission_path}\")\n",
    "        \n",
    "        # 예측 분포 확인\n",
    "        pred_dist = pd.Series(predictions).value_counts()\n",
    "        print(f\"\\n예측 분포:\")\n",
    "        for cls, count in pred_dist.items():\n",
    "            pct = count / len(predictions) * 100\n",
    "            print(f\"  클래스 {cls}: {count:,}개 ({pct:.1f}%)\")\n",
    "        \n",
    "        return submission\n",
    "    \n",
    "    def save_models(self, save_dir='models'):\n",
    "        \"\"\"모델 저장\"\"\"\n",
    "        save_path = Path(save_dir)\n",
    "        save_path.mkdir(exist_ok=True)\n",
    "        \n",
    "        for model_name, model in self.trainer.models.items():\n",
    "            model_path = save_path / f\"{model_name}_model.pkl\"\n",
    "            with open(model_path, 'wb') as f:\n",
    "                pickle.dump(model, f)\n",
    "        \n",
    "        # 전처리기도 저장\n",
    "        processor_path = save_path / \"processor.pkl\"\n",
    "        with open(processor_path, 'wb') as f:\n",
    "            pickle.dump(self.processor, f)\n",
    "        \n",
    "        print(f\"✅ 모델들이 {save_dir}에 저장되었습니다.\")\n",
    "\n",
    "# ================================================================================================\n",
    "# 7. 실행 코드\n",
    "# ================================================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"메인 실행 함수\"\"\"\n",
    "    print(\"🏥 갑상선암 진단 AI 모델 개발\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 예측기 생성 및 실행\n",
    "    predictor = ThyroidCancerPredictor()\n",
    "    \n",
    "    try:\n",
    "        # 전체 파이프라인 실행\n",
    "        predictor.run_pipeline(\n",
    "            train_path='train.csv',\n",
    "            test_path='test.csv',\n",
    "            tune_hyperparams=False  # 시간이 많으면 True로 설정\n",
    "        )\n",
    "        \n",
    "        # 예측 및 제출\n",
    "        submission = predictor.predict_and_submit('submission.csv')\n",
    "        \n",
    "        # 모델 저장\n",
    "        predictor.save_models('models')\n",
    "        \n",
    "        print(\"\\n🎉 파이프라인 완료!\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # 최종 결과 요약\n",
    "        print(\"\\n📊 최종 결과 요약:\")\n",
    "        for model_name, score in sorted(predictor.trainer.cv_scores.items(), \n",
    "                                       key=lambda x: x[1], reverse=True):\n",
    "            print(f\"  {model_name}: CV F1 = {score:.5f}\")\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"❌ 파일을 찾을 수 없습니다: {e}\")\n",
    "        print(\"💡 train.csv와 test.csv 파일이 현재 디렉터리에 있는지 확인하세요.\")\n",
    "        \n",
    "    except ValueError as e:\n",
    "        print(f\"❌ 데이터 처리 오류: {e}\")\n",
    "        print(\"💡 데이터 형식이나 결측값을 확인해보세요.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 예상치 못한 오류 발생: {e}\")\n",
    "        print(\"\\n🔍 상세 오류 정보:\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        # 디버깅 정보 제공\n",
    "        print(\"\\n🛠️  디버깅 도움말:\")\n",
    "        print(\"1. 데이터 파일이 올바른 형식인지 확인\")\n",
    "        print(\"2. 필요한 라이브러리가 모두 설치되었는지 확인\")\n",
    "        print(\"3. 메모리가 충분한지 확인\")\n",
    "        print(\"4. Python 버전이 3.7 이상인지 확인\")\n",
    "\n",
    "# 실행\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# ================================================================================================\n",
    "# 8. 추가 유틸리티 함수들\n",
    "# ================================================================================================\n",
    "\n",
    "def analyze_feature_importance(model, feature_names, top_k=20):\n",
    "    \"\"\"특성 중요도 분석\"\"\"\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(f\"\\n상위 {top_k}개 중요 특성:\")\n",
    "        print(importance_df.head(top_k))\n",
    "        \n",
    "        return importance_df\n",
    "    else:\n",
    "        print(\"이 모델은 특성 중요도를 지원하지 않습니다.\")\n",
    "        return None\n",
    "\n",
    "def cross_validate_threshold(model, X_val, y_val, thresholds=None):\n",
    "    \"\"\"최적 임계값 찾기\"\"\"\n",
    "    if thresholds is None:\n",
    "        thresholds = np.arange(0.3, 0.8, 0.05)\n",
    "    \n",
    "    best_threshold = 0.5\n",
    "    best_f1 = 0\n",
    "    \n",
    "    y_pred_proba = model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "        f1 = f1_score(y_val, y_pred)\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    print(f\"최적 임계값: {best_threshold:.3f} (F1: {best_f1:.5f})\")\n",
    "    return best_threshold, best_f1\n",
    "\n",
    "# 사용 예시 (선택사항)\n",
    "\"\"\"\n",
    "# 특성 중요도 분석\n",
    "if 'xgb' in predictor.trainer.models:\n",
    "    analyze_feature_importance(\n",
    "        predictor.trainer.models['xgb'], \n",
    "        predictor.X_train.columns\n",
    "    )\n",
    "\n",
    "# 임계값 최적화 (검증 데이터가 있는 경우)\n",
    "# best_threshold, best_f1 = cross_validate_threshold(\n",
    "#     predictor.trainer.models['xgb'], \n",
    "#     X_val, y_val\n",
    "# )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ade636ba-2f7d-4f14-864b-eac18fe4c779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 단순 베이스라인 시작!\n",
      "========================================\n",
      "📊 데이터 로딩...\n",
      "  Train: (87159, 16)\n",
      "  Test: (46204, 15)\n",
      "  클래스 0: 76,700개 (88.0%)\n",
      "  클래스 1: 10,459개 (12.0%)\n",
      "🔧 단순 전처리 시작...\n",
      "  Gender: 2개 클래스\n",
      "  Country: 10개 클래스\n",
      "  Race: 5개 클래스\n",
      "  Family_Background: 2개 클래스\n",
      "  Radiation_History: 2개 클래스\n",
      "  Iodine_Deficiency: 2개 클래스\n",
      "  Smoke: 2개 클래스\n",
      "  Weight_Risk: 2개 클래스\n",
      "  Diabetes: 2개 클래스\n",
      "✅ 전처리 완료: Train (87159, 14), Test (46204, 14)\n",
      "\n",
      "🤖 단순 모델 훈련...\n",
      "  Random Forest 훈련 중...\n",
      "    RF CV F1: 0.4736 ± 0.0079\n",
      "  XGBoost 훈련 중...\n",
      "    XGB CV F1: 0.4709 ± 0.0034\n",
      "  LightGBM 훈련 중...\n",
      "    LGB CV F1: 0.4765 ± 0.0037\n",
      "\n",
      "✅ 최고 모델: lgb (CV F1: 0.4765)\n",
      "\n",
      "📤 lgb 모델로 예측 중...\n",
      "\n",
      "예측 분포:\n",
      "  클래스 0: 40,221개 (87.1%)\n",
      "  클래스 1: 5,983개 (12.9%)\n",
      "✅ 제출 파일 저장: simple_submission.csv\n",
      "\n",
      "🎯 CV 점수 요약:\n",
      "  lgb: 0.4765\n",
      "  rf: 0.4736\n",
      "  xgb: 0.4709\n",
      "\n",
      "💡 만약 여전히 점수가 낮다면:\n",
      "1. 데이터에 문제가 있을 수 있음\n",
      "2. Public/Private 분할 문제\n",
      "3. 평가 지표 차이\n",
      "4. 다른 참가자들도 비슷한 점수일 가능성\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# 시드 고정\n",
    "np.random.seed(42)\n",
    "\n",
    "def simple_preprocessing(train_df, test_df):\n",
    "    \"\"\"초간단 전처리 - 안전하고 검증된 방법만\"\"\"\n",
    "    print(\"🔧 단순 전처리 시작...\")\n",
    "    \n",
    "    # Feature columns (ID와 Cancer 제외)\n",
    "    feature_cols = [col for col in train_df.columns if col not in ['ID', 'Cancer']]\n",
    "    \n",
    "    # 훈련/테스트 데이터 분리\n",
    "    X_train = train_df[feature_cols].copy()\n",
    "    y_train = train_df['Cancer'].copy()\n",
    "    X_test = test_df[feature_cols].copy()\n",
    "    \n",
    "    # 1. 카테고리컬 변수만 인코딩 (수치형은 그대로)\n",
    "    categorical_cols = X_train.select_dtypes(include=['object']).columns\n",
    "    label_encoders = {}\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        \n",
    "        # 훈련 데이터 인코딩\n",
    "        X_train[col] = le.fit_transform(X_train[col].astype(str))\n",
    "        \n",
    "        # 테스트 데이터 인코딩 (새로운 값은 0으로)\n",
    "        test_values = X_test[col].astype(str)\n",
    "        test_encoded = []\n",
    "        for val in test_values:\n",
    "            if val in le.classes_:\n",
    "                test_encoded.append(le.transform([val])[0])\n",
    "            else:\n",
    "                test_encoded.append(0)  # 새로운 값은 0\n",
    "        X_test[col] = test_encoded\n",
    "        \n",
    "        label_encoders[col] = le\n",
    "        print(f\"  {col}: {len(le.classes_)}개 클래스\")\n",
    "    \n",
    "    # 2. 결측값 단순 처리\n",
    "    # 수치형: 중앙값\n",
    "    numeric_cols = X_train.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        median_val = X_train[col].median()\n",
    "        X_train[col].fillna(median_val, inplace=True)\n",
    "        X_test[col].fillna(median_val, inplace=True)\n",
    "    \n",
    "    print(f\"✅ 전처리 완료: Train {X_train.shape}, Test {X_test.shape}\")\n",
    "    return X_train, y_train, X_test\n",
    "\n",
    "def train_simple_models(X_train, y_train):\n",
    "    \"\"\"간단한 모델들 훈련\"\"\"\n",
    "    print(\"\\n🤖 단순 모델 훈련...\")\n",
    "    \n",
    "    models = {}\n",
    "    cv_scores = {}\n",
    "    \n",
    "    # Cross Validation 설정\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    # 1. Random Forest (클래스 가중치만)\n",
    "    print(\"  Random Forest 훈련 중...\")\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        random_state=42,\n",
    "        class_weight='balanced',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf_scores = cross_val_score(rf, X_train, y_train, cv=cv, scoring='f1')\n",
    "    rf.fit(X_train, y_train)\n",
    "    models['rf'] = rf\n",
    "    cv_scores['rf'] = rf_scores.mean()\n",
    "    print(f\"    RF CV F1: {rf_scores.mean():.4f} ± {rf_scores.std():.4f}\")\n",
    "    \n",
    "    # 2. XGBoost (기본 설정)\n",
    "    print(\"  XGBoost 훈련 중...\")\n",
    "    # 클래스 가중치 계산\n",
    "    pos_count = (y_train == 1).sum()\n",
    "    neg_count = (y_train == 0).sum()\n",
    "    scale_pos_weight = neg_count / pos_count\n",
    "    \n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    xgb_scores = cross_val_score(xgb_model, X_train, y_train, cv=cv, scoring='f1')\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    models['xgb'] = xgb_model\n",
    "    cv_scores['xgb'] = xgb_scores.mean()\n",
    "    print(f\"    XGB CV F1: {xgb_scores.mean():.4f} ± {xgb_scores.std():.4f}\")\n",
    "    \n",
    "    # 3. LightGBM (기본 설정)\n",
    "    print(\"  LightGBM 훈련 중...\")\n",
    "    lgb_model = lgb.LGBMClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        class_weight='balanced',\n",
    "        verbose=-1\n",
    "    )\n",
    "    lgb_scores = cross_val_score(lgb_model, X_train, y_train, cv=cv, scoring='f1')\n",
    "    lgb_model.fit(X_train, y_train)\n",
    "    models['lgb'] = lgb_model\n",
    "    cv_scores['lgb'] = lgb_scores.mean()\n",
    "    print(f\"    LGB CV F1: {lgb_scores.mean():.4f} ± {lgb_scores.std():.4f}\")\n",
    "    \n",
    "    # 최고 모델 선택\n",
    "    best_model_name = max(cv_scores.items(), key=lambda x: x[1])[0]\n",
    "    print(f\"\\n✅ 최고 모델: {best_model_name} (CV F1: {cv_scores[best_model_name]:.4f})\")\n",
    "    \n",
    "    return models, cv_scores, best_model_name\n",
    "\n",
    "def make_predictions(models, best_model_name, X_test, test_df):\n",
    "    \"\"\"예측 및 제출 파일 생성\"\"\"\n",
    "    print(f\"\\n📤 {best_model_name} 모델로 예측 중...\")\n",
    "    \n",
    "    best_model = models[best_model_name]\n",
    "    predictions = best_model.predict(X_test)\n",
    "    \n",
    "    # 제출 파일 생성\n",
    "    submission = pd.DataFrame({\n",
    "        'ID': test_df['ID'],\n",
    "        'Cancer': predictions\n",
    "    })\n",
    "    \n",
    "    # 예측 분포 확인\n",
    "    pred_counts = pd.Series(predictions).value_counts()\n",
    "    print(f\"\\n예측 분포:\")\n",
    "    for cls in [0, 1]:\n",
    "        count = pred_counts.get(cls, 0)\n",
    "        pct = count / len(predictions) * 100\n",
    "        print(f\"  클래스 {cls}: {count:,}개 ({pct:.1f}%)\")\n",
    "    \n",
    "    return submission\n",
    "\n",
    "def main():\n",
    "    \"\"\"메인 실행\"\"\"\n",
    "    print(\"🚀 단순 베이스라인 시작!\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    try:\n",
    "        # 1. 데이터 로딩\n",
    "        print(\"📊 데이터 로딩...\")\n",
    "        train_df = pd.read_csv('train.csv')\n",
    "        test_df = pd.read_csv('test.csv')\n",
    "        \n",
    "        print(f\"  Train: {train_df.shape}\")\n",
    "        print(f\"  Test: {test_df.shape}\")\n",
    "        \n",
    "        # 클래스 분포 확인\n",
    "        class_dist = train_df['Cancer'].value_counts()\n",
    "        print(f\"  클래스 0: {class_dist[0]:,}개 ({class_dist[0]/len(train_df)*100:.1f}%)\")\n",
    "        print(f\"  클래스 1: {class_dist[1]:,}개 ({class_dist[1]/len(train_df)*100:.1f}%)\")\n",
    "        \n",
    "        # 2. 전처리\n",
    "        X_train, y_train, X_test = simple_preprocessing(train_df, test_df)\n",
    "        \n",
    "        # 3. 모델 훈련\n",
    "        models, cv_scores, best_model_name = train_simple_models(X_train, y_train)\n",
    "        \n",
    "        # 4. 예측\n",
    "        submission = make_predictions(models, best_model_name, X_test, test_df)\n",
    "        \n",
    "        # 5. 저장\n",
    "        submission.to_csv('simple_submission.csv', index=False)\n",
    "        print(f\"✅ 제출 파일 저장: simple_submission.csv\")\n",
    "        \n",
    "        print(\"\\n🎯 CV 점수 요약:\")\n",
    "        for model_name, score in sorted(cv_scores.items(), key=lambda x: x[1], reverse=True):\n",
    "            print(f\"  {model_name}: {score:.4f}\")\n",
    "        \n",
    "        print(\"\\n💡 만약 여전히 점수가 낮다면:\")\n",
    "        print(\"1. 데이터에 문제가 있을 수 있음\")\n",
    "        print(\"2. Public/Private 분할 문제\")\n",
    "        print(\"3. 평가 지표 차이\")\n",
    "        print(\"4. 다른 참가자들도 비슷한 점수일 가능성\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 오류: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# ============================================\n",
    "# 추가 디버깅용 함수들\n",
    "# ============================================\n",
    "\n",
    "def check_data_quality(train_df, test_df):\n",
    "    \"\"\"데이터 품질 체크\"\"\"\n",
    "    print(\"\\n🔍 데이터 품질 체크:\")\n",
    "    \n",
    "    feature_cols = [col for col in train_df.columns if col not in ['ID', 'Cancer']]\n",
    "    \n",
    "    for col in feature_cols:\n",
    "        train_unique = train_df[col].nunique()\n",
    "        test_unique = test_df[col].nunique()\n",
    "        \n",
    "        # 카테고리컬 변수에서 차이가 큰 경우\n",
    "        if train_df[col].dtype == 'object':\n",
    "            train_set = set(train_df[col].unique())\n",
    "            test_set = set(test_df[col].unique())\n",
    "            only_in_test = test_set - train_set\n",
    "            \n",
    "            if only_in_test:\n",
    "                print(f\"  ⚠️  {col}: 테스트에만 있는 값 {len(only_in_test)}개\")\n",
    "    \n",
    "    print(\"✅ 데이터 품질 체크 완료\")\n",
    "\n",
    "def quick_feature_importance(model, feature_names, top_k=10):\n",
    "    \"\"\"간단한 특성 중요도\"\"\"\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(f\"\\n📊 상위 {top_k}개 중요 특성:\")\n",
    "        for i, row in importance_df.head(top_k).iterrows():\n",
    "            print(f\"  {row['feature']}: {row['importance']:.4f}\")\n",
    "    \n",
    "# 사용 예시 (선택사항):\n",
    "# check_data_quality(train_df, test_df)\n",
    "# quick_feature_importance(models['xgb'], X_train.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
