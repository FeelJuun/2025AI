{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f467480-ba59-444c-ac30-de4eaf66c9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¥ ê°‘ìƒì„ ì•” ì§„ë‹¨ AI ëª¨ë¸ ê°œë°œ\n",
      "==================================================\n",
      "ğŸš€ ê°‘ìƒì„ ì•” ì˜ˆì¸¡ íŒŒì´í”„ë¼ì¸ ì‹œì‘!\n",
      "============================================================\n",
      "ğŸ“Š ë°ì´í„° ë¡œë”© ì¤‘...\n",
      "âœ… í›ˆë ¨ ë°ì´í„°: (87159, 16)\n",
      "âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°: (46204, 15)\n",
      "âœ… Feature ê°œìˆ˜: 14\n",
      "\n",
      "ğŸ“ˆ ê¸°ë³¸ EDA ìˆ˜í–‰ ì¤‘...\n",
      "\n",
      "í´ë˜ìŠ¤ ë¶„í¬:\n",
      "  í´ë˜ìŠ¤ 0: 76,700ê°œ (88.0%)\n",
      "  í´ë˜ìŠ¤ 1: 10,459ê°œ (12.0%)\n",
      "\n",
      "ê²°ì¸¡ê°’ í™•ì¸:\n",
      "\n",
      "ìˆ˜ì¹˜í˜• ë³€ìˆ˜: 5ê°œ\n",
      "ì¹´í…Œê³ ë¦¬ì»¬ ë³€ìˆ˜: 9ê°œ\n",
      "\n",
      "ğŸ”§ íŠ¹ì„± ì „ì²˜ë¦¬ ì¤‘...\n",
      "ğŸ“‹ ê²°ì¸¡ê°’ ì²˜ë¦¬ ì¤‘...\n",
      "ğŸ·ï¸  ì¹´í…Œê³ ë¦¬ì»¬ ë³€ìˆ˜ ì¸ì½”ë”© ì¤‘...\n",
      "  Gender: 2ê°œ í´ë˜ìŠ¤ ì¸ì½”ë”© ì™„ë£Œ\n",
      "  Country: 10ê°œ í´ë˜ìŠ¤ ì¸ì½”ë”© ì™„ë£Œ\n",
      "  Race: 5ê°œ í´ë˜ìŠ¤ ì¸ì½”ë”© ì™„ë£Œ\n",
      "  Family_Background: 2ê°œ í´ë˜ìŠ¤ ì¸ì½”ë”© ì™„ë£Œ\n",
      "  Radiation_History: 2ê°œ í´ë˜ìŠ¤ ì¸ì½”ë”© ì™„ë£Œ\n",
      "  Iodine_Deficiency: 2ê°œ í´ë˜ìŠ¤ ì¸ì½”ë”© ì™„ë£Œ\n",
      "  Smoke: 2ê°œ í´ë˜ìŠ¤ ì¸ì½”ë”© ì™„ë£Œ\n",
      "  Weight_Risk: 2ê°œ í´ë˜ìŠ¤ ì¸ì½”ë”© ì™„ë£Œ\n",
      "  Diabetes: 2ê°œ í´ë˜ìŠ¤ ì¸ì½”ë”© ì™„ë£Œ\n",
      "âš™ï¸  Feature Engineering ìˆ˜í–‰ ì¤‘...\n",
      "ğŸ“ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ìŠ¤ì¼€ì¼ë§ ì¤‘...\n",
      "âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: (87159, 20)\n",
      "âœ… ìµœì¢… feature ê°œìˆ˜: 20\n",
      "ğŸ”„ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...\n",
      "âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ: (46204, 20)\n",
      "\n",
      "ğŸ¯ ì „ì²´ ëª¨ë¸ í›ˆë ¨ ì‹œì‘...\n",
      "\n",
      "ğŸ¤– XGB ëª¨ë¸ í›ˆë ¨ ì¤‘...\n",
      "CV F1 Score: 0.00946 (+/- 0.00783)\n",
      "\n",
      "ğŸ¤– XGB ëª¨ë¸ í›ˆë ¨ ì¤‘...\n",
      "SMOTE ì ìš©: (87159, 20) â†’ (153400, 20)\n",
      "CV F1 Score: 0.92438 (+/- 0.00165)\n",
      "\n",
      "ğŸ¤– LGB ëª¨ë¸ í›ˆë ¨ ì¤‘...\n",
      "CV F1 Score: 0.41890 (+/- 0.01299)\n",
      "\n",
      "ğŸ¤– LGB ëª¨ë¸ í›ˆë ¨ ì¤‘...\n",
      "SMOTE ì ìš©: (87159, 20) â†’ (153400, 20)\n",
      "CV F1 Score: 0.92657 (+/- 0.00105)\n",
      "\n",
      "ğŸ¤– CATBOOST ëª¨ë¸ í›ˆë ¨ ì¤‘...\n",
      "CV F1 Score: 0.46614 (+/- 0.00410)\n",
      "\n",
      "ğŸ¤– RF ëª¨ë¸ í›ˆë ¨ ì¤‘...\n",
      "CV F1 Score: 0.46327 (+/- 0.01164)\n",
      "\n",
      "ğŸ“Š ëª¨ë¸ë³„ CV ì„±ëŠ¥:\n",
      "  lgb: 0.92657\n",
      "  lgb_smote: 0.92657\n",
      "  xgb: 0.92438\n",
      "  xgb_smote: 0.92438\n",
      "  catboost: 0.46614\n",
      "  rf: 0.46327\n",
      "\n",
      "ğŸ”„ VOTING ì•™ìƒë¸” ìƒì„± ì¤‘...\n",
      "ì•™ìƒë¸” CV F1 Score: 0.28125 (+/- 0.00961)\n",
      "\n",
      "ğŸ“¤ ì˜ˆì¸¡ ë° ì œì¶œ íŒŒì¼ ìƒì„± ì¤‘...\n",
      "âœ… ì„ íƒëœ ëª¨ë¸: lgb (CV F1: 0.92657)\n",
      "âœ… ì œì¶œ íŒŒì¼ ì €ì¥: submission.csv\n",
      "\n",
      "ì˜ˆì¸¡ ë¶„í¬:\n",
      "  í´ë˜ìŠ¤ 0: 42,604ê°œ (92.2%)\n",
      "  í´ë˜ìŠ¤ 1: 3,600ê°œ (7.8%)\n",
      "âœ… ëª¨ë¸ë“¤ì´ modelsì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ğŸ‰ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!\n",
      "==================================================\n",
      "\n",
      "ğŸ“Š ìµœì¢… ê²°ê³¼ ìš”ì•½:\n",
      "  lgb: CV F1 = 0.92657\n",
      "  lgb_smote: CV F1 = 0.92657\n",
      "  xgb: CV F1 = 0.92438\n",
      "  xgb_smote: CV F1 = 0.92438\n",
      "  catboost: CV F1 = 0.46614\n",
      "  rf: CV F1 = 0.46327\n",
      "  ensemble: CV F1 = 0.28125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n# íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„\\nif 'xgb' in predictor.trainer.models:\\n    analyze_feature_importance(\\n        predictor.trainer.models['xgb'], \\n        predictor.X_train.columns\\n    )\\n\\n# ì„ê³„ê°’ ìµœì í™” (ê²€ì¦ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°)\\n# best_threshold, best_f1 = cross_validate_threshold(\\n#     predictor.trainer.models['xgb'], \\n#     X_val, y_val\\n# )\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ================================================================================================\n",
    "# ê°‘ìƒì„ ì•” ì§„ë‹¨ ëŒ€íšŒ - ì™„ì „í•œ ì†”ë£¨ì…˜\n",
    "# ì „ëµ: ì•ˆì •ì ì´ê³  ì¬í˜„ ê°€ëŠ¥í•œ ì•™ìƒë¸” ê¸°ë°˜ ì ‘ê·¼ë²•\n",
    "# ================================================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ë¨¸ì‹ ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ…\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "\n",
    "# ë¶ˆê· í˜• ë°ì´í„° ì²˜ë¦¬\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ================================================================================================\n",
    "# 1. ì„¤ì • ë° ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜\n",
    "# ================================================================================================\n",
    "\n",
    "# ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ê³ ì •\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "def set_seeds(seed=42):\n",
    "    \"\"\"ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ì‹œë“œë¥¼ ê³ ì •í•˜ëŠ” í•¨ìˆ˜\"\"\"\n",
    "    import random\n",
    "    import os\n",
    "    \n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "class Config:\n",
    "    \"\"\"ì„¤ì • í´ë˜ìŠ¤\"\"\"\n",
    "    RANDOM_STATE = 42\n",
    "    N_SPLITS = 5\n",
    "    EARLY_STOPPING_ROUNDS = 100\n",
    "    VERBOSE = False\n",
    "    \n",
    "    # ëª¨ë¸ë³„ ê¸°ë³¸ íŒŒë¼ë¯¸í„°\n",
    "    XGB_PARAMS = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'logloss',\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'n_estimators': 1000,\n",
    "        'learning_rate': 0.05,\n",
    "        'max_depth': 6,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'reg_alpha': 0.1,\n",
    "        'reg_lambda': 0.1\n",
    "    }\n",
    "    \n",
    "    LGB_PARAMS = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'n_estimators': 1000,\n",
    "        'learning_rate': 0.05,\n",
    "        'max_depth': 6,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'reg_alpha': 0.1,\n",
    "        'reg_lambda': 0.1,\n",
    "        'verbose': -1\n",
    "    }\n",
    "    \n",
    "    CB_PARAMS = {\n",
    "        'objective': 'Logloss',\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'iterations': 1000,\n",
    "        'learning_rate': 0.05,\n",
    "        'depth': 6,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bylevel': 0.8,\n",
    "        'reg_lambda': 0.1,\n",
    "        'verbose': False\n",
    "    }\n",
    "\n",
    "set_seeds(Config.RANDOM_STATE)\n",
    "\n",
    "# ================================================================================================\n",
    "# 2. ë°ì´í„° ë¡œë”© ë° ê¸°ë³¸ ë¶„ì„\n",
    "# ================================================================================================\n",
    "\n",
    "class DataLoader:\n",
    "    \"\"\"ë°ì´í„° ë¡œë”© ë° ê¸°ë³¸ ì „ì²˜ë¦¬ í´ë˜ìŠ¤\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.train_data = None\n",
    "        self.test_data = None\n",
    "        self.feature_columns = None\n",
    "        self.target_column = 'Cancer'\n",
    "        \n",
    "    def load_data(self, train_path='train.csv', test_path='test.csv'):\n",
    "        \"\"\"ë°ì´í„° ë¡œë”©\"\"\"\n",
    "        print(\"ğŸ“Š ë°ì´í„° ë¡œë”© ì¤‘...\")\n",
    "        \n",
    "        self.train_data = pd.read_csv(train_path)\n",
    "        self.test_data = pd.read_csv(test_path)\n",
    "        \n",
    "        # Feature columns ì •ì˜ (IDì™€ target ì œì™¸)\n",
    "        self.feature_columns = [col for col in self.train_data.columns \n",
    "                               if col not in ['ID', self.target_column]]\n",
    "        \n",
    "        print(f\"âœ… í›ˆë ¨ ë°ì´í„°: {self.train_data.shape}\")\n",
    "        print(f\"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°: {self.test_data.shape}\")\n",
    "        print(f\"âœ… Feature ê°œìˆ˜: {len(self.feature_columns)}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def basic_eda(self):\n",
    "        \"\"\"ê¸°ë³¸ EDA ìˆ˜í–‰\"\"\"\n",
    "        print(\"\\nğŸ“ˆ ê¸°ë³¸ EDA ìˆ˜í–‰ ì¤‘...\")\n",
    "        \n",
    "        # í´ë˜ìŠ¤ ë¶„í¬\n",
    "        class_dist = self.train_data[self.target_column].value_counts()\n",
    "        print(f\"\\ní´ë˜ìŠ¤ ë¶„í¬:\")\n",
    "        for cls, count in class_dist.items():\n",
    "            pct = count / len(self.train_data) * 100\n",
    "            print(f\"  í´ë˜ìŠ¤ {cls}: {count:,}ê°œ ({pct:.1f}%)\")\n",
    "        \n",
    "        # ê²°ì¸¡ê°’ í™•ì¸\n",
    "        print(f\"\\nê²°ì¸¡ê°’ í™•ì¸:\")\n",
    "        missing_train = self.train_data.isnull().sum()\n",
    "        missing_test = self.test_data.isnull().sum()\n",
    "        \n",
    "        for col in self.feature_columns:\n",
    "            train_missing = missing_train[col] if col in missing_train.index else 0\n",
    "            test_missing = missing_test[col] if col in missing_test.index else 0\n",
    "            if train_missing > 0 or test_missing > 0:\n",
    "                print(f\"  {col}: í›ˆë ¨({train_missing}) í…ŒìŠ¤íŠ¸({test_missing})\")\n",
    "        \n",
    "        # ìˆ˜ì¹˜í˜•/ì¹´í…Œê³ ë¦¬ì»¬ ë³€ìˆ˜ ë¶„ë¦¬\n",
    "        numeric_cols = self.train_data[self.feature_columns].select_dtypes(\n",
    "            include=[np.number]).columns.tolist()\n",
    "        categorical_cols = [col for col in self.feature_columns if col not in numeric_cols]\n",
    "        \n",
    "        print(f\"\\nìˆ˜ì¹˜í˜• ë³€ìˆ˜: {len(numeric_cols)}ê°œ\")\n",
    "        print(f\"ì¹´í…Œê³ ë¦¬ì»¬ ë³€ìˆ˜: {len(categorical_cols)}ê°œ\")\n",
    "        \n",
    "        return numeric_cols, categorical_cols\n",
    "\n",
    "# ================================================================================================\n",
    "# 3. ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸\n",
    "# ================================================================================================\n",
    "\n",
    "class FeatureProcessor:\n",
    "    \"\"\"íŠ¹ì„± ì „ì²˜ë¦¬ ë° ì—”ì§€ë‹ˆì–´ë§ í´ë˜ìŠ¤\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.label_encoders = {}\n",
    "        self.scaler = StandardScaler()\n",
    "        self.numeric_cols = []\n",
    "        self.categorical_cols = []\n",
    "        \n",
    "    def fit_transform(self, train_data, feature_columns, target_column='Cancer'):\n",
    "        \"\"\"í›ˆë ¨ ë°ì´í„°ì— fití•˜ê³  transform\"\"\"\n",
    "        print(\"\\nğŸ”§ íŠ¹ì„± ì „ì²˜ë¦¬ ì¤‘...\")\n",
    "        \n",
    "        X_train = train_data[feature_columns].copy()\n",
    "        y_train = train_data[target_column].copy()\n",
    "        \n",
    "        # ê²°ì¸¡ê°’ ì‚¬ì „ ì²˜ë¦¬\n",
    "        print(\"ğŸ“‹ ê²°ì¸¡ê°’ ì²˜ë¦¬ ì¤‘...\")\n",
    "        \n",
    "        # ìˆ˜ì¹˜í˜•/ì¹´í…Œê³ ë¦¬ì»¬ ë³€ìˆ˜ ë¶„ë¦¬\n",
    "        self.numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        self.categorical_cols = [col for col in feature_columns if col not in self.numeric_cols]\n",
    "        \n",
    "        # ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ê²°ì¸¡ê°’ ì²˜ë¦¬ (ì¤‘ì•™ê°’ìœ¼ë¡œ ëŒ€ì²´)\n",
    "        for col in self.numeric_cols:\n",
    "            if X_train[col].isnull().any():\n",
    "                median_val = X_train[col].median()\n",
    "                X_train[col].fillna(median_val, inplace=True)\n",
    "                print(f\"  {col}: ê²°ì¸¡ê°’ì„ {median_val:.3f}ë¡œ ëŒ€ì²´\")\n",
    "        \n",
    "        # ì¹´í…Œê³ ë¦¬ì»¬ ë³€ìˆ˜ ê²°ì¸¡ê°’ ì²˜ë¦¬ (ìµœë¹ˆê°’ìœ¼ë¡œ ëŒ€ì²´)\n",
    "        for col in self.categorical_cols:\n",
    "            if X_train[col].isnull().any() or X_train[col].isna().any():\n",
    "                mode_val = X_train[col].mode().iloc[0] if len(X_train[col].mode()) > 0 else 'Unknown'\n",
    "                X_train[col].fillna(mode_val, inplace=True)\n",
    "                print(f\"  {col}: ê²°ì¸¡ê°’ì„ '{mode_val}'ë¡œ ëŒ€ì²´\")\n",
    "        \n",
    "        # 1. ì¹´í…Œê³ ë¦¬ì»¬ ë³€ìˆ˜ ì¸ì½”ë”©\n",
    "        print(\"ğŸ·ï¸  ì¹´í…Œê³ ë¦¬ì»¬ ë³€ìˆ˜ ì¸ì½”ë”© ì¤‘...\")\n",
    "        for col in self.categorical_cols:\n",
    "            le = LabelEncoder()\n",
    "            # ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬\n",
    "            X_train[col] = X_train[col].astype(str)\n",
    "            X_train[col] = le.fit_transform(X_train[col])\n",
    "            self.label_encoders[col] = le\n",
    "            print(f\"  {col}: {len(le.classes_)}ê°œ í´ë˜ìŠ¤ ì¸ì½”ë”© ì™„ë£Œ\")\n",
    "        \n",
    "        # 2. Feature Engineering\n",
    "        print(\"âš™ï¸  Feature Engineering ìˆ˜í–‰ ì¤‘...\")\n",
    "        X_train = self._feature_engineering(X_train)\n",
    "        \n",
    "        # 3. ìµœì¢… ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë“¤ í™•ì¸ ë° ìŠ¤ì¼€ì¼ë§\n",
    "        numeric_cols_final = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        \n",
    "        # ë¬´í•œëŒ€ë‚˜ NaN ê°’ ìµœì¢… ì²´í¬\n",
    "        for col in numeric_cols_final:\n",
    "            # ë¬´í•œëŒ€ ê°’ ì²˜ë¦¬\n",
    "            inf_mask = np.isinf(X_train[col])\n",
    "            if inf_mask.any():\n",
    "                print(f\"  {col}: {inf_mask.sum()}ê°œ ë¬´í•œëŒ€ ê°’ ë°œê²¬, ì¤‘ì•™ê°’ìœ¼ë¡œ ëŒ€ì²´\")\n",
    "                X_train.loc[inf_mask, col] = X_train[col][~inf_mask].median()\n",
    "            \n",
    "            # NaN ê°’ ìµœì¢… ì²˜ë¦¬\n",
    "            nan_mask = X_train[col].isnull()\n",
    "            if nan_mask.any():\n",
    "                print(f\"  {col}: {nan_mask.sum()}ê°œ NaN ê°’ ë°œê²¬, ì¤‘ì•™ê°’ìœ¼ë¡œ ëŒ€ì²´\")\n",
    "                X_train.loc[nan_mask, col] = X_train[col][~nan_mask].median()\n",
    "        \n",
    "        # ìŠ¤ì¼€ì¼ë§\n",
    "        print(\"ğŸ“ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ìŠ¤ì¼€ì¼ë§ ì¤‘...\")\n",
    "        X_train[numeric_cols_final] = self.scaler.fit_transform(X_train[numeric_cols_final])\n",
    "        \n",
    "        print(f\"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: {X_train.shape}\")\n",
    "        print(f\"âœ… ìµœì¢… feature ê°œìˆ˜: {len(X_train.columns)}\")\n",
    "        \n",
    "        return X_train, y_train\n",
    "    \n",
    "    def transform(self, data, feature_columns):\n",
    "        \"\"\"í…ŒìŠ¤íŠ¸ ë°ì´í„° transform\"\"\"\n",
    "        print(\"ğŸ”„ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...\")\n",
    "        X_test = data[feature_columns].copy()\n",
    "        \n",
    "        # ê²°ì¸¡ê°’ ì‚¬ì „ ì²˜ë¦¬ (í›ˆë ¨ ë°ì´í„°ì™€ ë™ì¼í•œ ë°©ì‹)\n",
    "        # ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ê²°ì¸¡ê°’ ì²˜ë¦¬\n",
    "        for col in self.numeric_cols:\n",
    "            if X_test[col].isnull().any():\n",
    "                # í›ˆë ¨ ë°ì´í„°ì˜ ì¤‘ì•™ê°’ ì‚¬ìš© (scalerì—ì„œ ì¶”ì¶œ)\n",
    "                if hasattr(self.scaler, 'scale_') and col in X_test.columns:\n",
    "                    # ìŠ¤ì¼€ì¼ëŸ¬ê°€ fitë˜ì–´ ìˆë‹¤ë©´ mean ê°’ ì‚¬ìš©\n",
    "                    fill_value = 0  # ìŠ¤ì¼€ì¼ë§ í›„ í‰ê· ê°’\n",
    "                else:\n",
    "                    fill_value = X_test[col].median()\n",
    "                X_test[col].fillna(fill_value, inplace=True)\n",
    "        \n",
    "        # ì¹´í…Œê³ ë¦¬ì»¬ ë³€ìˆ˜ ê²°ì¸¡ê°’ ì²˜ë¦¬\n",
    "        for col in self.categorical_cols:\n",
    "            if X_test[col].isnull().any() or X_test[col].isna().any():\n",
    "                # í›ˆë ¨ ë°ì´í„°ì—ì„œ ê°€ì¥ ë§ì•˜ë˜ ê°’ìœ¼ë¡œ ëŒ€ì²´ (ì²« ë²ˆì§¸ í´ë˜ìŠ¤)\n",
    "                if col in self.label_encoders:\n",
    "                    most_common = self.label_encoders[col].classes_[0]\n",
    "                    X_test[col].fillna(most_common, inplace=True)\n",
    "                else:\n",
    "                    X_test[col].fillna('Unknown', inplace=True)\n",
    "        \n",
    "        # 1. ì¹´í…Œê³ ë¦¬ì»¬ ë³€ìˆ˜ ì¸ì½”ë”©\n",
    "        for col in self.categorical_cols:\n",
    "            if col in self.label_encoders:\n",
    "                # ë¬¸ìì—´ë¡œ ë³€í™˜\n",
    "                X_test[col] = X_test[col].astype(str)\n",
    "                \n",
    "                # í›ˆë ¨ ë°ì´í„°ì— ì—†ë˜ ê°’ì€ ê°€ì¥ ë¹ˆë„ê°€ ë†’ì€ ê°’ìœ¼ë¡œ ì²˜ë¦¬\n",
    "                unknown_mask = ~X_test[col].isin(self.label_encoders[col].classes_)\n",
    "                if unknown_mask.any():\n",
    "                    most_common = self.label_encoders[col].classes_[0]\n",
    "                    X_test.loc[unknown_mask, col] = most_common\n",
    "                    print(f\"  {col}: {unknown_mask.sum()}ê°œ ë¯¸ì§€ì˜ ê°’ì„ '{most_common}'ë¡œ ëŒ€ì²´\")\n",
    "                \n",
    "                X_test[col] = self.label_encoders[col].transform(X_test[col])\n",
    "        \n",
    "        # 2. Feature Engineering\n",
    "        X_test = self._feature_engineering(X_test)\n",
    "        \n",
    "        # 3. ìµœì¢… ì•ˆì „ì„± ì²´í¬\n",
    "        numeric_cols_final = X_test.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        \n",
    "        # ë¬´í•œëŒ€ë‚˜ NaN ê°’ ìµœì¢… ì²´í¬\n",
    "        for col in numeric_cols_final:\n",
    "            # ë¬´í•œëŒ€ ê°’ ì²˜ë¦¬\n",
    "            inf_mask = np.isinf(X_test[col])\n",
    "            if inf_mask.any():\n",
    "                X_test.loc[inf_mask, col] = 0  # ìŠ¤ì¼€ì¼ë§ëœ í‰ê· ê°’\n",
    "            \n",
    "            # NaN ê°’ ìµœì¢… ì²˜ë¦¬\n",
    "            nan_mask = X_test[col].isnull()\n",
    "            if nan_mask.any():\n",
    "                X_test.loc[nan_mask, col] = 0  # ìŠ¤ì¼€ì¼ë§ëœ í‰ê· ê°’\n",
    "        \n",
    "        # 4. ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ìŠ¤ì¼€ì¼ë§\n",
    "        X_test[numeric_cols_final] = self.scaler.transform(X_test[numeric_cols_final])\n",
    "        \n",
    "        print(f\"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ: {X_test.shape}\")\n",
    "        return X_test\n",
    "    \n",
    "    def _feature_engineering(self, data):\n",
    "        \"\"\"í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§\"\"\"\n",
    "        # ê¸°ë³¸ì ì¸ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§\n",
    "        data = data.copy()\n",
    "        \n",
    "        # Age ê·¸ë£¹í™” (NaN ì•ˆì „ ì²˜ë¦¬)\n",
    "        if 'Age' in data.columns:\n",
    "            # NaNì´ ì•„ë‹Œ ê°’ë“¤ë§Œ ì²˜ë¦¬\n",
    "            age_mask = data['Age'].notna()\n",
    "            data['Age_Group'] = 0  # ê¸°ë³¸ê°’\n",
    "            \n",
    "            if age_mask.any():\n",
    "                age_groups = pd.cut(data.loc[age_mask, 'Age'], \n",
    "                                  bins=[0, 30, 50, 70, 100], \n",
    "                                  labels=[0, 1, 2, 3],\n",
    "                                  include_lowest=True)\n",
    "                data.loc[age_mask, 'Age_Group'] = age_groups.astype(int)\n",
    "        \n",
    "        # í˜¸ë¥´ëª¬ ìˆ˜ì¹˜ ë¹„ìœ¨ (ì•ˆì „í•œ ë‚˜ëˆ—ì…ˆ)\n",
    "        if all(col in data.columns for col in ['TSH_Result', 'T4_Result', 'T3_Result']):\n",
    "            # NaN ê°’ ì²˜ë¦¬\n",
    "            tsh_safe = data['TSH_Result'].fillna(data['TSH_Result'].median())\n",
    "            t4_safe = data['T4_Result'].fillna(data['T4_Result'].median())\n",
    "            t3_safe = data['T3_Result'].fillna(data['T3_Result'].median())\n",
    "            \n",
    "            data['TSH_T4_ratio'] = tsh_safe / (t4_safe + 1e-8)\n",
    "            data['T3_T4_ratio'] = t3_safe / (t4_safe + 1e-8)\n",
    "            \n",
    "            # ê·¹ê°’ ì²˜ë¦¬ (ì´ìƒì¹˜ ì œê±°)\n",
    "            data['TSH_T4_ratio'] = np.clip(data['TSH_T4_ratio'], 0, 10)\n",
    "            data['T3_T4_ratio'] = np.clip(data['T3_T4_ratio'], 0, 5)\n",
    "        \n",
    "        # ê²°ì ˆ í¬ê¸° ê·¸ë£¹í™” (NaN ì•ˆì „ ì²˜ë¦¬)\n",
    "        if 'Nodule_Size' in data.columns:\n",
    "            # NaNì´ ì•„ë‹Œ ê°’ë“¤ë§Œ ì²˜ë¦¬\n",
    "            nodule_mask = data['Nodule_Size'].notna()\n",
    "            data['Nodule_Size_Group'] = 0  # ê¸°ë³¸ê°’ (ê°€ì¥ ì‘ì€ ê·¸ë£¹)\n",
    "            \n",
    "            if nodule_mask.any():\n",
    "                # ì‹¤ì œ ë°ì´í„° ë²”ìœ„ì— ë§ê²Œ bins ì¡°ì •\n",
    "                min_size = data['Nodule_Size'].min()\n",
    "                max_size = data['Nodule_Size'].max()\n",
    "                \n",
    "                # ë” ì•ˆì „í•œ bins ì„¤ì •\n",
    "                bins = [min_size - 0.1, 1, 2, 3, max_size + 0.1]\n",
    "                nodule_groups = pd.cut(data.loc[nodule_mask, 'Nodule_Size'], \n",
    "                                     bins=bins, \n",
    "                                     labels=[0, 1, 2, 3],\n",
    "                                     include_lowest=True)\n",
    "                data.loc[nodule_mask, 'Nodule_Size_Group'] = nodule_groups.astype(int)\n",
    "        \n",
    "        # ì¶”ê°€ ì•ˆì „ í”¼ì²˜ë“¤\n",
    "        if 'Age' in data.columns:\n",
    "            data['Age_squared'] = data['Age'] ** 2\n",
    "            data['Age_log'] = np.log1p(data['Age'])  # log(1+x) - 0 ë°©ì§€\n",
    "        \n",
    "        return data\n",
    "\n",
    "# ================================================================================================\n",
    "# 4. ëª¨ë¸ í´ë˜ìŠ¤ë“¤\n",
    "# ================================================================================================\n",
    "\n",
    "class ModelTrainer:\n",
    "    \"\"\"ëª¨ë¸ í›ˆë ¨ ë° ì˜ˆì¸¡ í´ë˜ìŠ¤\"\"\"\n",
    "    \n",
    "    def __init__(self, config=Config()):\n",
    "        self.config = config\n",
    "        self.models = {}\n",
    "        self.cv_scores = {}\n",
    "        \n",
    "    def train_single_model(self, X_train, y_train, model_type='xgb', \n",
    "                          class_weight=None, use_smote=False):\n",
    "        \"\"\"ë‹¨ì¼ ëª¨ë¸ í›ˆë ¨\"\"\"\n",
    "        print(f\"\\nğŸ¤– {model_type.upper()} ëª¨ë¸ í›ˆë ¨ ì¤‘...\")\n",
    "        \n",
    "        # í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬\n",
    "        if use_smote:\n",
    "            smote = SMOTE(random_state=self.config.RANDOM_STATE)\n",
    "            X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "            print(f\"SMOTE ì ìš©: {X_train.shape} â†’ {X_train_balanced.shape}\")\n",
    "        else:\n",
    "            X_train_balanced, y_train_balanced = X_train, y_train\n",
    "        \n",
    "        # ëª¨ë¸ ìƒì„±\n",
    "        if model_type == 'xgb':\n",
    "            params = self.config.XGB_PARAMS.copy()\n",
    "            if class_weight:\n",
    "                scale_pos_weight = class_weight[0] / class_weight[1]\n",
    "                params['scale_pos_weight'] = scale_pos_weight\n",
    "            model = xgb.XGBClassifier(**params)\n",
    "            \n",
    "        elif model_type == 'lgb':\n",
    "            params = self.config.LGB_PARAMS.copy()\n",
    "            if class_weight:\n",
    "                params['class_weight'] = 'balanced'\n",
    "            model = lgb.LGBMClassifier(**params)\n",
    "            \n",
    "        elif model_type == 'catboost':\n",
    "            params = self.config.CB_PARAMS.copy()\n",
    "            if class_weight:\n",
    "                params['class_weights'] = class_weight\n",
    "            model = cb.CatBoostClassifier(**params)\n",
    "            \n",
    "        elif model_type == 'rf':\n",
    "            model = RandomForestClassifier(\n",
    "                n_estimators=500,\n",
    "                max_depth=10,\n",
    "                random_state=self.config.RANDOM_STATE,\n",
    "                class_weight='balanced' if class_weight else None,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "        \n",
    "        # Cross Validation\n",
    "        cv = StratifiedKFold(n_splits=self.config.N_SPLITS, \n",
    "                           shuffle=True, \n",
    "                           random_state=self.config.RANDOM_STATE)\n",
    "        \n",
    "        cv_scores = cross_val_score(model, X_train_balanced, y_train_balanced, \n",
    "                                   cv=cv, scoring='f1', n_jobs=-1)\n",
    "        \n",
    "        print(f\"CV F1 Score: {cv_scores.mean():.5f} (+/- {cv_scores.std() * 2:.5f})\")\n",
    "        \n",
    "        # ì „ì²´ ë°ì´í„°ë¡œ í›ˆë ¨\n",
    "        model.fit(X_train_balanced, y_train_balanced)\n",
    "        \n",
    "        # ì €ì¥\n",
    "        self.models[model_type] = model\n",
    "        self.cv_scores[model_type] = cv_scores.mean()\n",
    "        \n",
    "        return model, cv_scores.mean()\n",
    "    \n",
    "    def train_all_models(self, X_train, y_train):\n",
    "        \"\"\"ëª¨ë“  ëª¨ë¸ í›ˆë ¨\"\"\"\n",
    "        print(\"\\nğŸ¯ ì „ì²´ ëª¨ë¸ í›ˆë ¨ ì‹œì‘...\")\n",
    "        \n",
    "        # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚°\n",
    "        class_weights = compute_class_weight('balanced', \n",
    "                                           classes=np.unique(y_train), \n",
    "                                           y=y_train)\n",
    "        class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
    "        \n",
    "        # ê° ëª¨ë¸ í›ˆë ¨\n",
    "        model_configs = [\n",
    "            ('xgb', False, class_weight_dict),\n",
    "            ('xgb_smote', True, None),\n",
    "            ('lgb', False, class_weight_dict),\n",
    "            ('lgb_smote', True, None),\n",
    "            ('catboost', False, class_weight_dict),\n",
    "            ('rf', False, class_weight_dict)\n",
    "        ]\n",
    "        \n",
    "        for model_name, use_smote, class_weight in model_configs:\n",
    "            try:\n",
    "                base_name = model_name.replace('_smote', '')\n",
    "                self.train_single_model(X_train, y_train, \n",
    "                                      model_type=base_name,\n",
    "                                      class_weight=class_weight,\n",
    "                                      use_smote=use_smote)\n",
    "                if use_smote:\n",
    "                    # SMOTE ë²„ì „ì„ ë³„ë„ë¡œ ì €ì¥\n",
    "                    self.models[model_name] = self.models[base_name]\n",
    "                    self.cv_scores[model_name] = self.cv_scores[base_name]\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ {model_name} í›ˆë ¨ ì‹¤íŒ¨: {e}\")\n",
    "        \n",
    "        # ê²°ê³¼ ìš”ì•½\n",
    "        print(f\"\\nğŸ“Š ëª¨ë¸ë³„ CV ì„±ëŠ¥:\")\n",
    "        for model_name, score in sorted(self.cv_scores.items(), \n",
    "                                       key=lambda x: x[1], reverse=True):\n",
    "            print(f\"  {model_name}: {score:.5f}\")\n",
    "    \n",
    "    def create_ensemble(self, X_train, y_train, ensemble_type='voting'):\n",
    "        \"\"\"ì•™ìƒë¸” ëª¨ë¸ ìƒì„±\"\"\"\n",
    "        print(f\"\\nğŸ”„ {ensemble_type.upper()} ì•™ìƒë¸” ìƒì„± ì¤‘...\")\n",
    "        \n",
    "        if len(self.models) < 2:\n",
    "            print(\"âŒ ì•™ìƒë¸”ì„ ìœ„í•œ ì¶©ë¶„í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            return None\n",
    "        \n",
    "        # ìƒìœ„ ì„±ëŠ¥ ëª¨ë¸ë“¤ ì„ íƒ\n",
    "        top_models = sorted(self.cv_scores.items(), \n",
    "                           key=lambda x: x[1], reverse=True)[:4]\n",
    "        \n",
    "        estimators = [(name, self.models[name]) for name, _ in top_models]\n",
    "        \n",
    "        if ensemble_type == 'voting':\n",
    "            # ê°€ì¤‘ íˆ¬í‘œ (CV ì„±ëŠ¥ ê¸°ë°˜)\n",
    "            weights = [score for _, score in top_models]\n",
    "            ensemble = VotingClassifier(estimators=estimators, \n",
    "                                      voting='soft', \n",
    "                                      weights=weights)\n",
    "        \n",
    "        # ì•™ìƒë¸” ëª¨ë¸ í›ˆë ¨\n",
    "        ensemble.fit(X_train, y_train)\n",
    "        \n",
    "        # CV í‰ê°€\n",
    "        cv = StratifiedKFold(n_splits=self.config.N_SPLITS, \n",
    "                           shuffle=True, \n",
    "                           random_state=self.config.RANDOM_STATE)\n",
    "        ensemble_scores = cross_val_score(ensemble, X_train, y_train, \n",
    "                                        cv=cv, scoring='f1', n_jobs=-1)\n",
    "        \n",
    "        print(f\"ì•™ìƒë¸” CV F1 Score: {ensemble_scores.mean():.5f} (+/- {ensemble_scores.std() * 2:.5f})\")\n",
    "        \n",
    "        self.models['ensemble'] = ensemble\n",
    "        self.cv_scores['ensemble'] = ensemble_scores.mean()\n",
    "        \n",
    "        return ensemble\n",
    "\n",
    "# ================================================================================================\n",
    "# 5. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\n",
    "# ================================================================================================\n",
    "\n",
    "class HyperparameterTuner:\n",
    "    \"\"\"í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ í´ë˜ìŠ¤\"\"\"\n",
    "    \n",
    "    def __init__(self, random_state=42):\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def tune_xgboost(self, X_train, y_train, n_iter=50):\n",
    "        \"\"\"XGBoost í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\"\"\"\n",
    "        print(\"\\nğŸ¯ XGBoost í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì¤‘...\")\n",
    "        \n",
    "        param_dist = {\n",
    "            'n_estimators': [500, 1000, 1500],\n",
    "            'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "            'max_depth': [3, 4, 5, 6, 7],\n",
    "            'subsample': [0.7, 0.8, 0.9],\n",
    "            'colsample_bytree': [0.7, 0.8, 0.9],\n",
    "            'reg_alpha': [0, 0.1, 0.5],\n",
    "            'reg_lambda': [0.1, 0.5, 1.0]\n",
    "        }\n",
    "        \n",
    "        xgb_model = xgb.XGBClassifier(\n",
    "            objective='binary:logistic',\n",
    "            eval_metric='logloss',\n",
    "            random_state=self.random_state,\n",
    "            scale_pos_weight=7.3  # í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¹„ìœ¨\n",
    "        )\n",
    "        \n",
    "        cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=self.random_state)\n",
    "        \n",
    "        random_search = RandomizedSearchCV(\n",
    "            xgb_model, param_dist,\n",
    "            n_iter=n_iter,\n",
    "            scoring='f1',\n",
    "            cv=cv,\n",
    "            random_state=self.random_state,\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        random_search.fit(X_train, y_train)\n",
    "        \n",
    "        print(f\"âœ… ìµœì  XGB íŒŒë¼ë¯¸í„°: {random_search.best_params_}\")\n",
    "        print(f\"âœ… ìµœì  CV F1 Score: {random_search.best_score_:.5f}\")\n",
    "        \n",
    "        return random_search.best_estimator_, random_search.best_params_\n",
    "\n",
    "# ================================================================================================\n",
    "# 6. ë©”ì¸ ì‹¤í–‰ í´ë˜ìŠ¤\n",
    "# ================================================================================================\n",
    "\n",
    "class ThyroidCancerPredictor:\n",
    "    \"\"\"ê°‘ìƒì„ ì•” ì˜ˆì¸¡ ë©”ì¸ í´ë˜ìŠ¤\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.data_loader = DataLoader()\n",
    "        self.processor = FeatureProcessor()\n",
    "        self.trainer = ModelTrainer()\n",
    "        self.tuner = HyperparameterTuner()\n",
    "        \n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        self.X_test = None\n",
    "        \n",
    "    def run_pipeline(self, train_path='train.csv', test_path='test.csv', \n",
    "                    tune_hyperparams=False):\n",
    "        \"\"\"ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰\"\"\"\n",
    "        print(\"ğŸš€ ê°‘ìƒì„ ì•” ì˜ˆì¸¡ íŒŒì´í”„ë¼ì¸ ì‹œì‘!\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # 1. ë°ì´í„° ë¡œë”©\n",
    "        self.data_loader.load_data(train_path, test_path)\n",
    "        numeric_cols, categorical_cols = self.data_loader.basic_eda()\n",
    "        \n",
    "        # 2. ì „ì²˜ë¦¬\n",
    "        self.X_train, self.y_train = self.processor.fit_transform(\n",
    "            self.data_loader.train_data, \n",
    "            self.data_loader.feature_columns\n",
    "        )\n",
    "        \n",
    "        self.X_test = self.processor.transform(\n",
    "            self.data_loader.test_data,\n",
    "            self.data_loader.feature_columns\n",
    "        )\n",
    "        \n",
    "        # 3. ëª¨ë¸ í›ˆë ¨\n",
    "        self.trainer.train_all_models(self.X_train, self.y_train)\n",
    "        \n",
    "        # 4. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (ì„ íƒì‚¬í•­)\n",
    "        if tune_hyperparams:\n",
    "            best_xgb, best_params = self.tuner.tune_xgboost(self.X_train, self.y_train)\n",
    "            self.trainer.models['xgb_tuned'] = best_xgb\n",
    "            \n",
    "            # íŠœë‹ëœ ëª¨ë¸ CV í‰ê°€\n",
    "            cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "            tuned_scores = cross_val_score(best_xgb, self.X_train, self.y_train,\n",
    "                                         cv=cv, scoring='f1', n_jobs=-1)\n",
    "            self.trainer.cv_scores['xgb_tuned'] = tuned_scores.mean()\n",
    "            print(f\"íŠœë‹ëœ XGB CV F1: {tuned_scores.mean():.5f}\")\n",
    "        \n",
    "        # 5. ì•™ìƒë¸” ìƒì„±\n",
    "        ensemble_model = self.trainer.create_ensemble(self.X_train, self.y_train)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict_and_submit(self, submission_path='submission.csv'):\n",
    "        \"\"\"ì˜ˆì¸¡ ë° ì œì¶œ íŒŒì¼ ìƒì„±\"\"\"\n",
    "        print(\"\\nğŸ“¤ ì˜ˆì¸¡ ë° ì œì¶œ íŒŒì¼ ìƒì„± ì¤‘...\")\n",
    "        \n",
    "        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì„ íƒ\n",
    "        best_model_name = max(self.trainer.cv_scores.items(), \n",
    "                             key=lambda x: x[1])[0]\n",
    "        best_model = self.trainer.models[best_model_name]\n",
    "        \n",
    "        print(f\"âœ… ì„ íƒëœ ëª¨ë¸: {best_model_name} (CV F1: {self.trainer.cv_scores[best_model_name]:.5f})\")\n",
    "        \n",
    "        # ì˜ˆì¸¡\n",
    "        predictions = best_model.predict(self.X_test)\n",
    "        \n",
    "        # ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "        submission = pd.DataFrame({\n",
    "            'ID': self.data_loader.test_data['ID'],\n",
    "            'Cancer': predictions\n",
    "        })\n",
    "        \n",
    "        submission.to_csv(submission_path, index=False)\n",
    "        print(f\"âœ… ì œì¶œ íŒŒì¼ ì €ì¥: {submission_path}\")\n",
    "        \n",
    "        # ì˜ˆì¸¡ ë¶„í¬ í™•ì¸\n",
    "        pred_dist = pd.Series(predictions).value_counts()\n",
    "        print(f\"\\nì˜ˆì¸¡ ë¶„í¬:\")\n",
    "        for cls, count in pred_dist.items():\n",
    "            pct = count / len(predictions) * 100\n",
    "            print(f\"  í´ë˜ìŠ¤ {cls}: {count:,}ê°œ ({pct:.1f}%)\")\n",
    "        \n",
    "        return submission\n",
    "    \n",
    "    def save_models(self, save_dir='models'):\n",
    "        \"\"\"ëª¨ë¸ ì €ì¥\"\"\"\n",
    "        save_path = Path(save_dir)\n",
    "        save_path.mkdir(exist_ok=True)\n",
    "        \n",
    "        for model_name, model in self.trainer.models.items():\n",
    "            model_path = save_path / f\"{model_name}_model.pkl\"\n",
    "            with open(model_path, 'wb') as f:\n",
    "                pickle.dump(model, f)\n",
    "        \n",
    "        # ì „ì²˜ë¦¬ê¸°ë„ ì €ì¥\n",
    "        processor_path = save_path / \"processor.pkl\"\n",
    "        with open(processor_path, 'wb') as f:\n",
    "            pickle.dump(self.processor, f)\n",
    "        \n",
    "        print(f\"âœ… ëª¨ë¸ë“¤ì´ {save_dir}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# ================================================================================================\n",
    "# 7. ì‹¤í–‰ ì½”ë“œ\n",
    "# ================================================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜\"\"\"\n",
    "    print(\"ğŸ¥ ê°‘ìƒì„ ì•” ì§„ë‹¨ AI ëª¨ë¸ ê°œë°œ\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # ì˜ˆì¸¡ê¸° ìƒì„± ë° ì‹¤í–‰\n",
    "    predictor = ThyroidCancerPredictor()\n",
    "    \n",
    "    try:\n",
    "        # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰\n",
    "        predictor.run_pipeline(\n",
    "            train_path='train.csv',\n",
    "            test_path='test.csv',\n",
    "            tune_hyperparams=False  # ì‹œê°„ì´ ë§ìœ¼ë©´ Trueë¡œ ì„¤ì •\n",
    "        )\n",
    "        \n",
    "        # ì˜ˆì¸¡ ë° ì œì¶œ\n",
    "        submission = predictor.predict_and_submit('submission.csv')\n",
    "        \n",
    "        # ëª¨ë¸ ì €ì¥\n",
    "        predictor.save_models('models')\n",
    "        \n",
    "        print(\"\\nğŸ‰ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # ìµœì¢… ê²°ê³¼ ìš”ì•½\n",
    "        print(\"\\nğŸ“Š ìµœì¢… ê²°ê³¼ ìš”ì•½:\")\n",
    "        for model_name, score in sorted(predictor.trainer.cv_scores.items(), \n",
    "                                       key=lambda x: x[1], reverse=True):\n",
    "            print(f\"  {model_name}: CV F1 = {score:.5f}\")\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}\")\n",
    "        print(\"ğŸ’¡ train.csvì™€ test.csv íŒŒì¼ì´ í˜„ì¬ ë””ë ‰í„°ë¦¬ì— ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "        \n",
    "    except ValueError as e:\n",
    "        print(f\"âŒ ë°ì´í„° ì²˜ë¦¬ ì˜¤ë¥˜: {e}\")\n",
    "        print(\"ğŸ’¡ ë°ì´í„° í˜•ì‹ì´ë‚˜ ê²°ì¸¡ê°’ì„ í™•ì¸í•´ë³´ì„¸ìš”.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        print(\"\\nğŸ” ìƒì„¸ ì˜¤ë¥˜ ì •ë³´:\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        # ë””ë²„ê¹… ì •ë³´ ì œê³µ\n",
    "        print(\"\\nğŸ› ï¸  ë””ë²„ê¹… ë„ì›€ë§:\")\n",
    "        print(\"1. ë°ì´í„° íŒŒì¼ì´ ì˜¬ë°”ë¥¸ í˜•ì‹ì¸ì§€ í™•ì¸\")\n",
    "        print(\"2. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ëª¨ë‘ ì„¤ì¹˜ë˜ì—ˆëŠ”ì§€ í™•ì¸\")\n",
    "        print(\"3. ë©”ëª¨ë¦¬ê°€ ì¶©ë¶„í•œì§€ í™•ì¸\")\n",
    "        print(\"4. Python ë²„ì „ì´ 3.7 ì´ìƒì¸ì§€ í™•ì¸\")\n",
    "\n",
    "# ì‹¤í–‰\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# ================================================================================================\n",
    "# 8. ì¶”ê°€ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤\n",
    "# ================================================================================================\n",
    "\n",
    "def analyze_feature_importance(model, feature_names, top_k=20):\n",
    "    \"\"\"íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„\"\"\"\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(f\"\\nìƒìœ„ {top_k}ê°œ ì¤‘ìš” íŠ¹ì„±:\")\n",
    "        print(importance_df.head(top_k))\n",
    "        \n",
    "        return importance_df\n",
    "    else:\n",
    "        print(\"ì´ ëª¨ë¸ì€ íŠ¹ì„± ì¤‘ìš”ë„ë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
    "        return None\n",
    "\n",
    "def cross_validate_threshold(model, X_val, y_val, thresholds=None):\n",
    "    \"\"\"ìµœì  ì„ê³„ê°’ ì°¾ê¸°\"\"\"\n",
    "    if thresholds is None:\n",
    "        thresholds = np.arange(0.3, 0.8, 0.05)\n",
    "    \n",
    "    best_threshold = 0.5\n",
    "    best_f1 = 0\n",
    "    \n",
    "    y_pred_proba = model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "        f1 = f1_score(y_val, y_pred)\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    print(f\"ìµœì  ì„ê³„ê°’: {best_threshold:.3f} (F1: {best_f1:.5f})\")\n",
    "    return best_threshold, best_f1\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ (ì„ íƒì‚¬í•­)\n",
    "\"\"\"\n",
    "# íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„\n",
    "if 'xgb' in predictor.trainer.models:\n",
    "    analyze_feature_importance(\n",
    "        predictor.trainer.models['xgb'], \n",
    "        predictor.X_train.columns\n",
    "    )\n",
    "\n",
    "# ì„ê³„ê°’ ìµœì í™” (ê²€ì¦ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°)\n",
    "# best_threshold, best_f1 = cross_validate_threshold(\n",
    "#     predictor.trainer.models['xgb'], \n",
    "#     X_val, y_val\n",
    "# )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ade636ba-2f7d-4f14-864b-eac18fe4c779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ë‹¨ìˆœ ë² ì´ìŠ¤ë¼ì¸ ì‹œì‘!\n",
      "========================================\n",
      "ğŸ“Š ë°ì´í„° ë¡œë”©...\n",
      "  Train: (87159, 16)\n",
      "  Test: (46204, 15)\n",
      "  í´ë˜ìŠ¤ 0: 76,700ê°œ (88.0%)\n",
      "  í´ë˜ìŠ¤ 1: 10,459ê°œ (12.0%)\n",
      "ğŸ”§ ë‹¨ìˆœ ì „ì²˜ë¦¬ ì‹œì‘...\n",
      "  Gender: 2ê°œ í´ë˜ìŠ¤\n",
      "  Country: 10ê°œ í´ë˜ìŠ¤\n",
      "  Race: 5ê°œ í´ë˜ìŠ¤\n",
      "  Family_Background: 2ê°œ í´ë˜ìŠ¤\n",
      "  Radiation_History: 2ê°œ í´ë˜ìŠ¤\n",
      "  Iodine_Deficiency: 2ê°œ í´ë˜ìŠ¤\n",
      "  Smoke: 2ê°œ í´ë˜ìŠ¤\n",
      "  Weight_Risk: 2ê°œ í´ë˜ìŠ¤\n",
      "  Diabetes: 2ê°œ í´ë˜ìŠ¤\n",
      "âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: Train (87159, 14), Test (46204, 14)\n",
      "\n",
      "ğŸ¤– ë‹¨ìˆœ ëª¨ë¸ í›ˆë ¨...\n",
      "  Random Forest í›ˆë ¨ ì¤‘...\n",
      "    RF CV F1: 0.4736 Â± 0.0079\n",
      "  XGBoost í›ˆë ¨ ì¤‘...\n",
      "    XGB CV F1: 0.4709 Â± 0.0034\n",
      "  LightGBM í›ˆë ¨ ì¤‘...\n",
      "    LGB CV F1: 0.4765 Â± 0.0037\n",
      "\n",
      "âœ… ìµœê³  ëª¨ë¸: lgb (CV F1: 0.4765)\n",
      "\n",
      "ğŸ“¤ lgb ëª¨ë¸ë¡œ ì˜ˆì¸¡ ì¤‘...\n",
      "\n",
      "ì˜ˆì¸¡ ë¶„í¬:\n",
      "  í´ë˜ìŠ¤ 0: 40,221ê°œ (87.1%)\n",
      "  í´ë˜ìŠ¤ 1: 5,983ê°œ (12.9%)\n",
      "âœ… ì œì¶œ íŒŒì¼ ì €ì¥: simple_submission.csv\n",
      "\n",
      "ğŸ¯ CV ì ìˆ˜ ìš”ì•½:\n",
      "  lgb: 0.4765\n",
      "  rf: 0.4736\n",
      "  xgb: 0.4709\n",
      "\n",
      "ğŸ’¡ ë§Œì•½ ì—¬ì „íˆ ì ìˆ˜ê°€ ë‚®ë‹¤ë©´:\n",
      "1. ë°ì´í„°ì— ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆìŒ\n",
      "2. Public/Private ë¶„í•  ë¬¸ì œ\n",
      "3. í‰ê°€ ì§€í‘œ ì°¨ì´\n",
      "4. ë‹¤ë¥¸ ì°¸ê°€ìë“¤ë„ ë¹„ìŠ·í•œ ì ìˆ˜ì¼ ê°€ëŠ¥ì„±\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# ì‹œë“œ ê³ ì •\n",
    "np.random.seed(42)\n",
    "\n",
    "def simple_preprocessing(train_df, test_df):\n",
    "    \"\"\"ì´ˆê°„ë‹¨ ì „ì²˜ë¦¬ - ì•ˆì „í•˜ê³  ê²€ì¦ëœ ë°©ë²•ë§Œ\"\"\"\n",
    "    print(\"ğŸ”§ ë‹¨ìˆœ ì „ì²˜ë¦¬ ì‹œì‘...\")\n",
    "    \n",
    "    # Feature columns (IDì™€ Cancer ì œì™¸)\n",
    "    feature_cols = [col for col in train_df.columns if col not in ['ID', 'Cancer']]\n",
    "    \n",
    "    # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ë¦¬\n",
    "    X_train = train_df[feature_cols].copy()\n",
    "    y_train = train_df['Cancer'].copy()\n",
    "    X_test = test_df[feature_cols].copy()\n",
    "    \n",
    "    # 1. ì¹´í…Œê³ ë¦¬ì»¬ ë³€ìˆ˜ë§Œ ì¸ì½”ë”© (ìˆ˜ì¹˜í˜•ì€ ê·¸ëŒ€ë¡œ)\n",
    "    categorical_cols = X_train.select_dtypes(include=['object']).columns\n",
    "    label_encoders = {}\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        \n",
    "        # í›ˆë ¨ ë°ì´í„° ì¸ì½”ë”©\n",
    "        X_train[col] = le.fit_transform(X_train[col].astype(str))\n",
    "        \n",
    "        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¸ì½”ë”© (ìƒˆë¡œìš´ ê°’ì€ 0ìœ¼ë¡œ)\n",
    "        test_values = X_test[col].astype(str)\n",
    "        test_encoded = []\n",
    "        for val in test_values:\n",
    "            if val in le.classes_:\n",
    "                test_encoded.append(le.transform([val])[0])\n",
    "            else:\n",
    "                test_encoded.append(0)  # ìƒˆë¡œìš´ ê°’ì€ 0\n",
    "        X_test[col] = test_encoded\n",
    "        \n",
    "        label_encoders[col] = le\n",
    "        print(f\"  {col}: {len(le.classes_)}ê°œ í´ë˜ìŠ¤\")\n",
    "    \n",
    "    # 2. ê²°ì¸¡ê°’ ë‹¨ìˆœ ì²˜ë¦¬\n",
    "    # ìˆ˜ì¹˜í˜•: ì¤‘ì•™ê°’\n",
    "    numeric_cols = X_train.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        median_val = X_train[col].median()\n",
    "        X_train[col].fillna(median_val, inplace=True)\n",
    "        X_test[col].fillna(median_val, inplace=True)\n",
    "    \n",
    "    print(f\"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: Train {X_train.shape}, Test {X_test.shape}\")\n",
    "    return X_train, y_train, X_test\n",
    "\n",
    "def train_simple_models(X_train, y_train):\n",
    "    \"\"\"ê°„ë‹¨í•œ ëª¨ë¸ë“¤ í›ˆë ¨\"\"\"\n",
    "    print(\"\\nğŸ¤– ë‹¨ìˆœ ëª¨ë¸ í›ˆë ¨...\")\n",
    "    \n",
    "    models = {}\n",
    "    cv_scores = {}\n",
    "    \n",
    "    # Cross Validation ì„¤ì •\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    # 1. Random Forest (í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ë§Œ)\n",
    "    print(\"  Random Forest í›ˆë ¨ ì¤‘...\")\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        random_state=42,\n",
    "        class_weight='balanced',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf_scores = cross_val_score(rf, X_train, y_train, cv=cv, scoring='f1')\n",
    "    rf.fit(X_train, y_train)\n",
    "    models['rf'] = rf\n",
    "    cv_scores['rf'] = rf_scores.mean()\n",
    "    print(f\"    RF CV F1: {rf_scores.mean():.4f} Â± {rf_scores.std():.4f}\")\n",
    "    \n",
    "    # 2. XGBoost (ê¸°ë³¸ ì„¤ì •)\n",
    "    print(\"  XGBoost í›ˆë ¨ ì¤‘...\")\n",
    "    # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚°\n",
    "    pos_count = (y_train == 1).sum()\n",
    "    neg_count = (y_train == 0).sum()\n",
    "    scale_pos_weight = neg_count / pos_count\n",
    "    \n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    xgb_scores = cross_val_score(xgb_model, X_train, y_train, cv=cv, scoring='f1')\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    models['xgb'] = xgb_model\n",
    "    cv_scores['xgb'] = xgb_scores.mean()\n",
    "    print(f\"    XGB CV F1: {xgb_scores.mean():.4f} Â± {xgb_scores.std():.4f}\")\n",
    "    \n",
    "    # 3. LightGBM (ê¸°ë³¸ ì„¤ì •)\n",
    "    print(\"  LightGBM í›ˆë ¨ ì¤‘...\")\n",
    "    lgb_model = lgb.LGBMClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        class_weight='balanced',\n",
    "        verbose=-1\n",
    "    )\n",
    "    lgb_scores = cross_val_score(lgb_model, X_train, y_train, cv=cv, scoring='f1')\n",
    "    lgb_model.fit(X_train, y_train)\n",
    "    models['lgb'] = lgb_model\n",
    "    cv_scores['lgb'] = lgb_scores.mean()\n",
    "    print(f\"    LGB CV F1: {lgb_scores.mean():.4f} Â± {lgb_scores.std():.4f}\")\n",
    "    \n",
    "    # ìµœê³  ëª¨ë¸ ì„ íƒ\n",
    "    best_model_name = max(cv_scores.items(), key=lambda x: x[1])[0]\n",
    "    print(f\"\\nâœ… ìµœê³  ëª¨ë¸: {best_model_name} (CV F1: {cv_scores[best_model_name]:.4f})\")\n",
    "    \n",
    "    return models, cv_scores, best_model_name\n",
    "\n",
    "def make_predictions(models, best_model_name, X_test, test_df):\n",
    "    \"\"\"ì˜ˆì¸¡ ë° ì œì¶œ íŒŒì¼ ìƒì„±\"\"\"\n",
    "    print(f\"\\nğŸ“¤ {best_model_name} ëª¨ë¸ë¡œ ì˜ˆì¸¡ ì¤‘...\")\n",
    "    \n",
    "    best_model = models[best_model_name]\n",
    "    predictions = best_model.predict(X_test)\n",
    "    \n",
    "    # ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "    submission = pd.DataFrame({\n",
    "        'ID': test_df['ID'],\n",
    "        'Cancer': predictions\n",
    "    })\n",
    "    \n",
    "    # ì˜ˆì¸¡ ë¶„í¬ í™•ì¸\n",
    "    pred_counts = pd.Series(predictions).value_counts()\n",
    "    print(f\"\\nì˜ˆì¸¡ ë¶„í¬:\")\n",
    "    for cls in [0, 1]:\n",
    "        count = pred_counts.get(cls, 0)\n",
    "        pct = count / len(predictions) * 100\n",
    "        print(f\"  í´ë˜ìŠ¤ {cls}: {count:,}ê°œ ({pct:.1f}%)\")\n",
    "    \n",
    "    return submission\n",
    "\n",
    "def main():\n",
    "    \"\"\"ë©”ì¸ ì‹¤í–‰\"\"\"\n",
    "    print(\"ğŸš€ ë‹¨ìˆœ ë² ì´ìŠ¤ë¼ì¸ ì‹œì‘!\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    try:\n",
    "        # 1. ë°ì´í„° ë¡œë”©\n",
    "        print(\"ğŸ“Š ë°ì´í„° ë¡œë”©...\")\n",
    "        train_df = pd.read_csv('train.csv')\n",
    "        test_df = pd.read_csv('test.csv')\n",
    "        \n",
    "        print(f\"  Train: {train_df.shape}\")\n",
    "        print(f\"  Test: {test_df.shape}\")\n",
    "        \n",
    "        # í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸\n",
    "        class_dist = train_df['Cancer'].value_counts()\n",
    "        print(f\"  í´ë˜ìŠ¤ 0: {class_dist[0]:,}ê°œ ({class_dist[0]/len(train_df)*100:.1f}%)\")\n",
    "        print(f\"  í´ë˜ìŠ¤ 1: {class_dist[1]:,}ê°œ ({class_dist[1]/len(train_df)*100:.1f}%)\")\n",
    "        \n",
    "        # 2. ì „ì²˜ë¦¬\n",
    "        X_train, y_train, X_test = simple_preprocessing(train_df, test_df)\n",
    "        \n",
    "        # 3. ëª¨ë¸ í›ˆë ¨\n",
    "        models, cv_scores, best_model_name = train_simple_models(X_train, y_train)\n",
    "        \n",
    "        # 4. ì˜ˆì¸¡\n",
    "        submission = make_predictions(models, best_model_name, X_test, test_df)\n",
    "        \n",
    "        # 5. ì €ì¥\n",
    "        submission.to_csv('simple_submission.csv', index=False)\n",
    "        print(f\"âœ… ì œì¶œ íŒŒì¼ ì €ì¥: simple_submission.csv\")\n",
    "        \n",
    "        print(\"\\nğŸ¯ CV ì ìˆ˜ ìš”ì•½:\")\n",
    "        for model_name, score in sorted(cv_scores.items(), key=lambda x: x[1], reverse=True):\n",
    "            print(f\"  {model_name}: {score:.4f}\")\n",
    "        \n",
    "        print(\"\\nğŸ’¡ ë§Œì•½ ì—¬ì „íˆ ì ìˆ˜ê°€ ë‚®ë‹¤ë©´:\")\n",
    "        print(\"1. ë°ì´í„°ì— ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆìŒ\")\n",
    "        print(\"2. Public/Private ë¶„í•  ë¬¸ì œ\")\n",
    "        print(\"3. í‰ê°€ ì§€í‘œ ì°¨ì´\")\n",
    "        print(\"4. ë‹¤ë¥¸ ì°¸ê°€ìë“¤ë„ ë¹„ìŠ·í•œ ì ìˆ˜ì¼ ê°€ëŠ¥ì„±\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì˜¤ë¥˜: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# ============================================\n",
    "# ì¶”ê°€ ë””ë²„ê¹…ìš© í•¨ìˆ˜ë“¤\n",
    "# ============================================\n",
    "\n",
    "def check_data_quality(train_df, test_df):\n",
    "    \"\"\"ë°ì´í„° í’ˆì§ˆ ì²´í¬\"\"\"\n",
    "    print(\"\\nğŸ” ë°ì´í„° í’ˆì§ˆ ì²´í¬:\")\n",
    "    \n",
    "    feature_cols = [col for col in train_df.columns if col not in ['ID', 'Cancer']]\n",
    "    \n",
    "    for col in feature_cols:\n",
    "        train_unique = train_df[col].nunique()\n",
    "        test_unique = test_df[col].nunique()\n",
    "        \n",
    "        # ì¹´í…Œê³ ë¦¬ì»¬ ë³€ìˆ˜ì—ì„œ ì°¨ì´ê°€ í° ê²½ìš°\n",
    "        if train_df[col].dtype == 'object':\n",
    "            train_set = set(train_df[col].unique())\n",
    "            test_set = set(test_df[col].unique())\n",
    "            only_in_test = test_set - train_set\n",
    "            \n",
    "            if only_in_test:\n",
    "                print(f\"  âš ï¸  {col}: í…ŒìŠ¤íŠ¸ì—ë§Œ ìˆëŠ” ê°’ {len(only_in_test)}ê°œ\")\n",
    "    \n",
    "    print(\"âœ… ë°ì´í„° í’ˆì§ˆ ì²´í¬ ì™„ë£Œ\")\n",
    "\n",
    "def quick_feature_importance(model, feature_names, top_k=10):\n",
    "    \"\"\"ê°„ë‹¨í•œ íŠ¹ì„± ì¤‘ìš”ë„\"\"\"\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(f\"\\nğŸ“Š ìƒìœ„ {top_k}ê°œ ì¤‘ìš” íŠ¹ì„±:\")\n",
    "        for i, row in importance_df.head(top_k).iterrows():\n",
    "            print(f\"  {row['feature']}: {row['importance']:.4f}\")\n",
    "    \n",
    "# ì‚¬ìš© ì˜ˆì‹œ (ì„ íƒì‚¬í•­):\n",
    "# check_data_quality(train_df, test_df)\n",
    "# quick_feature_importance(models['xgb'], X_train.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
